# ChatBot AI - ë²„ì „ ëª…ì„¸ì„œ v1.5.x

## ê°œìš”
ì´ ë¬¸ì„œëŠ” ChatBot AI ì‹œìŠ¤í…œì˜ v1.5.x ê³„ì—´ ë²„ì „ì— ëŒ€í•œ ê³µì‹ ëª…ì„¸ì„œì…ë‹ˆë‹¤. v1.4.xì˜ HTTPS ë³´ì•ˆ ì‹œìŠ¤í…œì—ì„œ **OpenAI API í†µí•©**ê³¼ **ë¼ìš°í„° ê¸°ë°˜ ì•„í‚¤í…ì²˜**ë¡œ ëŒ€í­ í™•ì¥ë˜ì—ˆìœ¼ë©°, **í”„ë¡œí† íƒ€ì… ë¶„ë¦¬**, **ê³µìœ  ì„¤ì • ì‹œìŠ¤í…œ**, **UML ë‹¤ì´ì–´ê·¸ë¨ ë„ì…**ì„ í†µí•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ AI ì±—ë´‡ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

## ë²„ì „ ì •ë³´

| ë²„ì „ | ë¦´ë¦¬ì¦ˆ ë‚ ì§œ | ì»¤ë°‹ í•´ì‹œ | ìƒíƒœ |
|------|-------------|-----------|------|
| **v1.5.0** | 2025-03-21 | `e924df6e591ae81cb6718a18eb1ded24c76bc659` | Stable |
| **v1.5.1** | 2025-04-16 | `682362de5a91d68fe1cd37db38a1184cf958476b` | Stable |
| **v1.5.2** | 2025-04-16 | `3e0b9525e44f097d91485b718e10ce6073917168` | Latest |
| **None** | 2025-05-03 | `11adcd7ee4022e5b3f5b595ff132e02a869c145b` | Unstable |

## v1.4.xì—ì„œ v1.5.xë¡œì˜ ì£¼ìš” ë³€ê²½ì‚¬í•­

### ì•„í‚¤í…ì²˜ ëŒ€í˜ì‹ 
- **ë‹¨ì¼ ì—”ë“œí¬ì¸íŠ¸** â†’ **ë¼ìš°í„° ê¸°ë°˜ RESTful API** (`/office`, `/character`)
- **GGUF ì „ìš©** â†’ **GGUF + OpenAI API í•˜ì´ë¸Œë¦¬ë“œ** ì‹œìŠ¤í…œ
- **ëª¨ë…¸ë¦¬ì‹ êµ¬ì¡°** â†’ **ëª¨ë“ˆí™”ëœ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤** ì•„í‚¤í…ì²˜

### AI ëª¨ë¸ í™•ì¥
- **2ê°œ GGUF ëª¨ë¸** â†’ **5ê°œ AI ëª¨ë¸** ì§€ì›
- **OpenAI GPT í†µí•©**: GPT-4o-mini, GPT-4.1, GPT-4.1-mini
- **ê³µìœ  ì„¤ì • ì‹œìŠ¤í…œ**: í†µí•©ëœ í”„ë¡¬í”„íŠ¸ ë° ìƒì„± íŒŒë¼ë¯¸í„° ê´€ë¦¬

### ê°œë°œ í™˜ê²½ ê°œì„ 
- **í…ŒìŠ¤íŠ¸ í´ë”** â†’ **í”„ë¡œí† íƒ€ì… ë””ë ‰í† ë¦¬** ë¶„ë¦¬
- **ë°°ì¹˜ íŒŒì¼** ê²½ë¡œ ì •ë¦¬ (`batchfile` â†’ `batch`)
- **ì„¤ì • íŒŒì¼** ê²½ë¡œ í†µí•© (`models` â†’ `prompt`)
- **UML ë‹¤ì´ì–´ê·¸ë¨** ë„ì… (í´ë˜ìŠ¤/íŒ¨í‚¤ì§€ ë‹¤ì´ì–´ê·¸ë¨)

### ì‹ ê·œ ê¸°ëŠ¥
- **ì„ë² ë”© ê¸°ë°˜ ì±„íŒ… ë©”ëª¨ë¦¬** í”„ë¡œí† íƒ€ì…
- **SonarQube** ì •ì  ì½”ë“œ ë¶„ì„ ì§€ì›
- **ê³µìœ  ì„¤ì • í´ë˜ìŠ¤** (TypedDict, dataclass í™œìš©)
- **HTTP â†’ HTTPS** ê°œë°œ í™˜ê²½ ì „í™˜

### ì œê±°ëœ ê¸°ëŠ¥
- âŒ **Transformers ê¸°ë°˜ Llama ëª¨ë¸** ì™„ì „ ì œê±°
- âŒ **index.html** ì›¹ ì¸í„°í˜ì´ìŠ¤ ì œê±°
- âŒ **SSE ìŠ¤íŠ¸ë¦¬ë°** ì—”ë“œí¬ì¸íŠ¸ ë¹„í™œì„±í™”
- âŒ **Hypercorn ì„œë²„** (uvicorn ë³µê·€)

## ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **GPU**: NVIDIA RTX 3060 (CUDA:0) + RTX 2080 (CUDA:1)
- **VRAM**: ìµœì†Œ 20GB RAM (3060 12GB + 2080 8GB)
- **ì €ì¥ê³µê°„**: ìµœì†Œ 50GB ì—¬ìœ  ê³µê°„
- **ë„¤íŠ¸ì›Œí¬**: ê³µì¸ IP ì£¼ì†Œ + OpenAI API ì ‘ì†

### ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **ìš´ì˜ì²´ì œ**: Windows 10/11 (64-bit)
- **Python**: 3.11 ì´ìƒ
- **CUDA**: 11.8/12.8 ì§€ì›
- **llama-cpp-python**: CUDA ì§€ì› ë²„ì „
- **MongoDB**: ë¡œì»¬ ì„¤ì¹˜
- **OpenAI API**: ìœ íš¨í•œ API í‚¤

## ì•„í‚¤í…ì²˜

### í”„ë¡œì íŠ¸ êµ¬ì¡°
```
ğŸ“¦ ChatBot AI v1.5.x
â”œâ”€â”€ ğŸ“ fastapi/
â”‚   â”œâ”€â”€ ğŸ“ ai_model/              # GGUF ëª¨ë¸ ì €ì¥ì†Œ
â”‚   â”‚   â”œâ”€â”€ llama-3-Korean-Bllossom-8B-Q4_K_M.gguf
â”‚   â”‚   â”œâ”€â”€ v2-Llama-3-Lumimaid-8B-v0.1-OAS-Q5_K_S-imat.gguf
â”‚   â”‚   â””â”€â”€ README.md [UPDATED]
â”‚   â”œâ”€â”€ ğŸ“ batch/                 # í™˜ê²½ ì„¤ì • ë°°ì¹˜ íŒŒì¼ [RENAMED]
â”‚   â”‚   â”œâ”€â”€ venv_install.bat [MOVED]
â”‚   â”‚   â””â”€â”€ venv_setup.bat [MOVED]
â”‚   â”œâ”€â”€ ğŸ“ certificates/         # SSL ì¸ì¦ì„œ ê´€ë¦¬
â”‚   â”‚   â”œâ”€â”€ DNS_README.md
â”‚   â”‚   â”œâ”€â”€ PEM_README.md [UPDATED]
â”‚   â”‚   â””â”€â”€ *.pem [IGNORED]
â”‚   â”œâ”€â”€ ğŸ“ src/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ prototypes/        # ì‹¤í—˜/í”„ë¡œí† íƒ€ì… ì½”ë“œ [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ train/         # íŒŒì¸íŠœë‹ ê´€ë ¨ [MOVED]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ train.py [MOVED]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ train_README.md [MOVED]
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding_chat_memory.py [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ shared.py [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ DuckDuckGo.py [MOVED]
â”‚   â”‚   â”‚   â”œâ”€â”€ GGUF_CPU.py [MOVED]
â”‚   â”‚   â”‚   â”œâ”€â”€ GGUF_GPU.py [MOVED]
â”‚   â”‚   â”‚   â”œâ”€â”€ Llama_cpp_test.py [MOVED]
â”‚   â”‚   â”‚   â”œâ”€â”€ V2_prompt.py [MOVED]
â”‚   â”‚   â”‚   â”œâ”€â”€ bot.py [MOVED]
â”‚   â”‚   â”‚   â”œâ”€â”€ cuda_gpu.py [MOVED]
â”‚   â”‚   â”‚   â”œâ”€â”€ httpx_test.py [MOVED]
â”‚   â”‚   â”‚   â””â”€â”€ load_dataset.py [MOVED]
â”‚   â”‚   â”œâ”€â”€ ğŸ“ utils/             # ëª¨ë“ˆí™”ëœ ìœ í‹¸ë¦¬í‹°
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ ai_models/     # AI ëª¨ë¸ ëª¨ë“ˆ
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ shared/    # ê³µìœ  ì„¤ì • [NEW]
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ shared_configs.py [NEW]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ bllossom_model.py [UPDATED]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ lumimaid_model.py [UPDATED]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ openai_character_model.py [NEW]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ openai_office_model.py [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ handlers/      # í•¸ë“¤ëŸ¬ ëª¨ë“ˆ
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ error_handler.py [UPDATED]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ language_handler.py [UPDATED]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ mongodb_handler.py [UPDATED]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ schemas/       # ìŠ¤í‚¤ë§ˆ ëª¨ë“ˆ
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ chat_schema.py [UPDATED]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ services/      # ì„œë¹„ìŠ¤ ëª¨ë“ˆ
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ search_service.py [UPDATED]
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py [UPDATED]
â”‚   â”‚   â”œâ”€â”€ server.py [COMPLETELY REDESIGNED]
â”‚   â”‚   â”œâ”€â”€ bot.yaml
â”‚   â”‚   â””â”€â”€ .env
â”‚   â””â”€â”€ requirements.txt [UPDATED]
â”œâ”€â”€ ğŸ“ prompt/                    # í”„ë¡¬í”„íŠ¸ ë° ì„¤ì • íŒŒì¼ [RENAMED]
â”‚   â”œâ”€â”€ config-Llama.json [NEW]
â”‚   â”œâ”€â”€ config-OpenAI.json [RENAMED]
â”‚   â””â”€â”€ config-user.yaml [MOVED]
â””â”€â”€ .gitignore [UPDATED]
```

## API ëª…ì„¸

### ë¼ìš°í„° ê¸°ë°˜ API ì•„í‚¤í…ì²˜

#### Office Router (`/office`)
ì—…ë¬´ ë° ì¼ë°˜ì ì¸ ì§ˆì˜ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ë¼ìš°í„°ì…ë‹ˆë‹¤.

##### POST /office/Llama
Bllossom GGUF ëª¨ë¸ ê¸°ë°˜ ê²€ìƒ‰ ì—°ë™ JSON ì‘ë‹µ

**ìš”ì²­ í˜•ì‹:**
```json
{
  "input_data": "string (1-500ì)",
  "google_access": "boolean (default: false)",
  "db_id": "string (UUID)",
  "user_id": "string"
}
```

**ì‘ë‹µ í˜•ì‹:**
- **Content-Type**: `application/json`
- **Response**: `"string"` (JSON ë¬¸ìì—´ ì§ì ‘ ë°˜í™˜)

##### POST /office/{gpt_set}
OpenAI GPT ëª¨ë¸ ê¸°ë°˜ ê²€ìƒ‰ ì—°ë™ JSON ì‘ë‹µ

**ê²½ë¡œ ë§¤ê°œë³€ìˆ˜:**
- `gpt_set`: OpenAI ëª¨ë¸ ë³„ì¹­
  - `gpt4o_mini` â†’ `gpt-4o-mini`
  - `gpt4.1` â†’ `gpt-4.1`
  - `gpt4.1_mini` â†’ `gpt-4.1-mini`

**ìš”ì²­ í˜•ì‹:**
```json
{
  "input_data": "Llama AI ëª¨ë¸ì˜ ì¶œì‹œì¼ê³¼ ë²„ì „ë“¤ì„ ê°ê° ì•Œë ¤ì¤˜.",
  "google_access": true,
  "db_id": "123e4567-e89b-12d3-a456-426614174000",
  "user_id": "user123"
}
```

#### Character Router (`/character`)
ìºë¦­í„° ê¸°ë°˜ ëŒ€í™”ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë¼ìš°í„°ì…ë‹ˆë‹¤.

##### POST /character/Llama
Lumimaid GGUF ëª¨ë¸ ê¸°ë°˜ ìºë¦­í„° ëŒ€í™” JSON ì‘ë‹µ

**ìš”ì²­ í˜•ì‹:**
```json
{
  "input_data": "string (1-500ì)",
  "character_name": "string",
  "greeting": "string",
  "context": "string",
  "db_id": "string (UUID)",
  "user_id": "string"
}
```

##### POST /character/{gpt_set}
OpenAI GPT ëª¨ë¸ ê¸°ë°˜ ìºë¦­í„° ëŒ€í™” JSON ì‘ë‹µ

**ì˜ˆì‹œ ìš”ì²­:**
```json
{
  "input_data": "*I approach Rachel and talk to her.*",
  "character_name": "Rachel",
  "greeting": "*Rachel stands nervously at the lectern...*",
  "context": "Rachel is a devout Catholic girl of about 19 years old...",
  "db_id": "b440780c-cbaa-454f-a8d2-cf884786d89f",
  "user_id": "djjdjs74"
}
```

**HTTP ìƒíƒœ ì½”ë“œ:**
- `200`: ì„±ê³µ
- `400`: ì˜ëª»ëœ ìš”ì²­ (ì˜ëª»ëœ ëª¨ë¸ëª…)
- `422`: ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨
- `500`: ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜

## ì£¼ìš” ì»´í¬ë„ŒíŠ¸

### 1. ì„œë²„ ì»´í¬ë„ŒíŠ¸ (server.py) - v1.5.x ì™„ì „ ì¬ì„¤ê³„

#### ë¼ìš°í„° ê¸°ë°˜ ì•„í‚¤í…ì²˜
v1.4.xì˜ ë‹¨ì¼ ì—”ë“œí¬ì¸íŠ¸ì—ì„œ **RESTful ë¼ìš°í„° ì‹œìŠ¤í…œ**ìœ¼ë¡œ ëŒ€í­ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.

```python
# OpenAI ëª¨ë¸ ë§¤í•‘ ì‹œìŠ¤í…œ
OPENAI_MODEL_MAP = {
    "gpt4o_mini": "gpt-4o-mini",
    "gpt4.1": "gpt-4.1",
    "gpt4.1_mini": "gpt-4.1-mini",
}

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    FastAPI AI ëª¨ë¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™” - v1.5.x
    GGUF + OpenAI API í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ
    """
    global Bllossom_model, Lumimaid_model, GREEN, RESET

    try:
        # GGUF ëª¨ë¸ ë¡œë“œ
        Bllossom_model = Bllossom()                 # cuda:1 (RTX 2080)
        Lumimaid_model = Lumimaid()                 # cuda:0 (RTX 3060)
    except ChatError.InternalServerErrorException as e:
        component = "MongoDBHandler"
        print(f"{RED}ERROR{RESET}: {component} ì´ˆê¸°í™” ì¤‘ {e.__class__.__name__} ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        exit(1)

    # ë””ë²„ê¹…ìš© ì¶œë ¥
    print(f"{GREEN}INFO{RESET}: Bllossom ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({Bllossom_device_info})")
    print(f"{GREEN}INFO{RESET}: Lumimaid ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({Lumimaid_device_info})")
    print(f"{GREEN}INFO{RESET}: OpenAiOffice ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (API í˜¸ì¶œ)")
    print(f"{GREEN}INFO{RESET}: OpenAiCharacter ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (API í˜¸ì¶œ)")

    yield

    Bllossom_model = None
    Lumimaid_model = None
    print(f"{GREEN}INFO{RESET}: ëª¨ë¸ í•´ì œ ì™„ë£Œ")
```

#### Office ë¼ìš°í„° êµ¬í˜„
ì—…ë¬´ìš© AI ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ì „ìš© ë¼ìš°í„°ì…ë‹ˆë‹¤.

```python
office_router = APIRouter()

@office_router.post("/Llama", summary="Llama ëª¨ë¸ì´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ ìƒì„±")
async def office_llama(request: ChatModel.office_Request):
    """
    Bllossom GGUF ëª¨ë¸ ê¸°ë°˜ ê²€ìƒ‰ ì—°ë™ ì‘ë‹µ
    """
    chat_list = []
    search_context = ""
    
    # MongoDBì—ì„œ ì±„íŒ… ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
    if mongo_handler or request.db_id:
        try:
            chat_list = await mongo_handler.get_office_log(
                user_id=request.user_id,
                document_id=request.db_id,
                router="office",
            )
        except Exception as e:
            print(f"{YELLOW}WARNING{RESET}: ì±„íŒ… ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")

    # DuckDuckGo ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    if request.google_access:
        try:
            duck_results = await ChatSearch.fetch_duck_search_results(query=request.input_data)
            if duck_results:
                formatted_results = []
                for idx, item in enumerate(duck_results[:10], 1):
                    formatted_result = (
                        f"[ê²€ìƒ‰ê²°ê³¼ {idx}]\n"
                        f"ì œëª©: {item.get('title', 'ì œëª© ì—†ìŒ')}\n"
                        f"ë‚´ìš©: {item.get('snippet', 'ë‚´ìš© ì—†ìŒ')}\n"
                        f"ì¶œì²˜: {item.get('link', 'ë§í¬ ì—†ìŒ')}\n"
                    )
                    formatted_results.append(formatted_result)
                search_context = (
                    "ë‹¤ìŒì€ ê²€ìƒ‰ì—ì„œ ê°€ì ¸ì˜¨ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:\n\n" +
                    "\n".join(formatted_results)
                )
        except Exception:
            print(f"{YELLOW}WARNING{RESET}: ê²€ìƒ‰ì˜ í•œë„ ì´ˆê³¼ë¡œ DuckDuckGo ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    try:        
        full_response = Bllossom_model.generate_response(
            input_text=request.input_data,
            search_text=search_context,
            chat_list=chat_list,
        )
        return full_response
    except TimeoutError:
        raise ChatError.InternalServerErrorException(detail="Bllossom ëª¨ë¸ ì‘ë‹µì´ ì‹œê°„ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except ValidationError as e:
        raise ChatError.BadRequestException(detail=str(e))
    except Exception as e:
        raise ChatError.InternalServerErrorException(detail="ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@office_router.post("/{gpt_set}", summary="gpt ëª¨ë¸ì´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ ìƒì„±")
async def office_gpt(
        request: ChatModel.office_Request,
        gpt_set: str = Path(
            ...,
            title="GPT ëª¨ë¸ëª…",
            description="ì‚¬ìš©í•  OpenAI GPT ëª¨ë¸ì˜ ë³„ì¹­ (ì˜ˆ: gpt4o_mini, gpt4.1, gpt4.1_mini)",
            examples=list(OPENAI_MODEL_MAP.keys()),
        )
    ):
    """
    OpenAI GPT ëª¨ë¸ ê¸°ë°˜ ê²€ìƒ‰ ì—°ë™ ì‘ë‹µ
    """
    if gpt_set not in OPENAI_MODEL_MAP:
        raise HTTPException(status_code=400, detail="Invalid model name.")

    model_id = OPENAI_MODEL_MAP[gpt_set]
    # ... (ê²€ìƒ‰ ë¡œì§ ë™ì¼)

    OpenAiOffice_model = OpenAiOffice(model_id=model_id)
    try:
        full_response = OpenAiOffice_model.generate_response(
            input_text=request.input_data,
            search_text=search_context,
            chat_list=chat_list,
        )
        return full_response
    except TimeoutError:
        raise ChatError.InternalServerErrorException(detail="OpenAI ëª¨ë¸ ì‘ë‹µì´ ì‹œê°„ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except ValidationError as e:
        raise ChatError.BadRequestException(detail=str(e))
    except Exception as e:
        raise ChatError.InternalServerErrorException(detail="ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

app.include_router(
    office_router,
    prefix="/office",
    tags=["office Router"],
    responses={500: {"description": "Internal Server Error"}}
)
```

#### Character ë¼ìš°í„° êµ¬í˜„
ìºë¦­í„° ê¸°ë°˜ ëŒ€í™”ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì „ìš© ë¼ìš°í„°ì…ë‹ˆë‹¤.

```python
character_router = APIRouter()

@character_router.post("/Llama", summary="Llama ëª¨ë¸ì´ ì¼€ë¦­í„° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±")
async def character_llama(request: ChatModel.character_Request):
    """
    Lumimaid GGUF ëª¨ë¸ ê¸°ë°˜ ìºë¦­í„° ëŒ€í™”
    """
    chat_list = []
    
    # MongoDBì—ì„œ ì±„íŒ… ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
    if mongo_handler or request.db_id:
        try:
            chat_list = await mongo_handler.get_character_log(
                user_id=request.user_id,
                document_id=request.db_id,
                router="character",
            )
        except Exception as e:
            print(f"{YELLOW}WARNING{RESET}: ì±„íŒ… ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")
            
    try:
        character_settings = {
            "character_name": request.character_name,
            "greeting": request.greeting,
            "context": request.context,
            "chat_list": chat_list,
        }
        full_response = Lumimaid_model.generate_response(
            input_text=request.input_data,
            character_settings=character_settings,
        )
        return full_response
    except TimeoutError:
        raise ChatError.InternalServerErrorException(detail="Lumimaid ëª¨ë¸ ì‘ë‹µì´ ì‹œê°„ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except ValidationError as e:
        raise ChatError.BadRequestException(detail=str(e))
    except Exception as e:
        raise ChatError.InternalServerErrorException(detail="ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@character_router.post("/{gpt_set}", summary="gpt ëª¨ë¸ì´ ì¼€ë¦­í„° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±")
async def character_gpt(
        request: ChatModel.character_Request,
        gpt_set: str = Path(
            ...,
            title="GPT ëª¨ë¸ëª…",
            description="ì‚¬ìš©í•  OpenAI GPT ëª¨ë¸ì˜ ë³„ì¹­ (ì˜ˆ: gpt4o_mini, gpt4.1, gpt4.1_mini)",
            examples=list(OPENAI_MODEL_MAP.keys()),
        )
    ):
    """
    OpenAI GPT ëª¨ë¸ ê¸°ë°˜ ìºë¦­í„° ëŒ€í™”
    """
    if gpt_set not in OPENAI_MODEL_MAP:
        raise HTTPException(status_code=400, detail="Invalid model name.")

    model_id = OPENAI_MODEL_MAP[gpt_set]
    # ... (ì±„íŒ… ê¸°ë¡ ë¡œì§ ë™ì¼)

    OpenAiCharacter_model = OpenAiCharacter(model_id=model_id)
    try:
        character_settings = {
            "character_name": request.character_name,
            "greeting": request.greeting,
            "context": request.context,
            "chat_list": chat_list,
        }
        full_response = OpenAiCharacter_model.generate_response(
            input_text=request.input_data,
            character_settings=character_settings,
        )
        return full_response
    except TimeoutError:
        raise ChatError.InternalServerErrorException(detail="Lumimaid ëª¨ë¸ ì‘ë‹µì´ ì‹œê°„ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except ValidationError as e:
        raise ChatError.BadRequestException(detail=str(e))
    except Exception as e:
        raise ChatError.InternalServerErrorException(detail="ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

app.include_router(
    character_router,
    prefix="/character",
    tags=["character Router"],
    responses={500: {"description": "Internal Server Error"}}
)
```

#### ê°œë°œ í™˜ê²½ ë³µê·€
HTTPSì—ì„œ HTTP ê°œë°œ í™˜ê²½ìœ¼ë¡œ ë³µê·€í–ˆìŠµë‹ˆë‹¤.

```python
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001)
```

### 2. AI ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ - v1.5.x ê³µìœ  ì„¤ì • ì‹œìŠ¤í…œ

#### ê³µìœ  ì„¤ì • í´ë˜ìŠ¤ (shared/shared_configs.py) - ì‹ ê·œ ì¶”ê°€

TypedDictì™€ dataclassë¥¼ í™œìš©í•œ í†µí•© ì„¤ì • ê´€ë¦¬ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

```python
from typing import TypedDict, Optional, List, Dict
from dataclasses import dataclass

class BaseConfig(TypedDict):
    """
    ê¸°ë³¸ ëª¨ë¸ ì„¤ì •ì„ ìœ„í•œ TypedDict
    """
    character_name: str
    character_setting: str
    greeting: str

@dataclass
class OfficePrompt:
    """
    Office ë¼ìš°í„°ìš© í”„ë¡¬í”„íŠ¸ ë°ì´í„° í´ë˜ìŠ¤
    """
    name: str
    context: str
    reference_data: str
    user_input: str
    chat_list: List[Dict]

@dataclass
class CharacterPrompt:
    """
    Character ë¼ìš°í„°ìš© í”„ë¡¬í”„íŠ¸ ë°ì´í„° í´ë˜ìŠ¤
    """
    name: str
    greeting: str
    context: str
    user_input: str
    chat_list: List[Dict]

@dataclass
class LlamaGenerationConfig:
    """
    Llama ëª¨ë¸ ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
    """
    prompt: str
    max_tokens: int = 2048
    temperature: float = 0.5
    top_p: float = 0.80
    stop: Optional[List[str]] = None
```

#### BllossomChatModel í´ë˜ìŠ¤ (bllossom_model.py) - v1.5.x ë¦¬íŒ©í† ë§

ê³µìœ  ì„¤ì • ì‹œìŠ¤í…œì„ í™œìš©í•œ ëª¨ë“ˆí™”ëœ êµ¬ì¡°ë¡œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.

```python
from .shared.shared_configs import OfficePrompt, LlamaGenerationConfig, BaseConfig

class BllossomChatModel:
    def __init__(self) -> None:
        self.model_id = "llama-3-Korean-Bllossom-8B-Q4_K_M"
        self.model_path = "fastapi/ai_model/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf"
        self.file_path = './prompt/config-Llama.json'
        self.loading_text = f"{BLUE}LOADING{RESET}: {self.model_id} ë¡œë“œ ì¤‘..."
        self.gpu_layers: int = 70
        self.character_info: Optional[OfficePrompt] = None
        self.config: Optional[LlamaGenerationConfig] = None

        print("\n" + f"{BLUE}LOADING{RESET}: " + "="*len(self.loading_text))
        print(f"{BLUE}LOADING{RESET}: {__class__.__name__} ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")

        # JSON íŒŒì¼ ì½ê¸°
        with open(self.file_path, 'r', encoding='utf-8') as file:
            self.data: BaseConfig = json.load(file)

        # ì§„í–‰ ìƒíƒœ í‘œì‹œ
        print(f"{BLUE}LOADING{RESET}: {__class__.__name__} ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        self.model: Llama = self._load_model()
        print(f"{BLUE}LOADING{RESET}: ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        print(f"{BLUE}LOADING{RESET}: " + "="*len(self.loading_text) + "\n")
        
        self.response_queue: Queue = Queue()

    def generate_response(self, input_text: str, search_text: str, chat_list: List[Dict]) -> str:
        """
        JSON ì‘ë‹µ ìƒì„± ë©”ì„œë“œ (ìŠ¤íŠ¸ë¦¬ë°ì—ì„œ ë³€ê²½)
        """
        try:
            current_time = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„")
            time_info = f"í˜„ì¬ ì‹œê°„ì€ {current_time}ì…ë‹ˆë‹¤.\n\n"
            reference_text = time_info + (search_text if search_text else "")

            normalized_chat_list = []
            for chat in chat_list:
                normalized_chat_list.append({
                    "input_data": self._normalize_escape_chars(chat.get("input_data", "")),
                    "output_data": self._normalize_escape_chars(chat.get("output_data", ""))
                })

            self.character_info: OfficePrompt = OfficePrompt(
                name=self.data.get("character_name"),
                context=self.data.get("character_setting"),
                reference_data=reference_text,
                user_input=input_text,
                chat_list=normalized_chat_list,
            )

            messages = build_llama3_messages(character_info=self.character_info)

            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            self.config = LlamaGenerationConfig(
                prompt=prompt,
                max_tokens=2048,
                temperature=0.5,
                top_p=0.80,
                stop=["<|eot_id|>"]
            )

            chunks = []
            for text_chunk in self.create_streaming_completion(config=self.config):
                chunks.append(text_chunk)
            return "".join(chunks)

        except Exception as e:
            return f"ì˜¤ë¥˜: {str(e)}"
```

#### LumimaidChatModel í´ë˜ìŠ¤ (lumimaid_model.py) - v1.5.x ì—…ë°ì´íŠ¸

ê³µìœ  ì„¤ì • ì‹œìŠ¤í…œì„ í†µí•©í•œ ìºë¦­í„° ëŒ€í™” ëª¨ë¸ì…ë‹ˆë‹¤.

```python
from .shared.shared_configs import CharacterPrompt, LlamaGenerationConfig, BaseConfig

class LumimaidChatModel:
    def __init__(self) -> None:
        self.model_id = "Llama-3-Lumimaid-8B-v0.1-OAS"
        self.model_path = "fastapi/ai_model/v2-Llama-3-Lumimaid-8B-v0.1-OAS-Q5_K_S-imat.gguf"
        self.file_path = './prompt/config-Llama.json'
        self.loading_text = f"{BLUE}LOADING{RESET}: {self.model_id} ë¡œë“œ ì¤‘..."
        self.gpu_layers: int = 70
        self.character_info: Optional[CharacterPrompt] = None
        self.config: Optional[LlamaGenerationConfig] = None

        # JSON íŒŒì¼ ì½ê¸°
        with open(self.file_path, 'r', encoding='utf-8') as file:
            self.data: BaseConfig = json.load(file)

        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model: Llama = self._load_model()
        self.response_queue: Queue = Queue()

    def generate_response(self, input_text: str, character_settings: dict = None) -> str:
        """
        ìºë¦­í„° ê¸°ë°˜ JSON ì‘ë‹µ ìƒì„± ë©”ì„œë“œ
        """
        try:
            if character_settings:
                self.character_info = CharacterPrompt(
                    name=character_settings.get("character_name", "Assistant"),
                    greeting=character_settings.get("greeting", ""),
                    context=character_settings.get("context", ""),
                    user_input=input_text,
                    chat_list=character_settings.get("chat_list", [])
                )
                prompt = build_llama3_prompt(character_info=self.character_info)
            else:
                prompt = input_text

            self.config = LlamaGenerationConfig(
                prompt=prompt,
                max_tokens=2048,
                temperature=0.8,
                top_p=0.95,
                stop=["<|eot_id|>"]
            )

            chunks = []
            for text_chunk in self.create_streaming_completion(config=self.config):
                chunks.append(text_chunk)
            return "".join(chunks)

        except Exception as e:
            return f"ì˜¤ë¥˜: {str(e)}"
```

#### OpenAI ëª¨ë¸ í´ë˜ìŠ¤ - v1.5.x ì‹ ê·œ ì¶”ê°€

##### OpenAIChatModel í´ë˜ìŠ¤ (openai_office_model.py)
Office ë¼ìš°í„°ìš© OpenAI API í†µí•© ëª¨ë¸ì…ë‹ˆë‹¤.

```python
import openai
from typing import List, Dict
import os
from dotenv import load_dotenv
from datetime import datetime

class OpenAIChatModel:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•˜ëŠ” Officeìš© ì±„íŒ… ëª¨ë¸
    """
    def __init__(self, model_id: str = "gpt-4o-mini"):
        load_dotenv()
        self.client = openai.OpenAI(
            api_key=os.getenv("OPENAI_API_KEY")
        )
        self.model_id = model_id

    def generate_response(self, input_text: str, search_text: str = "", chat_list: List[Dict] = None) -> str:
        """
        OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
        
        Args:
            input_text (str): ì‚¬ìš©ì ì…ë ¥
            search_text (str): ê²€ìƒ‰ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸
            chat_list (List[Dict]): ëŒ€í™” ê¸°ë¡
            
        Returns:
            str: ìƒì„±ëœ ì‘ë‹µ
        """
        try:
            # í˜„ì¬ ì‹œê°„ ì •ë³´ ì¶”ê°€
            current_time = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„")
            time_info = f"í˜„ì¬ ì‹œê°„ì€ {current_time}ì…ë‹ˆë‹¤.\n\n"
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            system_content = (
                "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
                "ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.\n\n"
                f"{time_info}"
            )
            
            if search_text:
                system_content += f"ì°¸ê³  ì •ë³´:\n{search_text}\n\n"
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [{"role": "system", "content": system_content}]
            
            # ëŒ€í™” ê¸°ë¡ ì¶”ê°€
            if chat_list:
                for chat in chat_list[-5:]:  # ìµœê·¼ 5ê°œë§Œ
                    if chat.get("input_data"):
                        messages.append({"role": "user", "content": chat["input_data"]})
                    if chat.get("output_data"):
                        messages.append({"role": "assistant", "content": chat["output_data"]})
            
            # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
            messages.append({"role": "user", "content": input_text})
            
            # OpenAI API í˜¸ì¶œ
            response = self.client.chat.completions.create(
                model=self.model_id,
                messages=messages,
                max_tokens=2048,
                temperature=0.7,
                top_p=0.9
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"OpenAI API ì˜¤ë¥˜: {str(e)}"
```

##### OpenAICharacterModel í´ë˜ìŠ¤ (openai_character_model.py)
Character ë¼ìš°í„°ìš© OpenAI API í†µí•© ëª¨ë¸ì…ë‹ˆë‹¤.

```python
class OpenAICharacterModel:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•˜ëŠ” ìºë¦­í„° ëŒ€í™” ëª¨ë¸
    """
    def __init__(self, model_id: str = "gpt-4o-mini"):
        load_dotenv()
        self.client = openai.OpenAI(
            api_key=os.getenv("OPENAI_API_KEY")
        )
        self.model_id = model_id

    def generate_response(self, input_text: str, character_settings: dict = None) -> str:
        """
        ìºë¦­í„° ê¸°ë°˜ OpenAI API ì‘ë‹µ ìƒì„±
        
        Args:
            input_text (str): ì‚¬ìš©ì ì…ë ¥
            character_settings (dict): ìºë¦­í„° ì„¤ì •
            
        Returns:
            str: ìƒì„±ëœ ì‘ë‹µ
        """
        try:
            if character_settings:
                # ìºë¦­í„° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                system_content = (
                    f"ë‹¹ì‹ ì€ {character_settings.get('character_name', 'Assistant')}ë¼ëŠ” ìºë¦­í„°ì…ë‹ˆë‹¤.\n"
                    f"ì¸ì‚¬ë§: {character_settings.get('greeting', '')}\n"
                    f"ìºë¦­í„° ì„¤ëª…: {character_settings.get('context', '')}\n\n"
                    "ìœ„ì˜ ìºë¦­í„° ì„¤ì •ì— ë§ê²Œ ì¼ê´€ëœ ì„±ê²©ê³¼ ë§íˆ¬ë¡œ ëŒ€í™”í•´ì£¼ì„¸ìš”. "
                    "ê°ì • í‘œí˜„ê³¼ í–‰ë™ ë¬˜ì‚¬ë¥¼ í¬í•¨í•˜ì—¬ ìƒë™ê° ìˆëŠ” ëŒ€í™”ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
                )
            else:
                system_content = "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [{"role": "system", "content": system_content}]
            
            # ëŒ€í™” ê¸°ë¡ ì¶”ê°€
            if character_settings and character_settings.get("chat_list"):
                for chat in character_settings["chat_list"][-5:]:  # ìµœê·¼ 5ê°œë§Œ
                    if chat.get("input_data"):
                        messages.append({"role": "user", "content": chat["input_data"]})
                    if chat.get("output_data"):
                        messages.append({"role": "assistant", "content": chat["output_data"]})
            
            # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
            messages.append({"role": "user", "content": input_text})
            
            # OpenAI API í˜¸ì¶œ
            response = self.client.chat.completions.create(
                model=self.model_id,
                messages=messages,
                max_tokens=2048,
                temperature=0.8,
                top_p=0.95
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"OpenAI API ì˜¤ë¥˜: {str(e)}"
```

### 3. í”„ë¡œí† íƒ€ì… ì‹œìŠ¤í…œ - v1.5.x ì‹ ê·œ ë¶„ë¦¬

#### ì„ë² ë”© ê¸°ë°˜ ì±„íŒ… ë©”ëª¨ë¦¬ (embedding_chat_memory.py)
ë²¡í„° ì„ë² ë”©ì„ í™œìš©í•œ ëŒ€í™” ê¸°ë¡ ê²€ìƒ‰ í”„ë¡œí† íƒ€ì…ì…ë‹ˆë‹¤.

```python
"""
ì„ë² ë”© ê¸°ë°˜ ì±„íŒ… ë©”ëª¨ë¦¬ í”„ë¡œí† íƒ€ì…
ë²¡í„° ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬í•œ ëŒ€í™” ê¸°ë¡ì„ ê²€ìƒ‰í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
"""
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Tuple
import json
import os

class EmbeddingChatMemory:
    """
    ì„ë² ë”© ê¸°ë°˜ ì±„íŒ… ë©”ëª¨ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤
    """
    def __init__(self, model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        """
        ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            model_name (str): ì‚¬ìš©í•  sentence-transformers ëª¨ë¸ëª…
        """
        self.embedding_model = SentenceTransformer(model_name)
        self.chat_embeddings: List[np.ndarray] = []
        self.chat_texts: List[str] = []
        self.chat_metadata: List[Dict] = []

    def add_chat(self, user_input: str, bot_response: str, metadata: Dict = None):
        """
        ëŒ€í™”ë¥¼ ë©”ëª¨ë¦¬ì— ì¶”ê°€í•˜ê³  ì„ë² ë”© ìƒì„±
        
        Args:
            user_input (str): ì‚¬ìš©ì ì…ë ¥
            bot_response (str): ë´‡ ì‘ë‹µ
            metadata (Dict): ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        """
        chat_text = f"User: {user_input}\nBot: {bot_response}"
        embedding = self.embedding_model.encode(chat_text)
        
        self.chat_embeddings.append(embedding)
        self.chat_texts.append(chat_text)
        self.chat_metadata.append(metadata or {})

    def search_similar_chats(self, query: str, top_k: int = 5) -> List[Tuple[str, float, Dict]]:
        """
        ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ëŒ€í™” ê¸°ë¡ ê²€ìƒ‰
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            top_k (int): ë°˜í™˜í•  ìƒìœ„ kê°œ ê²°ê³¼
            
        Returns:
            List[Tuple[str, float, Dict]]: (ëŒ€í™”í…ìŠ¤íŠ¸, ìœ ì‚¬ë„ì ìˆ˜, ë©”íƒ€ë°ì´í„°) ë¦¬ìŠ¤íŠ¸
        """
        if not self.chat_embeddings:
            return []
        
        query_embedding = self.embedding_model.encode(query)
        similarities = []
        
        for i, chat_embedding in enumerate(self.chat_embeddings):
            similarity = np.dot(query_embedding, chat_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(chat_embedding)
            )
            similarities.append((self.chat_texts[i], similarity, self.chat_metadata[i]))
        
        # ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]

    def get_context_for_response(self, query: str, threshold: float = 0.3) -> str:
        """
        ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
        
        Args:
            query (str): ì‚¬ìš©ì ì¿¼ë¦¬
            threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            str: ê´€ë ¨ ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ ì»¨í…ìŠ¤íŠ¸
        """
        similar_chats = self.search_similar_chats(query, top_k=3)
        relevant_chats = [chat for chat in similar_chats if chat[1] >= threshold]
        
        if not relevant_chats:
            return ""
        
        context = "ê´€ë ¨ ëŒ€í™” ê¸°ë¡:\n"
        for chat_text, similarity, metadata in relevant_chats:
            context += f"(ìœ ì‚¬ë„: {similarity:.2f}) {chat_text}\n\n"
        
        return context

    def save_memory(self, filepath: str):
        """ë©”ëª¨ë¦¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        memory_data = {
            "chat_texts": self.chat_texts,
            "chat_embeddings": [emb.tolist() for emb in self.chat_embeddings],
            "chat_metadata": self.chat_metadata
        }
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(memory_data, f, ensure_ascii=False, indent=2)

    def load_memory(self, filepath: str):
        """íŒŒì¼ì—ì„œ ë©”ëª¨ë¦¬ ë¡œë“œ"""
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                memory_data = json.load(f)
            
            self.chat_texts = memory_data["chat_texts"]
            self.chat_embeddings = [np.array(emb) for emb in memory_data["chat_embeddings"]]
            self.chat_metadata = memory_data["chat_metadata"]
```

#### ê³µìœ  í”„ë¡œí† íƒ€ì… ì„¤ì • (shared.py)
í”„ë¡œí† íƒ€ì… ê°„ ê³µìœ ë˜ëŠ” ì„¤ì • ë° ìœ í‹¸ë¦¬í‹°ì…ë‹ˆë‹¤.

```python
"""
í”„ë¡œí† íƒ€ì… ê³µìœ  ì„¤ì • ë° ìœ í‹¸ë¦¬í‹°
"""
import os
from typing import Dict, Any

# ê³µí†µ ì„¤ì •
PROTOTYPE_CONFIG = {
    "model_paths": {
        "bllossom": "fastapi/ai_model/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf",
        "lumimaid": "fastapi/ai_model/v2-Llama-3-Lumimaid-8B-v0.1-OAS-Q5_K_S-imat.gguf"
    },
    "embedding_model": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "memory_save_path": "./prototypes/memory/",
    "log_level": "INFO",
    "cuda_devices": {
        "bllossom": 1,  # RTX 2080
        "lumimaid": 0   # RTX 3060
    }
}

def get_prototype_config() -> Dict[str, Any]:
    """í”„ë¡œí† íƒ€ì… ì„¤ì • ë°˜í™˜"""
    return PROTOTYPE_CONFIG.copy()

def ensure_memory_directory():
    """ë©”ëª¨ë¦¬ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
    memory_path = PROTOTYPE_CONFIG["memory_save_path"]
    os.makedirs(memory_path, exist_ok=True)
    return memory_path

class PrototypeLogger:
    """í”„ë¡œí† íƒ€ì…ìš© ê°„ë‹¨í•œ ë¡œê±°"""
    def __init__(self, name: str):
        self.name = name
    
    def info(self, message: str):
        print(f"[{self.name}] INFO: {message}")
    
    def warning(self, message: str):
        print(f"[{self.name}] WARNING: {message}")
    
    def error(self, message: str):
        print(f"[{self.name}] ERROR: {message}")
```

### 4. ë°ì´í„° ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ (chat_schema.py) - v1.5.x ë¼ìš°í„° ê¸°ë°˜ ì¬ì„¤ê³„

#### ë¼ìš°í„°ë³„ ìš”ì²­/ì‘ë‹µ ëª¨ë¸
ê¸°ì¡´ì˜ ë‹¨ì¼ ëª¨ë¸ì—ì„œ ë¼ìš°í„°ë³„ ì „ìš© ëª¨ë¸ë¡œ ë¶„ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.

```python
class office_Request(BaseModel):
    """
    Office ë¼ìš°í„° API ìš”ì²­ - v1.5.x ì‹ ê·œ
    ì—…ë¬´ìš© AI ì‘ë‹µì„ ìœ„í•œ ìš”ì²­ ëª¨ë¸
    """
    input_data: str = office_input_data_set
    google_access: bool = google_access_set
    db_id: str | None = db_id_set
    user_id: str | None = user_id_set

class character_Request(BaseModel):
    """
    Character ë¼ìš°í„° API ìš”ì²­ - v1.5.x ì‹ ê·œ
    ìºë¦­í„° ê¸°ë°˜ ëŒ€í™”ë¥¼ ìœ„í•œ ìš”ì²­ ëª¨ë¸
    """
    input_data: str = character_input_data_set
    character_name: str = character_name_set
    greeting: str = greeting_set
    context: str = context_set
    db_id: str | None = db_id_set
    user_id: str | None = user_id_set

# v1.4.x í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ ìœ ì§€
Bllossom_Request = office_Request
Lumimaid_Request = character_Request
```

## UML ë‹¤ì´ì–´ê·¸ë¨ ì‹œìŠ¤í…œ

### í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨
v1.5.xì—ì„œ ë„ì…ëœ UML í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ ì‹œìŠ¤í…œ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.

#### AI Models í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     AI Models Package                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BaseConfig (TypedDict)                                      â”‚
â”‚ + character_name: str                                       â”‚
â”‚ + character_setting: str                                    â”‚
â”‚ + greeting: str                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BllossomChatModel                                           â”‚
â”‚ + model_path: str                                           â”‚
â”‚ + character_info: OfficePrompt                              â”‚
â”‚ + config: LlamaGenerationConfig                             â”‚
â”‚ + generate_response(): str                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LumimaidChatModel                                           â”‚
â”‚ + model_path: str                                           â”‚
â”‚ + character_info: CharacterPrompt                           â”‚
â”‚ + config: LlamaGenerationConfig                             â”‚
â”‚ + generate_response(): str                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OpenAIChatModel                                             â”‚
â”‚ + client: openai.OpenAI                                     â”‚
â”‚ + model_id: str                                             â”‚
â”‚ + generate_response(): str                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OpenAICharacterModel                                        â”‚
â”‚ + client: openai.OpenAI                                     â”‚
â”‚ + model_id: str                                             â”‚
â”‚ + generate_response(): str                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Handlers í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Handlers Package                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MongoDBHandler                                              â”‚
â”‚ + client: AsyncIOMotorClient                                â”‚
â”‚ + db: AsyncIOMotorDatabase                                  â”‚
â”‚ + get_office_log(): List[Dict]                              â”‚
â”‚ + get_character_log(): List[Dict]                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LanguageProcessor                                           â”‚
â”‚ + translate(): str                                          â”‚
â”‚ + detect_language(): str                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ChatError                                                   â”‚
â”‚ + add_exception_handlers(): None                            â”‚
â”‚ + generic_exception_handler(): JSONResponse                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Schemas í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Schemas Package                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ office_Request (BaseModel)                                  â”‚
â”‚ + input_data: str                                           â”‚
â”‚ + google_access: bool                                       â”‚
â”‚ + db_id: str | None                                         â”‚
â”‚ + user_id: str | None                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ character_Request (BaseModel)                               â”‚
â”‚ + input_data: str                                           â”‚
â”‚ + character_name: str                                       â”‚
â”‚ + greeting: str                                             â”‚
â”‚ + context: str                                              â”‚
â”‚ + db_id: str | None                                         â”‚
â”‚ + user_id: str | None                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### íŒ¨í‚¤ì§€ ë‹¤ì´ì–´ê·¸ë¨
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ChatBot AI v1.5.x                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚   FastAPI       â”‚  â”‚   Prototypes    â”‚  â”‚    Prompt     â”‚ â”‚
â”‚ â”‚   Server        â”‚  â”‚   Research      â”‚  â”‚    Config     â”‚ â”‚
â”‚ â”‚                 â”‚  â”‚                 â”‚  â”‚               â”‚ â”‚ 
â”‚ â”‚ â€¢ server.py     â”‚  â”‚ â€¢ embedding_    â”‚  â”‚ â€¢ config-     â”‚ â”‚
â”‚ â”‚ â€¢ routers       â”‚  â”‚   chat_memory   â”‚  â”‚   Llama.json  â”‚ â”‚
â”‚ â”‚ â€¢ middlewares   â”‚  â”‚ â€¢ shared.py     â”‚  â”‚ â€¢ config-     â”‚ â”‚
â”‚ â”‚                 â”‚  â”‚ â€¢ train/        â”‚  â”‚   OpenAI.json â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚          â”‚                     â”‚                   â”‚        â”‚
â”‚          â–¼                     â–¼                   â–¼        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚                     Utils Package                       â”‚ â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚ â”‚ â”‚ AI Models   â”‚ â”‚  Handlers   â”‚ â”‚     Services        â”‚ â”‚ â”‚
â”‚ â”‚ â”‚             â”‚ â”‚             â”‚ â”‚                     â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â€¢ GGUF      â”‚ â”‚ â€¢ MongoDB   â”‚ â”‚ â€¢ DuckDuckGo        â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â€¢ OpenAI    â”‚ â”‚ â€¢ Error     â”‚ â”‚   Search            â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â€¢ Shared    â”‚ â”‚ â€¢ Language  â”‚ â”‚                     â”‚ â”‚ â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚ â”‚ â”‚                   Schemas                           â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â€¢ office_Request  â€¢ character_Request               â”‚ â”‚ â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ì„¤ì¹˜ ë° ì„¤ì •

### í™˜ê²½ ì„¤ì • ì‹œìŠ¤í…œ - v1.5.x í†µí•©

#### ê°œì„ ëœ í™˜ê²½ êµ¬ì„±
OpenAI API í‚¤ì™€ í•¨ê»˜ CUDA í™˜ê²½ì„ ì§€ì›í•˜ëŠ” í†µí•© ì„¤ì¹˜ ê°€ì´ë“œì…ë‹ˆë‹¤.

**í™˜ê²½ ë³€ìˆ˜ ì„¤ì •:**
```bash
# .env íŒŒì¼ ì¶”ê°€ ë³€ìˆ˜
OPENAI_API_KEY=your_openai_api_key_here
LOCAL_HOST=192.168.3.0/24

# ê¸°ì¡´ ë³€ìˆ˜ ìœ ì§€
SESSION_KEY=default-secret
MONGO_HOST=localhost
MONGO_PORT=27017
# ... (ê¸°íƒ€ MongoDB ì„¤ì •)
```

**í•„ìˆ˜ íŒ¨í‚¤ì§€:**
```bash
# ê¸°ë³¸ íŒ¨í‚¤ì§€ (v1.5.x ì—…ë°ì´íŠ¸)
pip install torch==2.3.1+cu118 torchvision==0.18.1+cu118 torchaudio==2.3.1+cu118

# OpenAI API
pip install openai

# GGUF ì§€ì›
pip install llama-cpp-python[cuda]

# ê²€ìƒ‰ ë° ë²ˆì—­
pip install duckduckgo-search langchain-community deep-translator

# ì„ë² ë”© ë° ë²¡í„° ê²€ìƒ‰
pip install sentence-transformers

# ìì—°ì–´ ì²˜ë¦¬
pip install spacy
```

#### ë°°ì¹˜ íŒŒì¼ ê²½ë¡œ ì •ë¦¬
ë°°ì¹˜ íŒŒì¼ì´ `batchfile` â†’ `batch`ë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤.

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
./fastapi/batch/venv_setup.bat

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
./fastapi/batch/venv_install.bat

# ì„œë²„ ì‹¤í–‰
./.venv/Scripts/python.exe ./fastapi/src/server.py
```

### í”„ë¡œí† íƒ€ì… ê°œë°œ í™˜ê²½
ì—°êµ¬ ë° ì‹¤í—˜ì„ ìœ„í•œ í”„ë¡œí† íƒ€ì… ì‹¤í–‰ í™˜ê²½ì…ë‹ˆë‹¤.

```bash
# ì„ë² ë”© ì±„íŒ… ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
cd fastapi/src/prototypes
python embedding_chat_memory.py

# GGUF ëª¨ë¸ í…ŒìŠ¤íŠ¸
python GGUF_GPU.py

# íŒŒì¸íŠœë‹ ì‹¤í–‰
cd train
python train.py
```

## ì„±ëŠ¥ íŠ¹ì„±

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- **Bllossom (GGUF)**: ~8GB VRAM (RTX 2080)
- **Lumimaid (GGUF)**: ~6GB VRAM (RTX 3060)
- **OpenAI API**: ~0MB VRAM (ì™¸ë¶€ API)
- **ì„ë² ë”© ëª¨ë¸**: ~500MB RAM
- **MongoDB**: ~512MB RAM
- **ì‹œìŠ¤í…œ RAM**: ~8-10GB

### ì²˜ë¦¬ëŸ‰
- **ë™ì‹œ ìš”ì²­**: 5ê°œ (GGUF 2ê°œ + OpenAI 3ê°œ)
- **ì‹œê°„ë‹¹ ìš”ì²­**: ~800-1200ê°œ
- **OpenAI API**: ì‹¤ì‹œê°„ ì‘ë‹µ (~2-5ì´ˆ)
- **GGUF ì„±ëŠ¥**: v1.4.x ëŒ€ë¹„ ì¼ê´€ëœ ì„±ëŠ¥
- **ë¼ìš°í„° ì˜¤ë²„í—¤ë“œ**: ìµœì†Œ (~5ms)

### API ì‘ë‹µ ì‹œê°„
- **Office/Llama**: 10-30ì´ˆ (GGUF ë¡œì»¬)
- **Office/GPT**: 2-5ì´ˆ (OpenAI API)
- **Character/Llama**: 15-40ì´ˆ (GGUF ë¡œì»¬)
- **Character/GPT**: 3-7ì´ˆ (OpenAI API)

## ìš´ì˜ ê°€ì´ë“œ

### í™˜ê²½ ì„¤ì •
1. **Windows 11** + **Python 3.11** í™˜ê²½ êµ¬ì„±
2. **OpenAI API í‚¤** ë°œê¸‰ ë° ì„¤ì •
3. **CUDA 11.8/12.8** ë“œë¼ì´ë²„ ì„¤ì¹˜
4. **ë¡œì»¬ MongoDB** ì„¤ì¹˜ ë° êµ¬ì„±
5. **GGUF ëª¨ë¸** ë‹¤ìš´ë¡œë“œ ë° ë°°ì¹˜

### ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸
- **ë¼ìš°í„°ë³„ API** ì‚¬ìš©ëŸ‰ ë° ì„±ëŠ¥
- **OpenAI API** í˜¸ì¶œ íšŸìˆ˜ ë° ë¹„ìš©
- **GGUF ëª¨ë¸** ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
- **MongoDB** ëŒ€í™” ê¸°ë¡ ì €ì¥ ì„±ëŠ¥
- **ì„ë² ë”© ë©”ëª¨ë¦¬** ì„±ëŠ¥ (í”„ë¡œí† íƒ€ì…)

### ë¬¸ì œ í•´ê²°
- **OpenAI API ì˜¤ë¥˜**: API í‚¤ ìœ íš¨ì„± ë° í¬ë ˆë”§ í™•ì¸
- **ë¼ìš°í„° ë¼ìš°íŒ… ì‹¤íŒ¨**: ì—”ë“œí¬ì¸íŠ¸ ê²½ë¡œ ë° ëª¨ë¸ëª… í™•ì¸
- **GGUF ë¡œë”© ì‹¤íŒ¨**: GPU ë©”ëª¨ë¦¬ í• ë‹¹ í™•ì¸
- **í”„ë¡œí† íƒ€ì… ì˜¤ë¥˜**: ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸

### ì„±ëŠ¥ íŠœë‹
1. **ë¼ìš°í„° ìµœì í™”**
   - ëª¨ë¸ë³„ ìš”ì²­ ë¶„ì‚°
   - ìºì‹± ì „ëµ êµ¬í˜„
   - ë¹„ë™ê¸° ì²˜ë¦¬ í–¥ìƒ

2. **OpenAI API ìµœì í™”**
   - ë°°ì¹˜ ìš”ì²­ êµ¬í˜„
   - í† í° ì‚¬ìš©ëŸ‰ ìµœì í™”
   - ì‘ë‹µ ìºì‹± êµ¬í˜„

3. **í”„ë¡œí† íƒ€ì… í™œìš©**
   - ì„ë² ë”© ë©”ëª¨ë¦¬ë¡œ ì‘ë‹µ í’ˆì§ˆ í–¥ìƒ
   - íŒŒì¸íŠœë‹ìœ¼ë¡œ íŠ¹í™” ëª¨ë¸ ê°œë°œ
   - A/B í…ŒìŠ¤íŠ¸ë¡œ ì„±ëŠ¥ ë¹„êµ

## ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### í™•ì¥ëœ ë³´ì•ˆ ê¸°ëŠ¥
- **OpenAI API í‚¤**: í™˜ê²½ ë³€ìˆ˜ë¥¼ í†µí•œ ì•ˆì „í•œ ê´€ë¦¬
- **ë¼ìš°í„° ê¸°ë°˜ ì ‘ê·¼ ì œì–´**: ì—”ë“œí¬ì¸íŠ¸ë³„ ê¶Œí•œ ê´€ë¦¬
- **í”„ë¡œí† íƒ€ì… ë¶„ë¦¬**: ì‹¤í—˜ ì½”ë“œì™€ í”„ë¡œë•ì…˜ ì½”ë“œ ê²©ë¦¬

### API ë³´ì•ˆ ê°•í™”
- **ëª¨ë¸ë³„ ì ‘ê·¼ ì œì–´**: GGUF/OpenAI ëª¨ë¸ ë¶„ë¦¬ ê´€ë¦¬
- **ë¹„ìš© ëª¨ë‹ˆí„°ë§**: OpenAI API ì‚¬ìš©ëŸ‰ ì¶”ì 
- **ë¼ìš°í„° ë³´ì•ˆ**: ê²½ë¡œë³„ ì¸ì¦ ë° ê²€ì¦
- **í”„ë¡œí† íƒ€ì… ë³´ì•ˆ**: ì‹¤í—˜ ë°ì´í„° ë³´í˜¸
