# ğŸ“Œ ë²„ì „ íˆìŠ¤í† ë¦¬

| ë²„ì „      | ë‚ ì§œ                                   | ì»¤ë°‹ í•´ì‹œ                                 |
|-----------|----------------------------------------|-------------------------------------------|
| **v1.1.0**| 2025-01-15 (ìˆ˜) 15:40:49 (KST)         | `d014744640fa72366c398541ec9f5b4c7361fd7f`|


# ğŸ—‚ï¸ ì£¼ìš” íŒŒì¼ êµ¬ì¡°
```
ğŸ“¦src
 â”£ ğŸ“‚log
 â”£ ğŸ“‚utils
 â”ƒ â”£ ğŸ“œAI_Bllossom_8B.py
 â”ƒ â”£ ğŸ“œAI_Llama_8B.py
 â”ƒ â”£ ğŸ“œBaseModels.py
 â”ƒ â”£ ğŸ“œError_handlers.py
 â”ƒ â”— ğŸ“œ__init__.py
 â”£ ğŸ“œbot.yaml
 â”£ ğŸ“œindex.html
 â”— ğŸ“œserver.py
```

# ğŸ“„ `__init__.py`
## ğŸ“Œ íŒ¨í‚¤ì§€
|íŒ¨í‚¤ì§€ ëª…ì¹­|ì„¤ëª…|
|:-----:|:-----|
|`utils`|AI ëª¨ë¸, ì˜ˆì™¸ ì²˜ë¦¬, ì–¸ì–´ ì²˜ë¦¬ ë“±ì„ í¬í•¨í•˜ëŠ” ìœ í‹¸ë¦¬í‹° íŒ¨í‚¤ì§€|

## ğŸ“Œ ëª¨ë“ˆ
|ëª¨ë“ˆ ëª…ì¹­|ë³„ì¹­|ì„¤ëª…|
|:-----:|:-----:|:-----|
|`AI_Bllossom_8B`|`Bllossom_8B`|Bllossom 8B ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ìƒì„±í•˜ëŠ” BllossomChatModel í´ë˜ìŠ¤ë¥¼ ì •ì˜|
|`AI_Llama_8B`|`Llama_8B`|Llama 8B ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ìƒì„±í•˜ëŠ” LlamaChatModel í´ë˜ìŠ¤ë¥¼ ì •ì˜|
|`BaseModels`|`ChatModel`|FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš©ë˜ëŠ” Pydantic ëª¨ë¸ì„ ì •ì˜|
|`Error_handlers`|`ChatError`|FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ëª¨ë“ˆ|
|`Language_handler`|`LanguageProcessor`|ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ LanguageProcessor í´ë˜ìŠ¤ë¥¼ ì •ì˜|

# ğŸ“„ `server.py`
## ğŸ“Œ Lifespan ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
> FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì‹œì‘ê³¼ ì¢…ë£Œ ì‹œì ì— ì‹¤í–‰ë˜ëŠ” ì½”ë“œë¥¼ ì •ì˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ì˜ˆì œì—ì„œëŠ” Llama ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ì¢…ë£Œí•  ë•Œ í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.

```python
from utils import Llama_8B, Bllossom_8B

@asynccontextmanager
async def lifespan(app: FastAPI):
    '''
    FastAPI AI ëª¨ë¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”
    '''
    global llama_model_8b, bllossom_model_8b

    def get_cuda_device_info(device_id: int) -> str:
        device_name = torch.cuda.get_device_name(device_id)
        device_properties = torch.cuda.get_device_properties(device_id)
        total_memory = device_properties.total_memory / (1024 ** 3)
        return f"Device {device_id}: {device_name} (Total Memory: {total_memory:.2f} GB)"

    llama_model_8b = Llama_8B()
    bllossom_model_8b = Bllossom_8B()

    llama_device_info = get_cuda_device_info(0)
    bllossom_device_info = get_cuda_device_info(1)

    print(f"Llama ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({llama_device_info})")
    print(f"Bllossom ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({bllossom_device_info})")

    yield

    llama_model_8b = None
    bllossom_model_8b = None
    print("ëª¨ë¸ í•´ì œ ì™„ë£Œ")
```

## ğŸ“Œ API
|API ëª…ì¹­|ì„¤ëª…|
|:-----:|:-----|
|`/Llama_stream`|Llama ëª¨ë¸ì— ëŒ€í•œ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸|
|`/Bllossom_stream`|Bllossom ëª¨ë¸ì— ëŒ€í•œ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸|

```python
@app.post("/Llama_stream", summary="AI ëª¨ë¸ì´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ ìƒì„±")
async def Llama_stream(request: ChatModel.Llama_Request):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ìœ„í‚¤ë°±ê³¼, ë‚˜ë¬´ìœ„í‚¤, ë‰´ìŠ¤ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ AI ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    :param request: ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì˜µì…˜ í¬í•¨
    :return: AI ëª¨ë¸ì˜ ë‹µë³€
    """
    print(f"Request: {request}")
    try:
        search_context = ""

        if request.google_access:
            search_results = await fetch_search_results(request.input_data, num_results=5)

            if search_results:
                search_context = "\n".join([
                    f"ì œëª©: {item['title']}\nì„¤ëª…: {item['snippet']}\në§í¬: {item['link']}"
                    for item in search_results[:5]
                ])
            else:
                search_context = ""
        
        print(f"Search Context: {search_context}")

        prompt = (
            f"ì‚¬ìš©ì ì§ˆë¬¸ì€ {request.input_data}\n\n"
            f"ì°¸ê³  ì •ë³´ëŠ” {search_context}\n\n"
        )

        response_stream = llama_model_8b.generate_response_stream(input_text=prompt)
        return StreamingResponse(response_stream, media_type="text/plain")

    except TimeoutError:
        raise ChatError.InternalServerErrorException(detail="ëª¨ë¸ ì‘ë‹µì´ ì‹œê°„ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"Unhandled Exception: {e}")
        raise ChatError.InternalServerErrorException(detail=str(e))

@app.post("/Bllossom_stream", summary="ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ Bllossom_8B ëª¨ë¸ ë‹µë³€ ìƒì„±")
async def bllossom_stream(request: ChatModel.Bllossom_Request):
    '''
    Bllossom_8B ëª¨ë¸ì— ì§ˆë¬¸ ì…ë ¥ ì‹œ ìºë¦­í„° ì„¤ì •ì„ ë°˜ì˜í•˜ì—¬ ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°˜í™˜
    '''
    try:
        character_settings = {
            "character_name": request.character_name,
            "description": request.description,
            "greeting": request.greeting,
            "character_setting": request.character_setting,
            "tone": request.tone,
            "energy_level": request.energy_level,
            "politeness": request.politeness,
            "humor": request.humor,
            "assertiveness": request.assertiveness,
            "access_level": request.access_level
        }
        
        response_stream = bllossom_model_8b.generate_response_stream(
            input_text=request.input_data,
            character_settings=character_settings
        )
        return StreamingResponse(response_stream, media_type="text/plain")
    except TimeoutError:
        raise ChatError.InternalServerErrorException(detail="Bllossom ëª¨ë¸ ì‘ë‹µì´ ì‹œê°„ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except ValidationError as e:
        raise ChatError.BadRequestException(detail=str(e))
    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"Unhandled Exception: {e}")
        raise ChatError.InternalServerErrorException(detail=str(e))
```

---

# ğŸ“„ `AI_Llama_8B.py`

## ğŸ“Œ í´ë˜ìŠ¤

|í´ë˜ìŠ¤ ëª…ì¹­|ì„¤ëª…|
|:-----:|:-----|
|`LlamaChatModel`|Llama ëª¨ë¸ì„ GPUì— í• ë‹¹í•˜ì—¬ ì§ˆì˜ì‘ë‹µ í•˜ë„ë¡ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤|

## ğŸ“Œ í•¨ìˆ˜/ë©”ì„œë“œ

- `__init__`
    > í´ë˜ìŠ¤ ì´ˆê¸°í™” ë©”ì„œë“œ, Llama ëª¨ë¸ì˜ íŒŒì¼ ì£¼ì†Œì™€ GPU 0ì— ëª¨ë¸ì„ í• ë‹¹í•˜ì—¬ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”, í† í¬ë‚˜ì´ì €ì™€ GPUì˜ íŒŒì´í”„ë¼ì¸ ë¡œë“œ, ëª¨ë¸ì˜ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì¤€ë¹„í•˜ëŠ” ê³¼ì •ì„ ì²˜ë¦¬í•˜ëŠ” ë©”ì„œë“œ

    ```python
    def __init__(self):
        '''
        LlamaChatModel í´ë˜ìŠ¤ ì´ˆê¸°í™”
        '''
        current_dir = os.path.dirname(os.path.abspath(__file__))
        parent_dir = os.path.dirname(current_dir)
        dotenv_path = os.path.join(parent_dir, '.env')
        load_dotenv(dotenv_path)
        self.cache_dir = "/app/ai_model/"
        self.model_id = "meta-llama/Llama-3.1-8B-Instruct"
        self.device = torch.device("cuda:0")

        self.model_kwargs = {
            "torch_dtype": torch.float16,
            "trust_remote_code": True,
            "device_map": {"": self.device},
            "quantization_config": BitsAndBytesConfig(
                load_in_4bit=True,
                double_quant=True,
                compute_dtype=torch.float16
            )
        }

        self.hf_token = os.getenv("HUGGING_FACE_TOKEN")
        self.accelerator = Accelerator(mixed_precision="fp16", device_placement=False)
        self.scaler = GradScaler()

        print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
        self.tokenizer = self.load_tokenizer()
        print("ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.model = self.load_model()
        print("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!")
        
        self.model.gradient_checkpointing_enable()
        self.conversation_history = []
    ```

- `load_tokenizer`
    > `transformers.AutoTokenizer`ì— Llama ëª¨ë¸(`self.model_id`)ì„ í• ë‹¹í•œ ë³€ìˆ˜(`tokenizer`)ë¥¼ ë°˜í™˜

    ```python
    def load_tokenizer(self) -> transformers.PreTrainedTokenizerBase:
        '''
        í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        :return: ë¡œë“œëœ í† í¬ë‚˜ì´ì €
        '''
        tokenizer = transformers.AutoTokenizer.from_pretrained(
            self.model_id,
            token=self.hf_token
        )
        if tokenizer.eos_token_id is None:
            tokenizer.add_special_tokens({'eos_token': '<|endoftext|>'})
        tokenizer.pad_token_id = tokenizer.eos_token_id
        return tokenizer
    ```

- `load_model`
    > `transformers.AutoModelForCausalLM`ì— Llama ëª¨ë¸(`self.model_id`)ì„ í• ë‹¹í•œ ë³€ìˆ˜(`model`)ë¥¼ ë°˜í™˜

    ```python
    def load_model(self) -> transformers.PreTrainedModel:
        '''
        Llama ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        :return: ë¡œë“œëœ Llama ëª¨ë¸
        '''
        model = transformers.AutoModelForCausalLM.from_pretrained(
            self.model_id,
            cache_dir=self.cache_dir,
            token=self.hf_token,
            **self.model_kwargs
        )
        return model

    ```

- `generate_response_stream`
    > `input_text: str`ë¥¼ ë©”ê²Œë³€ìˆ˜ë¡œ ë°›ì•„ì™€ì„œ í† í¬ë‚˜ì´ì €ë¡œ ì¸ì½”ë”©í•˜ì—¬ llama ëª¨ë¸ì— ì„¤ì • ê°’ë“¤ê³¼ í•¨ê»˜ ì „ì†¡, ë°›ì•„ì˜¨ ë¬¸ì¥ì„ ë””ì½”ë”©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜í•œë‹¤.
    ```python
    def generate_response_stream(self, input_text: str):
        """
        Llama ëª¨ë¸ì— ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬í•˜ê³ , ìƒì„±ëœ ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        :param input_text: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
        :return: ìƒì„±ëœ ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜
        """
        input_ids = self.tokenizer.encode(
            text=input_text,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(self.device)
        attention_mask = (input_ids != self.tokenizer.pad_token_id).long().to(self.device)
        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True)

        generation_kwargs = {
            "input_ids": input_ids.to(self.device),
            "attention_mask": attention_mask.to(self.device),
            "min_new_tokens": 1,
            "max_new_tokens": 512,
            "do_sample": True,          # ìƒ˜í”Œë§ í™œì„±í™”
            "top_p": 0.9,               # Top-p Sampling í™œì„±í™” (0.9ë¡œ ì„¤ì •)
            "top_k": 0,                 # Top-k ë¹„í™œì„±í™” (Top-pì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            "temperature": 0.7,         # ë‹¤ì–‘ì„±ì„ ìœ„í•œ ì˜¨ë„ ì¡°ì •
            "eos_token_id": self.tokenizer.eos_token_id,
            "pad_token_id": (
                self.tokenizer.pad_token_id
                if self.tokenizer.pad_token_id is not None
                else self.tokenizer.eos_token_id
            ),
            "repetition_penalty": 1.2,  # ë°˜ë³µ ë°©ì§€ íŒ¨ë„í‹°
            "num_return_sequences": 1,  # í•œ ë²ˆì— í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ ìƒì„±
            "streamer": streamer        # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë¨¸ ì„¤ì •
        }

        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
        thread.start()

        for new_text in streamer:
            yield new_text
    ```
---

# ğŸ“„ `Bllossom_8B.py`
## ğŸ“Œ í´ë˜ìŠ¤
|í´ë˜ìŠ¤ ëª…ì¹­|ì„¤ëª…|
|:-----:|:-----|
|`BllossomChatModel`|Bllossom ëª¨ë¸ì„ GPUì— í• ë‹¹í•˜ì—¬ ì§ˆì˜ì‘ë‹µ í•˜ë„ë¡ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤|


## ğŸ“Œ í•¨ìˆ˜/ë©”ì„œë“œ

- `__init__`
    > í´ë˜ìŠ¤ ì´ˆê¸°í™” ë©”ì„œë“œ, Llama-Bllossom ëª¨ë¸ì˜ íŒŒì¼ ì£¼ì†Œì™€ GPU 1ì— ëª¨ë¸ì„ í• ë‹¹í•˜ì—¬ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”, í† í¬ë‚˜ì´ì €ì™€ GPUì˜ íŒŒì´í”„ë¼ì¸ ë¡œë“œ, ëª¨ë¸ì˜ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì¤€ë¹„í•˜ëŠ” ê³¼ì •ì„ ì²˜ë¦¬í•˜ëŠ” ë©”ì„œë“œ

    ```python
    def __init__(self):
        '''
        BllossomChatModel í´ë˜ìŠ¤ ì´ˆê¸°í™”
        '''
        current_dir = os.path.dirname(os.path.abspath(__file__))
        parent_dir = os.path.dirname(current_dir)
        dotenv_path = os.path.join(parent_dir, '.env')
        load_dotenv(dotenv_path)
        self.cache_dir = "/app/ai_model/"
        self.model_id = "MLP-KTLim/llama-3-Korean-Bllossom-8B"
        self.device = torch.device("cuda:1")

        self.model_kwargs = {
            "torch_dtype": torch.float16,
            "trust_remote_code": True,
            "device_map": {"": self.device},
            "quantization_config": BitsAndBytesConfig(load_in_4bit=True)
        }

        self.hf_token = os.getenv("HUGGING_FACE_TOKEN")
        self.accelerator = Accelerator(mixed_precision="fp16", device_placement=False)
        self.scaler = GradScaler()

        print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
        self.tokenizer = self.load_tokenizer()
        print("ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.model = self.load_model()
        print("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!")

        self.model.gradient_checkpointing_enable()
        self.conversation_history = []
    ```

- `load_tokenizer`
    > `transformers.AutoTokenizer`ì— Llama ëª¨ë¸(`self.model_id`)ì„ í• ë‹¹í•œ ë³€ìˆ˜(`tokenizer`)ë¥¼ ë°˜í™˜

    ```python
    def load_tokenizer(self) -> transformers.PreTrainedTokenizerBase:
        '''
        í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        :return: ë¡œë“œëœ í† í¬ë‚˜ì´ì €
        '''
        tokenizer = transformers.AutoTokenizer.from_pretrained(
            self.model_id,
            token=self.hf_token
        )
        tokenizer.pad_token_id = tokenizer.eos_token_id
        return tokenizer
    ```

- `load_model`
    > `transformers.AutoModelForCausalLM`ì— Llama ëª¨ë¸(`self.model_id`)ì„ í• ë‹¹í•œ ë³€ìˆ˜(`model`)ë¥¼ ë°˜í™˜

    ```python
    def load_model(self) -> transformers.PreTrainedModel:
        '''
        Llama ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        :return: ë¡œë“œëœ Llama ëª¨ë¸
        '''
        model = transformers.AutoModelForCausalLM.from_pretrained(
            self.model_id,
            cache_dir=self.cache_dir,
            token=self.hf_token,
            **self.model_kwargs
        )
        return model

    ```

- `generate_response_stream`
    > `input_text: str`ë¥¼ ë©”ê²Œë³€ìˆ˜ë¡œ ë°›ì•„ì™€ì„œ `_build_prompt`ë¥¼ í†µí•´ `prompt`ì„ ìƒì„±, í† í¬ë‚˜ì´ì €ë¡œ ì¸ì½”ë”©í•˜ì—¬ llama ëª¨ë¸ì— ì„¤ì • ê°’ë“¤ê³¼ í•¨ê»˜ ì „ì†¡, ë°›ì•„ì˜¨ ë¬¸ì¥ì„ ë””ì½”ë”©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜í•œë‹¤.
    ```python
    def generate_response_stream(self, input_text: str, character_settings: dict):
        prompt = self._build_prompt(input_text, character_settings)
        input_ids = self.tokenizer.encode(
            text=input_text,
            text_pair=prompt,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(self.device)
        attention_mask = (input_ids != self.tokenizer.pad_token_id).long().to(self.device)
        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True)

        generation_kwargs = {
            "input_ids": input_ids.to(self.device),
            "attention_mask": attention_mask.to(self.device),
            "min_new_tokens": 1,
            "max_new_tokens": 512,
            "do_sample": True,
            "temperature": 0.7,
            "top_k": 40,
            "top_p": 0.9,
            "eos_token_id": self.tokenizer.eos_token_id,
            "pad_token_id": self.tokenizer.eos_token_id,
            "repetition_penalty": 1.2,
            "streamer": streamer
        }

        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
        thread.start()

        for new_text in streamer:
            yield new_text
    ```

- `_build_prompt`
  - > Bllossom ëª¨ë¸ì˜ ìºë¦­í„° ì„¤ì •ì„ ë°˜ì˜í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë©”ì„œë“œë¡œ, ì…ë ¥ í…ìŠ¤íŠ¸ì™€ ìºë¦­í„° ì„¤ì •ì„ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

    ```python
    def _build_prompt(self, user_input: str, character_settings: dict):
        """
        ëŒ€í™” ê¸°ë¡ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        """
        recent_history = self.conversation_history[-5:]  # ìµœê·¼ 5ê°œì˜ ëŒ€í™”ë§Œ ìœ ì§€
        history = "\n".join([f"{entry['role']}: {entry['content']}" for entry in recent_history])
        prompt = (
            "ìºë¦­í„° ì„¤ì •:\n"
            f"ì´ë¦„: {character_settings['character_name']}\n"
            f"ì„¤ëª…: {character_settings['description']}\n"
            f"ì¸ì‚¬ë§: {character_settings['greeting']}\n"
            f"ì„±ê²©: {character_settings['character_setting']}\n"
            f"ë§íˆ¬: {character_settings['tone']}\n"
            f"ì—ë„ˆì§€ ë ˆë²¨: {character_settings['energy_level']}\n"
            f"ê³µì†í•¨: {character_settings['politeness']}\n"
            f"ìœ ë¨¸ ê°ê°: {character_settings['humor']}\n"
            f"ë‹¨í˜¸í•¨: {character_settings['assertiveness']}\n"
            f"ì•¡ì„¸ìŠ¤ ìˆ˜ì¤€: {'í—ˆìš©ë¨' if character_settings['access_level'] else 'ì œí•œë¨'}\n\n"
            "ë¶€ì • ë¼ë²¨:\n"
            "ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ ì œê³µë©ë‹ˆë‹¤."
            "ë‹µë³€ì—ì„œ max_new_tokensì˜ ê°’ì„ ë‹¤ ì±„ì›Œì„œ ë‹µë³€ í•  í•„ìš” ì—†ìŒ."
            "'ìºë¦­í„° ì„¤ì •' ë˜ëŠ” ì„¤ì • ì„¸ë¶€ ì •ë³´ëŠ” ì‘ë‹µì— í¬í•¨ë˜ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤."
            "ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤."
            "í•„ìš” ì´ìƒì˜ ë°˜ë³µì„ í”¼í•˜ì‹­ì‹œì˜¤."
            "ì§ˆë¬¸ì— ëŒ€í•´ ì ì ˆíˆ ì‘ë‹µí•˜ë©°, ëŒ€ë‹µì´ ì—†ì„ ê²½ìš° 'ì£„ì†¡í•©ë‹ˆë‹¤, ì´ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤."
            f"ëŒ€í™” ê¸°ë¡: {history}\n"
        )
        self.conversation_history.append({"role": "user", "content": user_input})
        return prompt
---

# ğŸ“„ `Error_handlers.py`
## ğŸ“Œ í´ë˜ìŠ¤

|í´ë˜ìŠ¤ ëª…ì¹­|ì„¤ëª…|
|:-----:|:-----|
|`NotFoundException`|HTTP 404 ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì™¸ í´ë˜ìŠ¤. ìš”ì²­í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ ë°œìƒ|
|`BadRequestException`|HTTP 400 ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì™¸ í´ë˜ìŠ¤. í´ë¼ì´ì–¸íŠ¸ì˜ ì˜ëª»ëœ ìš”ì²­ìœ¼ë¡œ ì¸í•´ ë°œìƒ|
|`UnauthorizedException`|HTTP 401 ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì™¸ í´ë˜ìŠ¤. ì¸ì¦ë˜ì§€ ì•Šì€ ì‚¬ìš©ìì˜ ì ‘ê·¼ ì‹œë„ ì‹œ ë°œìƒ|
|`ForbiddenException`|HTTP 403 ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì™¸ í´ë˜ìŠ¤. ê¶Œí•œì´ ì—†ëŠ” ë¦¬ì†ŒìŠ¤ì— ëŒ€í•œ ì ‘ê·¼ ì‹œë„ ì‹œ ë°œìƒ|
|`ValueErrorException`|HTTP 422 ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì™¸ í´ë˜ìŠ¤. ì…ë ¥ê°’ì˜ í˜•ì‹ì´ë‚˜ íƒ€ì…ì´ ì˜¬ë°”ë¥´ì§€ ì•Šì„ ë•Œ ë°œìƒ|
|`InternalServerErrorException`|HTTP 500 ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì™¸ í´ë˜ìŠ¤. ì„œë²„ ë‚´ë¶€ì—ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì„ ë•Œ ì‚¬ìš©|
|`DatabaseErrorException`|HTTP 503 ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì™¸ í´ë˜ìŠ¤. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨ë‚˜ ì¿¼ë¦¬ ì˜¤ë¥˜ ë“± DB ê´€ë ¨ ë¬¸ì œ ë°œìƒ ì‹œ ì‚¬ìš©|
|`IPRestrictedException`|HTTP 403 ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì™¸ í´ë˜ìŠ¤. í—ˆìš©ë˜ì§€ ì•Šì€ IP ì£¼ì†Œì—ì„œì˜ ì ‘ê·¼ ì‹œë„ ì‹œ ë°œìƒ|
|`MethodNotAllowedException`|HTTP 405 ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì™¸ í´ë˜ìŠ¤. ì§€ì›í•˜ì§€ ì•ŠëŠ” HTTP ë©”ì„œë“œë¡œ ìš”ì²­í–ˆì„ ë•Œ ë°œìƒ|

## ğŸ“Œ í•¨ìˆ˜/ë©”ì„œë“œ

- `generic_exception_handler`
    > FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë°œìƒí•œ HTTPExceptionì„ ì²˜ë¦¬í•˜ë©°, ìš”ì²­ ì •ë³´ì™€ ì˜ˆì™¸ì— ëŒ€í•œ ì„¸ë¶€ ì‚¬í•­ì„ ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤.

    ```python
    async def generic_exception_handler(request: Request, exc: HTTPException) -> JSONResponse:
        """
        FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë°œìƒí•œ HTTPExceptionì„ ì²˜ë¦¬í•˜ë©°,
        ìš”ì²­ ì •ë³´ì™€ ì˜ˆì™¸ì— ëŒ€í•œ ì„¸ë¶€ ì‚¬í•­ì„ ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤.
        """
        handler = exception_handlers.get(type(exc), None)
        
        body = await request.body()

        log_data = {
            "url": str(request.url),
            "method": request.method,
            "headers": dict(request.headers),
            "body": body.decode("utf-8") if body else "",
            "exception_class": exc.__class__.__name__,
            "detail": exc.detail
        }

        error_message = f"{exc.status_code} - {exc.detail}"
        logger.error(f"{error_message} | URL: {log_data['url']} | Method: {log_data['method']}")

        if handler:
            return handler(request, exc)
        else:
            return JSONResponse(
                status_code=500,
                content={"detail": "An unexpected error occurred."},
            )
    ```

- `add_exception_handlers`
    > FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì˜ˆì™¸ í•¸ë“¤ëŸ¬ë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜. ì •ì˜ëœ ì˜ˆì™¸ íƒ€ì…ê³¼ ê´€ë ¨ëœ í•¸ë“¤ëŸ¬ë¥¼ FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì— ë“±ë¡í•©ë‹ˆë‹¤.

    ```python
    def add_exception_handlers(app: FastAPI):
        """
        FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì˜ˆì™¸ í•¸ë“¤ëŸ¬ë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜.
        ì •ì˜ëœ ì˜ˆì™¸ íƒ€ì…ê³¼ ê´€ë ¨ëœ í•¸ë“¤ëŸ¬ë¥¼ FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì— ë“±ë¡í•©ë‹ˆë‹¤.
        """
        for exc_type in exception_handlers:
            app.add_exception_handler(exc_type, generic_exception_handler)
    ```

---

# ğŸ“„ `BaseModels.py`

## ğŸ“Œ í´ë˜ìŠ¤
|í´ë˜ìŠ¤ ëª…ì¹­|ì„¤ëª…|
|:-----:|:-----|
|`Validators`|URL í˜•ì‹ ê²€ì¦ ë° ì´ë¯¸ì§€ URL ì—°ê²° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤|
|`Llama_Request`|Llamaì˜ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” Pydantic ëª¨ë¸. ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨|
|`Llama_Response`|Llamaì˜ ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” Pydantic ëª¨ë¸. ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨|
|`Bllossom_Request`|Bllossomì˜ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” Pydantic ëª¨ë¸. ì…ë ¥ í…ìŠ¤íŠ¸ì™€ ìºë¦­í„° ì„¤ì •ì„ í¬í•¨|
|`Bllossom_Response`|Bllossomì˜ ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” Pydantic ëª¨ë¸. ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨|

## ğŸ“Œ í•¨ìˆ˜/í•„ë“œ
- `Validators`
    > URL í˜•ì‹ ê²€ì¦ ë° ì´ë¯¸ì§€ URL ì—°ê²° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤
    ```python
    class Validators:
        @staticmethod
        def validate_URL(v: str) -> str:
            """
            URL í˜•ì‹ ê²€ì¦ í•¨ìˆ˜
            """
            url_pattern = re.compile(
                r'''
                ^                     # ë¬¸ìì—´ì˜ ì‹œì‘
                https?://             # http:// ë˜ëŠ” https://
                (drive\.google\.com)  # Google Drive ë„ë©”ì¸
                /thumbnail            # ê²½ë¡œì˜ ì¼ë¶€
                \?id=([a-zA-Z0-9_-]+) # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° id
                $                     # ë¬¸ìì—´ì˜ ë
                ''', re.VERBOSE
            )
            if not url_pattern.match(v):
                raise ValueError('ìœ íš¨í•œ URL í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.')
            return v

        @staticmethod
        async def check_img_url(img_url: str):
            '''
            URLì˜ ì—°ê²° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
            '''
            try:
                async with httpx.AsyncClient() as client:
                    response = await client.head(img_url, follow_redirects=True)
                if response.status_code != 200:
                    raise ValueError('ì´ë¯¸ì§€ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
            except httpx.RequestError:
                raise ValueError('ì´ë¯¸ì§€ URLì— ì ‘ê·¼í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.')
    ```

- `Field`
    > Pydantic ëª¨ë¸ í•„ë“œë¥¼ ì •ì˜í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜ë¡œ, ê° í•„ë“œì˜ ì˜ˆì‹œ, ì œëª©, ì„¤ëª…, ê¸¸ì´ ì œì•½ ë“±ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    ```python
    input_data_set = Field(
        examples=["Llama AI ëª¨ë¸ì˜ ì¶œì‹œì¼ê³¼ ë²„ì „ë“¤ì„ ê°ê° ì•Œë ¤ì¤˜."],
        title="ì‚¬ìš©ì ì…ë ¥ ë¬¸ì¥",
        description="ì‚¬ìš©ì ì…ë ¥ ë¬¸ì¥ ê¸¸ì´ ì œì•½",
        min_length=1, max_length=500
    )

    google_access_set = Field(
        examples=[False, True],
        default=False,
        title="ê²€ìƒ‰ ê¸°ë°˜ ì•¡ì„¸ìŠ¤",
        description="ê²€ìƒ‰ ê¸°ë°˜ ì•¡ì„¸ìŠ¤ ìˆ˜ì¤€ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. True: ê²€ìƒ‰ ê¸°ë°˜ í™œì„±í™”. False: ê²€ìƒ‰ ê¸°ë°˜ ì œí•œë¨."
    )

    NATURAL_NUM: int = conint(ge=1, le=10)  # 1~10 ë²”ìœ„ì˜ ìì—°ìˆ˜

    character_name_set = Field(
        examples=["KindBot"],
        title="ì¼€ë¦­í„° ì´ë¦„",
        description="ìºë¦­í„°ì˜ ì´ë¦„ì…ë‹ˆë‹¤. ë´‡ì˜ ì •ì²´ì„±ì„ ë‚˜íƒ€ë‚´ë©°, ì‚¬ìš©ìê°€ ì´ ì´ë¦„ìœ¼ë¡œ ë´‡ì„ ë¶€ë¦…ë‹ˆë‹¤.",
        min_length=1
    )
    description_set = Field(
        examples=["ì¹œì ˆí•œ ë„ìš°ë¯¸ ë´‡"],
        title="ì¼€ë¦­í„° ì„¤ëª…",
        description="ìºë¦­í„°ì˜ ì§§ì€ ì„¤ëª…ì…ë‹ˆë‹¤. ì´ ë´‡ì˜ ì„±ê²©ì´ë‚˜ ì—­í• ì„ ê°„ëµíˆ í‘œí˜„í•˜ë©°, ì‚¬ìš©ìì—ê²Œ ì²«ì¸ìƒì„ ì œê³µí•©ë‹ˆë‹¤.",
        min_length=1
    )
    greeting_set = Field(
        examples=["ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"],
        title="ì¼€ë¦­í„° ì¸ì‚¬ë§",
        description="ì‚¬ìš©ìê°€ ë´‡ê³¼ ìƒí˜¸ì‘ìš©ì„ ì‹œì‘í•  ë•Œ í‘œì‹œë˜ëŠ” ì¸ì‚¬ë§ì…ë‹ˆë‹¤. ë´‡ì˜ ì„±ê²©ê³¼ ì˜ë„ë¥¼ ë°˜ì˜í•©ë‹ˆë‹¤.",
        min_length=1
    )
    image_set = Field(
        examples=["https://drive.google.com/thumbnail?id=12PqUS6bj4eAO_fLDaWQmoq94-771xfim"],
        title="ì¼€ë¦­í„° ì´ë¯¸ì§€ URL",
        description="URLì˜ ìµœëŒ€ ê¸¸ì´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ 2048ì",
        min_length=1, max_length=2048
    )
    character_setting_set = Field(
        examples=["ì¹œì ˆí•˜ê³  ê³µì†í•œ ë´‡"],
        title="ì¼€ë¦­í„° ì„¤ì • ê°’",
        description="ìºë¦­í„°ì˜ ì„±ê²©ì´ë‚˜ íƒœë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ëŠ” ë´‡ì´ ëŒ€í™”ì—ì„œ ì–´ë–»ê²Œ í–‰ë™í•˜ê³  ì‘ë‹µí• ì§€ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.",
        min_length=1
    )
    tone_set = Field(
        examples=["ê³µì†í•œ"],
        title="ì¼€ë¦­í„° ë§íˆ¬",
        description="ëŒ€í™”ì˜ ì–´ì¡°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë´‡ì´ ëŒ€í™”ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´ ìŠ¤íƒ€ì¼ì´ë‚˜ íƒœë„ì…ë‹ˆë‹¤.",
        min_length=1
    )
    energy_level_set = Field(
        examples=[8],
        title="ì¼€ë¦­í„° ì—ë„ˆì§€ ",
        description="ë´‡ì˜ ì—ë„ˆì§€ ìˆ˜ì¤€ì„ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ìì…ë‹ˆë‹¤. ë†’ì€ ê°’ì¼ìˆ˜ë¡ í™œê¸°ì°¨ê³  ì ê·¹ì ì¸ ëŒ€í™”ë¥¼ í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 1(ë§¤ìš° ëŠê¸‹í•¨) ~ 10(ë§¤ìš° í™œê¸°ì°¸)."
    )
    politeness_set = Field(
        examples=[10],
        title="ì¼€ë¦­í„° ê³µì†í•¨",
        description="ë´‡ì˜ ê³µì†í•¨ì„ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ìì…ë‹ˆë‹¤. ë†’ì€ ê°’ì¼ìˆ˜ë¡ ê³µì†í•˜ê³  ì¡´ì¤‘í•˜ëŠ” ì–¸ì–´ë¥¼ ì‚¬ìš©í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. 1(ì§ì„¤ì ì„) ~ 10(ë§¤ìš° ê³µì†í•¨)"
    )
    humor_set = Field(
        examples=[5],
        title="ì¼€ë¦­í„° ìœ ë¨¸ ê°ê°",
        description="ë´‡ì˜ ìœ ë¨¸ ê°ê°ì˜ ì •ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ìˆ«ìê°€ ë†’ì„ìˆ˜ë¡ ëŒ€í™”ì—ì„œ ìœ ë¨¸ëŸ¬ìŠ¤í•œ ìš”ì†Œë¥¼ ì¶”ê°€í•˜ë ¤ê³  ì‹œë„í•©ë‹ˆë‹¤. 1(ìœ ë¨¸ ì—†ìŒ) ~ 10(ë§¤ìš° ìœ ë¨¸ëŸ¬ìŠ¤í•¨)."
    )
    assertiveness_set = Field(
        examples=[3],
        title="ì¼€ë¦­í„° ë‹¨í˜¸í•¨",
        description="ë´‡ì˜ ë‹¨í˜¸í•¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ìˆ«ìê°€ ë†’ì„ìˆ˜ë¡ ì£¼ì¥ì„ ê°•í•˜ê²Œ í•˜ê±°ë‚˜ ëª…í™•íˆ í‘œí˜„í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. 1(ë§¤ìš° ìœ ì—°í•¨) ~ 10(ë§¤ìš° ë‹¨í˜¸í•¨)."
    )
    access_level_set = Field(
        examples=[True, False],
        default=True,
        title="ì¼€ë¦­í„° ì•¡ì„¸ìŠ¤",
        description="ë´‡ì˜ ì•¡ì„¸ìŠ¤ ìˆ˜ì¤€ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. True: íŠ¹ì • ê¸°ëŠ¥ì´ë‚˜ ì˜ì—­ì— ëŒ€í•œ ì ‘ê·¼ ê¶Œí•œì´ í—ˆìš©ë¨. False: ì œí•œë¨."
    )

    output_data_set = Field(
        examples=['''
        ë¬¼ë¡ ì´ì£ ! Llama AI ëª¨ë¸ì˜ ì¶œì‹œì¼ê³¼ ë²„ì „ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

        1. Llama 1: 2023ë…„ ì¶œì‹œ1

        2. Llama 2: 2024ë…„ 6ì›” 1ì¼ ì¶œì‹œ2

        3. Llama 3: 2024ë…„ 7ì›” 23ì¼ ì¶œì‹œ3

        4. Llama 3.1: 2024ë…„ 7ì›” 24ì¼ ì¶œì‹œ4

        ì´ ëª¨ë¸ë“¤ì€ Meta (êµ¬ Facebook)ì—ì„œ ê°œë°œí•œ AI ëª¨ë¸ì…ë‹ˆë‹¤.
        ê° ë²„ì „ë§ˆë‹¤ ì„±ëŠ¥ê³¼ ê¸°ëŠ¥ì´ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤. ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”?
        '''],
        title="Llama ë‹µë³€"
    )
    ```

## ğŸ“Œ Body ëª¨ë¸
- `Llama_Request`
    > Llama ëª¨ë¸ì— ëŒ€í•œ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” Pydantic ëª¨ë¸ë¡œ, ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

    ```python
    class Llama_Request(BaseModel):
        input_data: str = input_data_set
        google_access: bool = google_access_set
    
    ```
    - ìš”ì²­ í˜•ì‹
        ```JSON
        {
            "input_data": "Llama AI ëª¨ë¸ì˜ ì¶œì‹œì¼ê³¼ ë²„ì „ë“¤ì„ ê°ê° ì•Œë ¤ì¤˜.",
            "google_access": false
        }
        ```
- `Llama_Response`
    > Llama ëª¨ë¸ì˜ ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” Pydantic ëª¨ë¸ë¡œ, ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

    ```python
    class Llama_Response(BaseModel):
        output_data: str = output_data_set
    ```
    - ì‘ë‹µ í˜•ì‹
        ```JSON
        {
            "output_text": "ë¬¼ë¡ ì´ì£ ! Llama AI ëª¨ë¸ì˜ ì¶œì‹œì¼ê³¼ ë²„ì „ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n1. Llama 1: 2023ë…„ ì¶œì‹œ\n\n2. Llama 2: 2024ë…„ 6ì›” 1ì¼ ì¶œì‹œ\n\n3. Llama 3: 2024ë…„ 7ì›” 23ì¼ ì¶œì‹œ\n\n4. Llama 3.1: 2024ë…„ 7ì›” 24ì¼ ì¶œì‹œ\n\nì´ ëª¨ë¸ë“¤ì€ Meta (êµ¬ Facebook)ì—ì„œ ê°œë°œí•œ AI ëª¨ë¸ì…ë‹ˆë‹¤.\nê° ë²„ì „ë§ˆë‹¤ ì„±ëŠ¥ê³¼ ê¸°ëŠ¥ì´ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤. ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”?"
        }
        ```

- `Bllossom_Request`
    > Bllossom ëª¨ë¸ì— ëŒ€í•œ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” Pydantic ëª¨ë¸ë¡œ, ì…ë ¥ í…ìŠ¤íŠ¸ì™€ ìºë¦­í„° ì„¤ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.
    ```python
    class Bllossom_Request(BaseModel):
        input_data: str = input_data_set
        character_name: str = character_name_set
        description: str = description_set
        greeting: str = greeting_set
        image: str = image_set
        character_setting: str = character_setting_set
        tone: str = tone_set
        energy_level: NATURAL_NUM = energy_level_set    # type: ignore
        politeness: NATURAL_NUM = politeness_set        # type: ignore
        humor: NATURAL_NUM = humor_set                  # type: ignore
        assertiveness: NATURAL_NUM = assertiveness_set  # type: ignore
        access_level: bool = access_level_set
        
        @field_validator('image', mode='before')
        def check_img_url(cls, v):
            return Validators.validate_URL(v)
        
        def model_dump(self, **kwargs):
            """
            Pydantic BaseModelì˜ dict() ë©”ì„œë“œë¥¼ ëŒ€ì²´í•˜ëŠ” model_dump() ë©”ì„œë“œì…ë‹ˆë‹¤.
            í•„í„°ë§ëœ ë°ì´í„°ë§Œ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            """
            return super().model_dump(**kwargs)
    
    ```
    - ìš”ì²­ í˜•ì‹
        ```JSON
        {
            "input_data": "ì•ˆë…•, ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?",
            "character_name": "KindBot",
            "description": "ì¹œì ˆí•œ ë„ìš°ë¯¸ ë´‡",
            "greeting": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
            "image": "https://drive.google.com/thumbnail?id=12PqUS6bj4eAO_fLDaWQmoq94-771xfim",
            "character_setting": "ì¹œì ˆí•˜ê³  ê³µì†í•œ ë´‡",
            "tone": "ê³µì†í•œ",
            "energy_level": 8,
            "politeness": 10,
            "humor": 5,
            "assertiveness": 3,
            "access_level": true
        }
        ```

- `Bllossom_Response`
    ```python
    class Bllossom_Response(BaseModel):
        output_data: str = output_data_set
    ```
    - ì‘ë‹µ í˜•ì‹
        ```JSON
        {
            "output_text": "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ë§‘ê³  ê¸°ì˜¨ì´ 25ë„ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
        }
        ```