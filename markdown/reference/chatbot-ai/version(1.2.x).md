# ChatBot AI - ë²„ì „ ëª…ì„¸ì„œ v1.2.x

## ê°œìš”
ì´ ë¬¸ì„œëŠ” ChatBot AI ì‹œìŠ¤í…œì˜ v1.2.x ê³„ì—´ ë²„ì „ì— ëŒ€í•œ ê³µì‹ ëª…ì„¸ì„œì…ë‹ˆë‹¤. v1.1.xì˜ ë“€ì–¼ AI ëª¨ë¸ ì‹œìŠ¤í…œì—ì„œ **GGUF ê¸°ë°˜ ìµœì í™”**ì™€ **MongoDB ì—°ë™**, **íŒŒì¸íŠœë‹ ì§€ì›**ì„ ì¶”ê°€í•œ ì°¨ì„¸ëŒ€ AI ì±—ë´‡ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

## ë²„ì „ ì •ë³´

| ë²„ì „ | ë¦´ë¦¬ì¦ˆ ë‚ ì§œ | ì»¤ë°‹ í•´ì‹œ | ìƒíƒœ |
|------|-------------|-----------|------|
| **v1.2.0** | 2025-02-18 | `cba10b2cebceaf36d2836c9e9e0bb5fcaf37dffd` | Latest |

## v1.1.xì—ì„œ v1.2.xë¡œì˜ ì£¼ìš” ë³€ê²½ì‚¬í•­

### ëª¨ë¸ ì•„í‚¤í…ì²˜ í˜ì‹ 
- **HuggingFace Transformers** â†’ **GGUF (llama.cpp) ê¸°ë°˜**
- **Korean Bllossom 8B** â†’ **Lumimaid 8B** (ìºë¦­í„° íŠ¹í™” ëª¨ë¸)
- **4-bit ì–‘ìí™”** â†’ **Q5_K_S ì–‘ìí™”** (í’ˆì§ˆê³¼ ì„±ëŠ¥ ê· í˜•)

### ì¸í”„ë¼ ë³€í™”
- **MySQL** â†’ **MongoDB** ë°ì´í„°ë² ì´ìŠ¤ ì „í™˜
- **Docker FastAPI** â†’ **ë¡œì»¬ ë„¤ì´í‹°ë¸Œ** í™˜ê²½ ìµœì í™”

### ì‹ ê·œ ê¸°ëŠ¥
- **íŒŒì¸íŠœë‹ í”„ë ˆì„ì›Œí¬** (PEFT + LoRA)
- **ìºë¦­í„° ì¹´ë“œ ì‹œìŠ¤í…œ** (PNG ë©”íƒ€ë°ì´í„° ì§€ì›)
- **ë°ì´í„°ì…‹ ê´€ë¦¬** (ko_wikidata_QA)
- **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§** ë„êµ¬

### ì œê±°ëœ ê¸°ëŠ¥
- âŒ **Transformers ê¸°ë°˜ ëª¨ë¸** ì§€ì›
- âŒ **Docker ì»¨í…Œì´ë„ˆ** í™˜ê²½
- âŒ **ê²€ìƒ‰ API ì—°ë™**

## ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **GPU**: NVIDIA RTX 2080 (Llama ëª¨ë¸ìš©) + NVIDIA RTX 3060 (Lumimaid 8B ëª¨ë¸ìš©)
- **VRAM**: ìµœì†Œ 20GB RAM (2080 8GB + 3060 12GB)
- **ì €ì¥ê³µê°„**: ìµœì†Œ 50GB ì—¬ìœ  ê³µê°„

### ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **ìš´ì˜ì²´ì œ**: Windows 10/11 (64-bit)
- **Python**: 3.11 ì´ìƒ
- **CUDA**: 12.1 ì´ìƒ
- **llama-cpp-python**: CUDA ì§€ì› ë²„ì „
- **MongoDB**: 7.0 ì´ìƒ

## ì•„í‚¤í…ì²˜

### í”„ë¡œì íŠ¸ êµ¬ì¡°
```
ğŸ“¦ ChatBot AI v1.2.x
â”œâ”€â”€ ğŸ“ fastapi/
â”‚   â”œâ”€â”€ ğŸ“ ai_model/              # GGUF ëª¨ë¸ ì €ì¥ì†Œ
â”‚   â”‚   â”œâ”€â”€ v2-Llama-3-Lumimaid-8B-v0.1-OAS-Q5_K_S-imat.gguf
â”‚   â”‚   â”œâ”€â”€ llama-3.1-8b-instruct.gguf
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ ğŸ“ src/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ utils/             # í•µì‹¬ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ
â”‚   â”‚   â”‚   â”œâ”€â”€ AI_Llama_8B.py    # Llama 3.1 8B ëª¨ë¸ (CUDA:0)
â”‚   â”‚   â”‚   â”œâ”€â”€ AI_Lumimaid_8B.py # Lumimaid 8B ëª¨ë¸ (CUDA:1) [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ Database_mongo.py # MongoDB í•¸ë“¤ëŸ¬ [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ BaseModels.py     # ë°ì´í„° ëª¨ë¸ ì •ì˜ [UPDATED]
â”‚   â”‚   â”‚   â”œâ”€â”€ Error_handlers.py # ì˜ˆì™¸ ì²˜ë¦¬ í•¸ë“¤ëŸ¬
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py       # íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
â”‚   â”‚   â”œâ”€â”€ server.py             # FastAPI ì„œë²„ ì§„ì…ì  [UPDATED]
â”‚   â”‚   â”œâ”€â”€ bot.yaml              # ë´‡ ì„¤ì • íŒŒì¼
â”‚   â”‚   â””â”€â”€ index.html            # ì›¹ ì¸í„°í˜ì´ìŠ¤
â”‚   â”œâ”€â”€ requirements.txt          # ê¸°ë³¸ íŒ¨í‚¤ì§€ [UPDATED]
â”‚   â””â”€â”€ requirements_llama.txt    # llama.cpp ê´€ë ¨ íŒ¨í‚¤ì§€ [NEW]
â”œâ”€â”€ ğŸ“ mongo/                     # MongoDB ì»¨í…Œì´ë„ˆ ì„¤ì • [NEW]
â”‚   â”œâ”€â”€ Dockerfile                # MongoDB ë„ì»¤íŒŒì¼
â”‚   â”œâ”€â”€ mongod.conf               # MongoDB ì„¤ì •
â”‚   â””â”€â”€ .env                      # MongoDB í™˜ê²½ë³€ìˆ˜
â”œâ”€â”€ docker-compose.yml            # MongoDB ì „ìš© ì»´í¬ì¦ˆ [UPDATED]
â”œâ”€â”€ rebuild.bat                   # ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ [UPDATED]
â””â”€â”€ .env                          # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```

## API ëª…ì„¸

### ì—”ë“œí¬ì¸íŠ¸

#### POST /Llama_stream
Llama 3.1 8B ëª¨ë¸ ê¸°ë°˜ ê²€ìƒ‰ ì—°ë™ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

**ìš”ì²­ í˜•ì‹:**
```json
{
  "input_data": "string (1-500ì)",
  "google_access": "boolean (default: false)"
}
```

**ì‘ë‹µ í˜•ì‹:**
- **Content-Type**: `text/plain`
- **Transfer-Encoding**: `chunked`

#### POST /Lumimaid_stream
GGUF ê¸°ë°˜ Lumimaid 8B ëª¨ë¸ ìºë¦­í„° ëŒ€í™” API

**ìš”ì²­ í˜•ì‹:**
```json
{
  "input_data": "string (1-500ì)",
  "character_name": "string",
  "greeting": "string",
  "context": "string",
  "image": "string (Google Drive URL)",
  "access_level": "boolean (default: true)"
}
```

**ì˜ˆì‹œ ìš”ì²­:**
```json
{
  "input_data": "*I approach Rachel and talk to her.*",
  "character_name": "Rachel",
  "greeting": "*Rachel stands nervously at the lectern...*",
  "context": "Rachel is a devout Catholic girl of about 19 years old...",
  "image": "https://drive.google.com/thumbnail?id=12PqUS6bj4eAO_fLDaWQmoq94-771xfim",
  "access_level": true
}
```

#### MongoDB API ì—”ë“œí¬ì¸íŠ¸

##### GET /mongo/db
ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ

**ì‘ë‹µ í˜•ì‹:**
```json
{
  "Database": ["admin", "config", "local", "chatbot"]
}
```

##### GET /mongo/collections?db_name=chatbot
ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ

**ì‘ë‹µ í˜•ì‹:**
```json
{
  "Collections": ["users", "conversations", "characters"]
}
```

**HTTP ìƒíƒœ ì½”ë“œ:**
- `200`: ì„±ê³µ
- `400`: ì˜ëª»ëœ ìš”ì²­
- `404`: ë¦¬ì†ŒìŠ¤ ì—†ìŒ
- `422`: ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨
- `500`: ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜

## ì£¼ìš” ì»´í¬ë„ŒíŠ¸

### 1. ì„œë²„ ì»´í¬ë„ŒíŠ¸ (server.py) - v1.2.x ì¬ì„¤ê³„

#### í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì•„í‚¤í…ì²˜
v1.1.xì˜ ì™„ì „í•œ ë“€ì–¼ ì‹œìŠ¤í…œì—ì„œ **Transformers + GGUF í•˜ì´ë¸Œë¦¬ë“œ**ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.

```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    FastAPI AI ëª¨ë¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™” - v1.2.x
    Transformers(CUDA:0) + GGUF(CUDA:1) í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ
    """
    global llama_model_8b, Lumimaid_model_8b, mongo_handler

    def get_cuda_device_info(device_id: int) -> str:
        device_name = torch.cuda.get_device_name(device_id)
        device_properties = torch.cuda.get_device_properties(device_id)
        total_memory = device_properties.total_memory / (1024 ** 3)
        return f"Device {device_id}: {device_name} (Total Memory: {total_memory:.2f} GB)"

    # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì´ˆê¸°í™”
    llama_model_8b = Llama_8B()         # Transformers (CUDA:0)
    Lumimaid_model_8b = Lumimaid_8B()   # GGUF llama.cpp (CUDA:1)
    mongo_handler = MongoDBHandler()    # MongoDB í•¸ë“¤ëŸ¬

    llama_device_info = get_cuda_device_info(0)
    lumimaid_device_info = get_cuda_device_info(1)

    print(f"Llama ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({llama_device_info})")
    print(f"Lumimaid ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({lumimaid_device_info})")
    print(f"MongoDB ì—°ê²° ì™„ë£Œ")

    yield

    llama_model_8b = None
    Lumimaid_model_8b = None
    print("ëª¨ë¸ í•´ì œ ì™„ë£Œ")
```

#### MongoDB ë¼ìš°í„° ì¶”ê°€
MongoDB ë°ì´í„°ë² ì´ìŠ¤ì™€ì˜ ìƒí˜¸ì‘ìš©ì„ ìœ„í•œ ì „ìš© ë¼ìš°í„°ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.

```python
mongo_router = APIRouter()

@mongo_router.get("/db", summary="ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°")
async def list_databases():
    """
    MongoDB ì„œë²„ì— ìˆëŠ” ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        databases = await mongo_handler.get_db()
        return {"Database": databases}
    except Exception as e:
        raise ChatError.InternalServerErrorException(detail=str(e))

@mongo_router.get("/collections", summary="ë°ì´í„°ë² ì´ìŠ¤ ì»¬ë ‰ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°")
async def list_collections(db_name: str = Query(..., description="ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„")):
    """
    í˜„ì¬ ì„ íƒëœ ë°ì´í„°ë² ì´ìŠ¤ ë‚´ì˜ ëª¨ë“  ì»¬ë ‰ì…˜ ì´ë¦„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        collections = await mongo_handler.get_collection(database_name=db_name)
        return {"Collections": collections}
    except Exception as e:
        raise ChatError.InternalServerErrorException(detail=str(e))

app.include_router(
    mongo_router,
    prefix="/mongo",
    tags=["MongoDB Router"],
    responses={500: {"description": "Internal Server Error"}}
)
```

#### ê°œì„ ëœ Lumimaid ìŠ¤íŠ¸ë¦¬ë° API
GGUF ê¸°ë°˜ì˜ ìµœì í™”ëœ ìºë¦­í„° ëŒ€í™” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

```python
@app.post("/Lumimaid_stream", summary="ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ Lumimaid_8B ëª¨ë¸ ë‹µë³€ ìƒì„±")
async def Lumimaid_stream(request: ChatModel.Lumimaid_Request):
    """
    Lumimaid_8B GGUF ëª¨ë¸ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  ìºë¦­í„° ì„¤ì •ì„ ë°˜ì˜í•˜ì—¬ 
    ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        request (ChatModel.Lumimaid_Request): ì‚¬ìš©ì ìš”ì²­ ë°ì´í„°

    Returns:
        StreamingResponse: AI ëª¨ë¸ì˜ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€
    """
    try:
        # ìºë¦­í„° ì„¤ì • êµ¬ì„±
        character_settings = {
            "character_name": request.character_name,
            "greeting": request.greeting,
            "context": request.context,
            "access_level": request.access_level
        }
        
        # GGUF ëª¨ë¸ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        response_stream = Lumimaid_model_8b.generate_response_stream(
            input_text=request.input_data,
            character_settings=character_settings
        )
        
        return StreamingResponse(
            response_stream,
            media_type="text/plain",
            headers={
                "Content-Type": "text/event-stream",
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            }
        )
        
    except TimeoutError:
        raise ChatError.InternalServerErrorException(
            detail="Lumimaid ëª¨ë¸ ì‘ë‹µì´ ì‹œê°„ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤."
        )
    except ValidationError as e:
        raise ChatError.BadRequestException(detail=str(e))
    except Exception as e:
        print(f"ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì˜ˆì™¸: {e}")
        raise ChatError.InternalServerErrorException(detail=str(e))
```

### 2. AI ëª¨ë¸ ì»´í¬ë„ŒíŠ¸

#### LlamaChatModel í´ë˜ìŠ¤ (AI_Llama_8B.py) - v1.2.x ì—…ë°ì´íŠ¸

**v1.1.xì—ì„œ ë³€ê²½ëœ ì‚¬í•­:**
- âœ… **ë¡œì»¬ í™˜ê²½ ìµœì í™”** (Docker ì˜ì¡´ì„± ì œê±°)
- âœ… **ì„±ëŠ¥ íŠœë‹** (ìƒì„± íŒŒë¼ë¯¸í„° ìµœì í™”)

```python
def __init__(self):
    """
    LlamaChatModel í´ë˜ìŠ¤ ì´ˆê¸°í™” - v1.2.x
    ë¡œì»¬ í™˜ê²½ì—ì„œ CUDA:0 ì „ìš©ìœ¼ë¡œ ìµœì í™”
    """
    # í™˜ê²½ ì„¤ì • (ë¡œì»¬ ê²½ë¡œ)
    load_dotenv('.env')
    self.cache_dir = "./fastapi/ai_model"  # Docker â†’ ë¡œì»¬ ê²½ë¡œ
    self.model_id = "meta-llama/Llama-3.1-8B-Instruct"
    self.device = torch.device("cuda:0")   # RTX 2080

    # 4-bit ì–‘ìí™” ì„¤ì • (ë™ì¼)
    self.model_kwargs = {
        "torch_dtype": torch.float16,
        "trust_remote_code": True,
        "device_map": {"": self.device},
        "quantization_config": BitsAndBytesConfig(
            load_in_4bit=True,
            double_quant=True,
            compute_dtype=torch.float16
        )
    }

    # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    self.tokenizer = self.load_tokenizer()
    self.model = self.load_model()
    self.model.gradient_checkpointing_enable()
    self.conversation_history = []
```

#### LlamaModelHandler í´ë˜ìŠ¤ (AI_Lumimaid_8B.py) - ì‹ ê·œ GGUF ëª¨ë¸

**ì£¼ìš” íŠ¹ì§•:**
- **ëª¨ë¸**: `v2-Llama-3-Lumimaid-8B-v0.1-OAS-Q5_K_S-imat.gguf`
- **ì—”ì§„**: llama-cpp-python (CUDA ì§€ì›)
- **GPU**: CUDA:1 (RTX 3060) ì „ìš©
- **ì–‘ìí™”**: Q5_K_S (ê³ í’ˆì§ˆ 5-bit ì–‘ìí™”)
- **ìºë¦­í„° ì‹œìŠ¤í…œ**: PNG V2 ì¹´ë“œ ì§€ì›

**í´ë˜ìŠ¤ ì´ˆê¸°í™”:**
```python
def __init__(self, gpu_layers: int = 50) -> None:
    """
    GGUF ëª¨ë¸ ì´ˆê¸°í™” - RTX 3060 ìµœì í™”
    
    Args:
        gpu_layers (int): GPUì— ë¡œë“œí•  ë ˆì´ì–´ ìˆ˜ (ê¸°ë³¸ê°’ 50)
    """
    self.model_path = "fastapi/ai_model/v2-Llama-3-Lumimaid-8B-v0.1-OAS-Q5_K_S-imat.gguf"
    self.verbose = False
    self.gpu_layers = gpu_layers
    self.model = self._load_model()
    self.response_queue = Queue()

def _load_model(self) -> Llama:
    """
    Llama ëª¨ë¸ì„ CUDA:1 ë””ë°”ì´ìŠ¤(RTX 3060)ì—ë§Œ ë¡œë“œ
    
    Returns:
        Llama: ë¡œë“œëœ GGUF ëª¨ë¸ ê°ì²´
    """
    print("GGUF ëª¨ë¸ ë¡œë“œ ì¤‘...")
    try:
        model = Llama(
            model_path=self.model_path,
            n_gpu_layers=self.gpu_layers,
            main_gpu=1,                # RTX 3060 ì‚¬ìš©
            n_ctx=2048,                # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            n_batch=512,               # ë°°ì¹˜ í¬ê¸°
            verbose=self.verbose,
            offload_kqv=True,          # KQV ìºì‹œë¥¼ GPUì— ì˜¤í”„ë¡œë“œ
            use_mmap=False,            # ë©”ëª¨ë¦¬ ë§¤í•‘ ë¹„í™œì„±í™”
            use_mlock=True,            # ë©”ëª¨ë¦¬ ì ê¸ˆ í™œì„±í™”
            n_threads=8                # ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ
        )
        print("âœ… GGUF ëª¨ë¸ì´ CUDA:1 (RTX 3060)ì— ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ”§ GPU ì„¤ì •: {self.gpu_layers}ê°œ ë ˆì´ì–´, KQV ìºì‹œ ì˜¤í”„ë¡œë“œ í™œì„±í™”")
        return model
    except Exception as e:
        print(f"âŒ GGUF ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
```

**Llama3 í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿:**
```python
def build_llama3_prompt(character: CharacterPrompt, user_input: str) -> str:
    """
    ìºë¦­í„° ì •ë³´ë¥¼ í¬í•¨í•œ Llama3 í”„ë¡¬í”„íŠ¸ í˜•ì‹ ìƒì„±

    Args:
        character (CharacterPrompt): ìºë¦­í„° ì •ë³´
        user_input (str): ì‚¬ìš©ì ì…ë ¥

    Returns:
        str: Llama3 í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    system_prompt = (
        f"Character Name: {character.name}\n"
        f"Character Context: {character.context}\n"
        f"Initial Greeting: {character.greeting}"
    )
    
    return (
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n"
        f"{system_prompt}<|eot_id|>"
        "<|start_header_id|>user<|end_header_id|>\n"
        f"{user_input}<|eot_id|>"
        "<|start_header_id|>assistant<|end_header_id|>\n"
    )
```

**ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±:**
```python
def generate_response_stream(self, input_text: str, character_settings: dict = None) -> Generator[str, None, None]:
    """
    API í˜¸í™˜ì„ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ë©”ì„œë“œ

    Args:
        input_text (str): ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
        character_settings (dict): ìºë¦­í„° ì„¤ì • ë”•ì…”ë„ˆë¦¬

    Yields:
        str: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤
    """
    try:
        # ìºë¦­í„° ì •ë³´ ì„¤ì •
        if character_settings:
            character_info = CharacterPrompt(
                name=character_settings.get("character_name", "Assistant"),
                greeting=character_settings.get("greeting", ""),
                context=character_settings.get("context", "")
            )
            # Llama3 í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            prompt = build_llama3_prompt(character_info, input_text)
        else:
            prompt = input_text
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        for text_chunk in self.create_streaming_completion(
            prompt=prompt,
            max_tokens=2048,
            temperature=0.7,
            top_p=0.95,
            stop=["<|eot_id|>"]
        ):
            yield text_chunk

    except Exception as e:
        print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        yield f"ì˜¤ë¥˜: {str(e)}"
```

### 3. ë°ì´í„°ë² ì´ìŠ¤ ì»´í¬ë„ŒíŠ¸ (Database_mongo.py) - ì‹ ê·œ ì¶”ê°€

#### MongoDBHandler í´ë˜ìŠ¤
MySQLì„ ëŒ€ì²´í•˜ëŠ” MongoDB ì—°ë™ í•¸ë“¤ëŸ¬ì…ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- **ë¹„ë™ê¸° ì²˜ë¦¬**: motor.motor_asyncio ì‚¬ìš©
- **ì—°ê²° í’€ë§**: ìë™ ì—°ê²° ê´€ë¦¬
- **ì—ëŸ¬ í•¸ë“¤ë§**: í†µí•© ì˜ˆì™¸ ì²˜ë¦¬

```python
class MongoDBHandler:
    def __init__(self) -> None:
        """
        MongoDBHandler í´ë˜ìŠ¤ ì´ˆê¸°í™”.
        MongoDBì— ì—°ê²°í•˜ê³  í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        try:
            # í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ê²½ë¡œ ì„¤ì •
            current_directory = os.path.dirname(os.path.abspath(__file__))
            env_file_path = os.path.join(current_directory, '../.env')
            load_dotenv(env_file_path)
            
            # í™˜ê²½ ë³€ìˆ˜ì—ì„œ MongoDB ì—°ê²° URI ê°€ì ¸ì˜¤ê¸°
            mongo_host = os.getenv("MONGO_HOST")
            mongo_port = os.getenv("MONGO_PORT", 27018)
            mongo_user = os.getenv("MONGO_ADMIN_USER")
            mongo_password = os.getenv("MONGO_ADMIN_PASSWORD")
            mongo_db = os.getenv("MONGO_DATABASE")
            mongo_auth = os.getenv("MONGO_AUTH")
            
            # MongoDB URI ìƒì„±
            self.mongo_uri = (
                f"mongodb://{mongo_user}:{mongo_password}@{mongo_host}:{mongo_port}/"
                f"{mongo_db}?authSource={mongo_auth}"
            )
            
            # MongoDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            self.client = AsyncIOMotorClient(self.mongo_uri)
            self.db = self.client[mongo_db]
        except PyMongoError as e:
            raise InternalServerErrorException(detail=f"MongoDB connection error: {str(e)}")
        except Exception as e:
            raise InternalServerErrorException(detail=f"Error initializing MongoDBHandler: {str(e)}")

    async def get_db(self) -> List[str]:
        """
        ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            List[str]: ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        Raises:
            InternalServerErrorException: ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ì„ ê°€ì ¸ì˜¤ëŠ” ë„ì¤‘ ë¬¸ì œê°€ ë°œìƒí•  ê²½ìš°
        """
        try:
            return await self.client.list_database_names()
        except PyMongoError as e:
            raise InternalServerErrorException(detail=f"Error retrieving database names: {str(e)}")
        except Exception as e:
            raise InternalServerErrorException(detail=f"Unexpected error: {str(e)}")

    async def get_collection(self, database_name: str) -> List[str]:
        """
        ë°ì´í„°ë² ì´ìŠ¤ì˜ ì»¬ë ‰ì…˜ ì´ë¦„ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            database_name (str): ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„
        Returns:
            List[str]: ì»¬ë ‰ì…˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        Raises:
            NotFoundException: ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°
            InternalServerErrorException: ì»¬ë ‰ì…˜ ì´ë¦„ì„ ê°€ì ¸ì˜¤ëŠ” ë„ì¤‘ ë¬¸ì œê°€ ë°œìƒí•  ê²½ìš°
        """
        db_names = await self.get_db()
        if database_name not in db_names:
            raise NotFoundException(f"Database '{database_name}' not found.")
        try:
            return await self.client[database_name].list_collection_names()
        except PyMongoError as e:
            raise InternalServerErrorException(detail=f"Error retrieving collection names: {str(e)}")
        except Exception as e:
            raise InternalServerErrorException(detail=f"Unexpected error: {str(e)}")
```

### 4. ë°ì´í„° ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ (BaseModels.py) - v1.2.x ê°„ì†Œí™”

#### Lumimaid_Request (ì‹ ê·œ)
v1.1.xì˜ ë³µì¡í•œ Bllossom_Requestë¥¼ ê°„ì†Œí™”í•œ ìºë¦­í„° ìš”ì²­ ëª¨ë¸ì…ë‹ˆë‹¤.

**ì œê±°ëœ í•„ë“œ:**
- âŒ `description` (greetingì— í†µí•©)
- âŒ `character_setting` (contextë¡œ í†µí•©)
- âŒ `tone` (contextì— í¬í•¨)
- âŒ `energy_level`, `politeness`, `humor`, `assertiveness` (ìˆ˜ì¹˜ íŒŒë¼ë¯¸í„° ì œê±°)

**ìœ ì§€ëœ í•„ë“œ:**
- âœ… `input_data` (ì‚¬ìš©ì ì…ë ¥)
- âœ… `character_name` (ìºë¦­í„° ì´ë¦„)
- âœ… `greeting` (ì¸ì‚¬ë§/ì²« ë©”ì‹œì§€)
- âœ… `context` (ìºë¦­í„° ì„¤ì • í…ìŠ¤íŠ¸)
- âœ… `image` (ìºë¦­í„° ì´ë¯¸ì§€ URL)
- âœ… `access_level` (ì ‘ê·¼ ê¶Œí•œ)

```python
class Lumimaid_Request(BaseModel):
    """
    Lumimaid ëª¨ë¸ API ìš”ì²­ - v1.2.x ê°„ì†Œí™”
    ìºë¦­í„° ê¸°ë°˜ ëŒ€í™”ë¥¼ ìœ„í•œ ìµœì í™”ëœ ìš”ì²­ ëª¨ë¸
    """
    input_data: str = character_input_data_set
    character_name: str = character_name_set
    greeting: str = greeting_set
    context: str = context_set
    image: str = image_set
    access_level: bool = access_level_set
    
    @field_validator('image', mode='before')
    def check_img_url(cls, v):
        """Google Drive URL í˜•ì‹ ê²€ì¦"""
        return Validators.validate_URL(v)

    def model_dump(self, **kwargs):
        """
        ëª¨ë¸ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë©”ì„œë“œ
        """
        return super().model_dump(**kwargs)
    
class Lumimaid_Response(BaseModel):
    output_data: str = output_data_set
```

#### í–¥ìƒëœ URL ê²€ì¦
Google Drive ì´ë¯¸ì§€ URLì— ëŒ€í•œ ê³ ê¸‰ ê²€ì¦ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

```python
class Validators:
    @staticmethod
    def validate_URL(v: str) -> str:
        """
        Google Drive URL í˜•ì‹ ê²€ì¦ í•¨ìˆ˜
        
        Args:
            v (str): ê²€ì¦í•  URL ë¬¸ìì—´
            
        Returns:
            str: ê²€ì¦ëœ URL
            
        Raises:
            ValueError: ìœ íš¨í•˜ì§€ ì•Šì€ URL í˜•ì‹
        """
        url_pattern = re.compile(
            r'''
            ^                     # ë¬¸ìì—´ì˜ ì‹œì‘
            https?://             # http:// ë˜ëŠ” https://
            (drive\.google\.com)  # Google Drive ë„ë©”ì¸
            /thumbnail            # ê²½ë¡œì˜ ì¼ë¶€
            \?id=([a-zA-Z0-9_-]+) # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° id
            $                     # ë¬¸ìì—´ì˜ ë
            ''', re.VERBOSE
        )
        if not url_pattern.match(v):
            raise ValueError('ìœ íš¨í•œ URL í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.')
        return v

    @staticmethod
    async def check_img_url(img_url: str):
        """
        URLì˜ ì—°ê²° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
        
        Args:
            img_url (str): í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ URL
            
        Raises:
            ValueError: ì´ë¯¸ì§€ ì ‘ê·¼ ë¶ˆê°€ ë˜ëŠ” ì—°ê²° ì˜¤ë¥˜
        """
        try:
            async with httpx.AsyncClient() as client:
                response = await client.head(img_url, follow_redirects=True)
            if response.status_code != 200:
                raise ValueError('ì´ë¯¸ì§€ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
        except httpx.RequestError:
            raise ValueError('ì´ë¯¸ì§€ URLì— ì ‘ê·¼í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.')
```

## ì„¤ì¹˜ ë° ì„¤ì •

### íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹œìŠ¤í…œ (venv_install.bat) - v1.2.x ëŒ€í­ ê°œì„ 

Windows í™˜ê²½ì—ì„œ ë³µì¡í•œ CUDA ì˜ì¡´ì„±ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” í–¥ìƒëœ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

```batch
@echo off
chcp 65001
SETLOCAL

:: pip ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ
python.exe -m pip install --upgrade pip

:: numpy ë¨¼ì € ì„¤ì¹˜ (ë²„ì „ ì œí•œ)
pip install "numpy>=1.22.4,<2.0.0"

:: CUDA ê´€ë ¨ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch==2.3.1+cu118 torchvision==0.18.1+cu118 torchaudio==2.3.1+cu118 -f https://download.pytorch.org/whl/torch_stable.html

:: CUDA llama-cpp ì„¤ì¹˜
set CMAKE_ARGS="-DLLAMA_CUBLAS=on"
set FORCE_CMAKE=1
pip install --no-cache-dir "https://github.com/oobabooga/llama-cpp-python-cuBLAS-wheels/releases/download/textgen-webui/llama_cpp_python_cuda-0.3.6+cu121-cp311-cp311-win_amd64.whl"

:: ExLlamaV2 ì„¤ì¹˜ (ìµœì‹  ë²„ì „)
pip install exllamav2==0.2.8

:: Flash Attention ì„¤ì¹˜ (pre-built wheel ì‚¬ìš©)
pip install --no-cache-dir --find-links https://github.com/Dao-AILab/flash-attention/releases/download/v2.3.3/ flash-attn==2.3.3

:: CUDA ë¹Œë“œ ë„êµ¬ ì„¤ì¹˜
pip install ninja

:: spaCy ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
pip install spacy
python -m spacy download en_core_web_sm
python -m spacy download ko_core_news_sm

:: ë‚˜ë¨¸ì§€ requirements.txt íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r .\fastapi\requirements.txt
pip install -r .\fastapi\requirements_llama.txt

echo ê°€ìƒ í™˜ê²½ì´ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.
ENDLOCAL
```

### MongoDB ì»¨í…Œì´ë„ˆ ì„¤ì •

#### Docker Compose (docker-compose.yml) - MongoDB ì „ìš©
FastAPI ì»¨í…Œì´ë„ˆë¥¼ ì œê±°í•˜ê³  MongoDBë§Œ ì»¨í…Œì´ë„ˆë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.

```yaml
version: '3.8'

services:
  mongodb:
    restart: unless-stopped
    build:
      context: ./mongo
    ports:
      - "27018:27018"
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_ADMIN_USER}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ADMIN_PASSWORD}
      MONGO_DATABASE: ${MONGO_DATABASE}
    volumes:
      - ./mongo/data:/data/db
      - ./mongo/log:/var/log/mongodb
      - ./mongo/.env:/docker-entrypoint-initdb.d/.env
```

#### MongoDB ì„¤ì • (mongo/mongod.conf)
```yaml
# MongoDB êµ¬ì„± íŒŒì¼
storage:
  dbPath: /data/db
  journal:
    enabled: true

systemLog:
  destination: file
  path: /var/log/mongodb/mongod.log
  logAppend: true

net:
  port: 27018
  bindIp: 0.0.0.0

security:
  authorization: enabled
```

## ì„±ëŠ¥ íŠ¹ì„±

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- **Llama (Transformers)**: ~8GB VRAM (RTX 2080)
- **Lumimaid (GGUF)**: ~6GB VRAM (RTX 3060)
- **MongoDB**: ~1GB RAM
- **ì‹œìŠ¤í…œ RAM**: ~8-12GB

### ì²˜ë¦¬ëŸ‰
- **ë™ì‹œ ìš”ì²­**: 2ê°œ (í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ)
- **ì‹œê°„ë‹¹ ìš”ì²­**: ~300-500ê°œ
- **GGUF ì„±ëŠ¥**: Transformers ëŒ€ë¹„ 20-30% í–¥ìƒ

## ìš´ì˜ ê°€ì´ë“œ

### í™˜ê²½ ì„¤ì •
1. **Windows 11** + **Python 3.11** í™˜ê²½ êµ¬ì„±
2. **CUDA 12.1** ë“œë¼ì´ë²„ ì„¤ì¹˜
3. **MongoDB ì»¨í…Œì´ë„ˆ** ì‹¤í–‰
4. **ê°€ìƒí™˜ê²½** ì„¤ì • ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜

### ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸
- **í•˜ì´ë¸Œë¦¬ë“œ GPU** ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (CUDA:0, CUDA:1)
- **GGUF ëª¨ë¸** ë¡œë”© ë° ì¶”ë¡  ì„±ëŠ¥
- **MongoDB** ì—°ê²° ìƒíƒœ ë° ì¿¼ë¦¬ ì„±ëŠ¥
- **ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ** ì§€ì—°ì‹œê°„ ë° ì²˜ë¦¬ëŸ‰

### ë¬¸ì œ í•´ê²°
- **GGUF ë¡œë”© ì‹¤íŒ¨**: GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë ˆì´ì–´ ìˆ˜ ì¡°ì •
- **MongoDB ì—°ê²° ì˜¤ë¥˜**: ì»¨í…Œì´ë„ˆ ìƒíƒœ ë° í¬íŠ¸ í™•ì¸
- **ìŠ¤íŠ¸ë¦¬ë° ëŠê¹€**: ë„¤íŠ¸ì›Œí¬ ì„¤ì • ë° í í¬ê¸° ì¡°ì •
- **ìºë¦­í„° ì¹´ë“œ ì˜¤ë¥˜**: PNG ë©”íƒ€ë°ì´í„° í˜•ì‹ í™•ì¸

### ì„±ëŠ¥ íŠœë‹
1. **GGUF íŒŒë¼ë¯¸í„°** ì¡°ì •
   - `n_gpu_layers`: 50 (ê¸°ë³¸ê°’) â†’ 35-60 (VRAMì— ë”°ë¼)
   - `n_ctx`: 2048 â†’ 4096 (ê¸´ ëŒ€í™” ì§€ì›)
   - `n_batch`: 512 â†’ 256-1024 (ë©”ëª¨ë¦¬ vs ì†ë„)

2. **MongoDB ìµœì í™”**
   - ì¸ë±ìŠ¤ ìƒì„± (ìì£¼ ì¿¼ë¦¬í•˜ëŠ” í•„ë“œ)
   - ì—°ê²° í’€ í¬ê¸° ì¡°ì •
   - ìºì‹œ ì„¤ì • ìµœì í™”

3. **ìºë¦­í„° ì‹œìŠ¤í…œ ìµœì í™”**
   - í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°„ì†Œí™”
   - ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
   - ë°˜ë³µ í† í° ë°©ì§€ ì„¤ì •

## ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### í™•ì¥ëœ ë³´ì•ˆ ê¸°ëŠ¥
- **MongoDB ì¸ì¦**: ì‚¬ìš©ì ê¸°ë°˜ ì ‘ê·¼ ì œì–´
- **GGUF ëª¨ë¸ ê²€ì¦**: íŒŒì¼ ë¬´ê²°ì„± í™•ì¸
- **ë¡œì»¬ í™˜ê²½ ë³´ì•ˆ**: Docker ì˜ì¡´ì„± ì œê±°

### API ë³´ì•ˆ ê°•í™”
- **MongoDB ë¼ìš°í„°** ì ‘ê·¼ ì œí•œ
- **ìŠ¤íŠ¸ë¦¬ë° ì—°ê²°** ì†ë„ ì œí•œ
- **ìºë¦­í„° ë°ì´í„°** í¬ê¸° ì œí•œ
- **íŒŒì¼ ì—…ë¡œë“œ** ê²€ì¦ ê°•í™”
