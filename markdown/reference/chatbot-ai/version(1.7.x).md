# ChatBot AI - ë²„ì „ ëª…ì„¸ì„œ v1.7.x

## ê°œìš”
ì´ ë¬¸ì„œëŠ” ChatBot AI ì‹œìŠ¤í…œì˜ v1.7.x ê³„ì—´ ë²„ì „ì— ëŒ€í•œ ê³µì‹ ëª…ì„¸ì„œì…ë‹ˆë‹¤. v1.6.xì˜ ì»¨íŠ¸ë¡¤ëŸ¬ íŒ¨í„´ì—ì„œ **ì™„ì „í•œ ë„ì»¤ ê¸°ë°˜ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜**ë¡œ ëŒ€ì „í™˜ë˜ì—ˆìœ¼ë©°, **nginx API Gateway**, **í´ë¦° ì•„í‚¤í…ì²˜ íŒ¨í„´**, **ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìë™í™”**ë¥¼ ë„ì…í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ AI ì±—ë´‡ í”Œë«í¼ì…ë‹ˆë‹¤.

## ë²„ì „ ì •ë³´

| ë²„ì „ | ë¦´ë¦¬ì¦ˆ ë‚ ì§œ | ì»¤ë°‹ í•´ì‹œ | ìƒíƒœ |
|------|-------------|-----------|------|
| **None** | 2025-05-16 | `0b5d65da73cae825fbf76e853fce56d86e103ae1` | Unstable |
| **v1.7.0** | 2025-05-30 | `a066bff70648487579f6c95219b42d23552e5501` | Stable |
| **v1.7.1** | 2025-05-30 | `a0e78809eef728324f9dde17ee7481052cd2fbae` | Stable |
| **v1.7.2** | 2025-06-10 | `6ed3a4af85834340523085fc4c4e1b1e125a5f4f` | Stable |
| **v1.7.3** | 2025-06-16 | `b96f909d752032d39510891a8c147ee340584f36` | Stable |
| **v1.7.4** | 2025-06-29 | `61e47793c0647baa5272cba6a3a82bee9a12ce74` | Latest |

## v1.7.3ì—ì„œ v1.7.4ë¡œì˜ ì£¼ìš” ë³€ê²½ì‚¬í•­

### AI ëª¨ë¸ í†µí•© ë° ìµœì í™”
- **DarkIdol-Llama** â†’ **Meta-Llama-3.1-8B-Claude** ëª¨ë¸ë¡œ í†µí•©
- **Character ì„œë¹„ìŠ¤**: Q4_0 ì–‘ìí™” ëª¨ë¸ ì‚¬ìš© (GPU 0, 50 ë ˆì´ì–´)
- **Office ì„œë¹„ìŠ¤**: Q4_1 ì–‘ìí™” ëª¨ë¸ ì‚¬ìš© (GPU 1, ëª¨ë“  ë ˆì´ì–´)
- **ëª¨ë¸ ì œì‘ì**: QuantFactoryë¡œ ì¼ì›í™”

### ì„±ëŠ¥ ë° ì•ˆì •ì„± ê°œì„ 
- **Office ì„œë¹„ìŠ¤**: ë™ì‹œ ì²˜ë¦¬ ìˆ˜ 2 â†’ 1ë¡œ ì¡°ì • (ì•ˆì •ì„± ìš°ì„ )
- **GPU í• ë‹¹**: ëª…í™•í•œ GPU ë¶„ë¦¬ (Character: GPU 0, Office: GPU 1)
- **ì»¨í…ìŠ¤íŠ¸ ë° ë°°ì¹˜ í¬ê¸°**: í†µì¼ëœ ì„¤ì • (8191 í† í°, 2048 ë°°ì¹˜)
- **ë°±ì—… ì‘ë‹µ ìƒì„±**: ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ í´ë°± ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€

### ê¸°ìˆ ì  íŠ¹ì§• ê°•í™”
- **Flash Attention**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
- **ì—°ì† ë°°ì¹­**: ë©€í‹° ì‚¬ìš©ì ì²˜ë¦¬ ìµœì í™”
- **16bit KV ìºì‹œ**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
- **RoPE ìŠ¤ì¼€ì¼ë§**: ê¸´ ë¬¸ë§¥ ì§€ì› (linear scaling 2x)

### ë³¼ë¥¨ êµ¬ì¡° ë³€ê²½
- **MLP-KTLim** â†’ **QuantFactory** í´ë”ë¡œ ë³€ê²½
- ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì—…ë°ì´íŠ¸ ë° í†µí•© ê´€ë¦¬

## v1.6.xì—ì„œ v1.7.xë¡œì˜ ì£¼ìš” ë³€ê²½ì‚¬í•­

### ì•„í‚¤í…ì²˜ ëŒ€í˜ì‹ 
- **ëª¨ë…¸ë¦¬ì‹ FastAPI** â†’ **ë…ë¦½ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤** (office/character ë¶„ë¦¬)
- **ë¡œì»¬ ì„œë²„** â†’ **Docker ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**
- **ë‹¨ì¼ ì§„ì…ì ** â†’ **nginx API Gateway** (í¬íŠ¸ 8001)
- **ì»¨íŠ¸ë¡¤ëŸ¬ íŒ¨í„´** â†’ **í´ë¦° ì•„í‚¤í…ì²˜** (DDD ê¸°ë°˜)

### ì¸í”„ë¼ ì™„ì „ ì „í™˜
- **Windows ë„¤ì´í‹°ë¸Œ** â†’ **Docker Compose** ê¸°ë°˜ ë°°í¬
- **ë¡œì»¬ ëª¨ë¸ ê´€ë¦¬** â†’ **ê³µìœ  ë³¼ë¥¨** ì‹œìŠ¤í…œ
- **ê°œë³„ ì„¤ì •** â†’ **í†µí•© ì„¤ì • ê´€ë¦¬**
- **ìˆ˜ë™ ë°°í¬** â†’ **ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**

### ê°œë°œ í™˜ê²½ í˜ì‹ 
- **ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìë™í™”** (Locust ê¸°ë°˜)
- **AI ëª¨ë¸ í†µí•©** (llama.cpp CUDA 12.1)
- **í´ë¦° ì½”ë“œ ì•„í‚¤í…ì²˜** (Domain-Core-API ë¶„ë¦¬)
- **ì»¤ìŠ¤í…€ 404 í˜ì´ì§€** ì§€ì›

### ì‹ ê·œ ê¸°ëŠ¥
- **nginx ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œ** (ì»¤ìŠ¤í…€ 404, ë¡œë“œ ë°¸ëŸ°ì‹±)
- **ê³µìœ  Python ë¼ì´ë¸ŒëŸ¬ë¦¬** ë³¼ë¥¨
- **ì»¨í…Œì´ë„ˆ ê°„ í†µì‹ ** ìµœì í™”
- **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§** ëŒ€ì‹œë³´ë“œ (visualization.html)

### ì œê±°ëœ ê¸°ëŠ¥
- âŒ **ëª¨ë“  utils í´ë”** êµ¬ì¡° (í´ë¦° ì•„í‚¤í…ì²˜ë¡œ ì¬í¸)
- âŒ **Windows ë°°ì¹˜ íŒŒì¼** (ë„ì»¤ í™˜ê²½ìœ¼ë¡œ ì „í™˜)
- âŒ **ë¡œì»¬ ì„œë²„ ì‹¤í–‰** (ì»¨í…Œì´ë„ˆ ì „ìš©)
- âŒ **í”„ë¡œí† íƒ€ì… ì½”ë“œ** ì •ë¦¬ (í”„ë¡œë•ì…˜ ìµœì í™”)

## ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **GPU**: NVIDIA RTX 3060 (CUDA:0) + RTX 2080 (CUDA:1)
- **VRAM**: ìµœì†Œ 20GB RAM (3060 12GB + 2080 8GB)
- **ì €ì¥ê³µê°„**: ìµœì†Œ 100GB ì—¬ìœ  ê³µê°„ (ë„ì»¤ ì´ë¯¸ì§€ í¬í•¨)
- **ë„¤íŠ¸ì›Œí¬**: ê³ ì† ì¸í„°ë„· (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ + API í˜¸ì¶œ)

### ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **ìš´ì˜ì²´ì œ**: Docker ì§€ì› OS (Linux/Windows/macOS)
- **Docker**: 20.10 ì´ìƒ
- **Docker Compose**: 2.0 ì´ìƒ
- **NVIDIA Container Toolkit**: CUDA 12.1 ì§€ì›
- **OpenAI API**: ìœ íš¨í•œ API í‚¤

## ì•„í‚¤í…ì²˜

### í”„ë¡œì íŠ¸ êµ¬ì¡°
```
ğŸ“¦ ChatBot AI v1.7.x
â”œâ”€â”€ ğŸ“ fastapi/                  # ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
â”‚   â”œâ”€â”€ ğŸ“ ai_model/             # AI ëª¨ë¸ íŒŒì¼ (ë³¼ë¥¨ ë§ˆìš´íŠ¸)
â”‚   â”‚   â””â”€â”€ ğŸ“ QuantFactory/     # Meta-Llama-3.1-8B-Claude ëª¨ë¸ [v1.7.4]
â”‚   â”‚       â”œâ”€â”€ Meta-Llama-3.1-8B-Claude.Q4_0.gguf [NEW v1.7.4]
â”‚   â”‚       â””â”€â”€ Meta-Llama-3.1-8B-Claude.Q4_1.gguf [NEW v1.7.4]
â”‚   â”œâ”€â”€ ğŸ“ logs/                 # ë¡œê·¸ íŒŒì¼ (ê³µìœ  ë³¼ë¥¨)
â”‚   â”œâ”€â”€ ğŸ“ prompt/               # í”„ë¡¬í”„íŠ¸ ì„¤ì •
â”‚   â”‚   â”œâ”€â”€ config-Llama.json [MOVED]
â”‚   â”‚   â””â”€â”€ config-OpenAI.json [MOVED]
â”‚   â”œâ”€â”€ ğŸ“ src/                  # ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”‚   â”œâ”€â”€ ğŸ“ api/              # API ê³„ì¸µ [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ character/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ llm_controller.py [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ office/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ llm_controller.py [NEW]
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py [NEW]
â”‚   â”‚   â”œâ”€â”€ ğŸ“ core/             # í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ character/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ app_state.py [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ office/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ app_state.py [UPDATED v1.7.4]
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py [NEW]
â”‚   â”‚   â”œâ”€â”€ ğŸ“ domain/           # ë„ë©”ì¸ ëª¨ë¸ [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ character/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ config.py [NEW]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ schema.py [MOVED]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ office/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ config.py [NEW]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ schema.py [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ shared/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ base_config.py [NEW]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ error_tools.py [MOVED]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ mongodb_client.py [MOVED]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ queue_tools.py [NEW]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ search_adapter.py [MOVED]
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py [NEW]
â”‚   â”‚   â”œâ”€â”€ ğŸ“ llm/              # LLM ëª¨ë¸ ê³„ì¸µ [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ llama/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ character.py [UPDATED v1.7.4]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ office.py [UPDATED v1.7.4]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ openai/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ character.py [MOVED]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ office.py [MOVED]
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py [NEW]
â”‚   â”‚   â”œâ”€â”€ ğŸ“ server/           # ì„œë²„ ì§„ì…ì  [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ character/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Dockerfile [NEW]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ server.py [NEW]
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“ office/
â”‚   â”‚   â”‚       â”œâ”€â”€ Dockerfile [NEW]
â”‚   â”‚   â”‚       â””â”€â”€ server.py [NEW]
â”‚   â”‚   â”œâ”€â”€ ğŸ“ test/             # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ performance_results/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ character_2025-06-04_031424.csv [NEW]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ character_2025-06-04_180716.csv [NEW]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ visualization.html [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ test.bat [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ test_character_load.py [NEW]
â”‚   â”‚   â”‚   â””â”€â”€ test_office_load.py [NEW]
â”‚   â”‚   â”œâ”€â”€ Dockerfile.base [NEW]
â”‚   â”‚   â”œâ”€â”€ Dockerfile.libs [NEW]
â”‚   â”‚   â””â”€â”€ install_libs.sh [NEW]
â”‚   â”œâ”€â”€ requirements.txt [UPDATED]
â”‚   â””â”€â”€ requirements_llama.txt [UPDATED]
â”œâ”€â”€ ğŸ“ nginx/                    # API Gateway [NEW]
â”‚   â”œâ”€â”€ nginx.conf [NEW]
â”‚   â””â”€â”€ 404.html [NEW]
â”œâ”€â”€ docker-compose.yml [UPDATED v1.7.4]
â”œâ”€â”€ rebuild.bat [NEW]
â””â”€â”€ README.md [UPDATED v1.7.4]
```

## API ëª…ì„¸

### nginx API Gateway êµ¬ì¡°

#### í†µí•© ì§„ì…ì  (í¬íŠ¸ 8001)
ëª¨ë“  API ìš”ì²­ì„ nginxê°€ ë°›ì•„ ì ì ˆí•œ ì„œë¹„ìŠ¤ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.

```nginx
server {
    listen 8001;
    server_name localhost;

    # Office API ë¼ìš°íŒ…
    location ^~ /office/ {
        proxy_pass http://office_backend/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # Character API ë¼ìš°íŒ…
    location ^~ /character/ {
        proxy_pass http://character_backend/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # ì»¤ìŠ¤í…€ 404 í˜ì´ì§€
    error_page 404 /404.html;
    location = /404.html {
        root /etc/nginx/html;
        internal;
    }

    # ê¸°ë³¸ ê²½ë¡œ ì²˜ë¦¬
    location / {
        return 404;
    }
}
```

#### Office API (`/office/`)
ì—…ë¬´ìš© AI ì„œë¹„ìŠ¤ (í¬íŠ¸ 8002)

##### POST /office/Llama
Meta-Llama-3.1-8B-Claude (Q4_1) ëª¨ë¸ ê¸°ë°˜ ê²€ìƒ‰ ì—°ë™ JSON ì‘ë‹µ

**ìš”ì²­ í˜•ì‹:**
```json
{
  "input_data": "string (1-500ì)",
  "google_access": "boolean (default: false)",
  "db_id": "string (UUID)",
  "user_id": "string"
}
```

**ì‘ë‹µ í˜•ì‹:**
- **Content-Type**: `application/json`
- **Response**: `"string"` (JSON ë¬¸ìì—´ ì§ì ‘ ë°˜í™˜)
- **GPU**: CUDA:1 (RTX 2080) ì‚¬ìš©
- **ëª¨ë¸**: Meta-Llama-3.1-8B-Claude.Q4_1.gguf

##### GET /office/performance
Office ì„œë¹„ìŠ¤ ì„±ëŠ¥ í†µê³„ ì¡°íšŒ

**ì‘ë‹µ í˜•ì‹:**
```json
{
  "total_requests": 1250,
  "avg_response_time": 18.5,
  "success_rate": 98.4,
  "active_connections": 1,
  "max_concurrent": 1
}
```

#### Character API (`/character/`)
ìºë¦­í„° ëŒ€í™” AI ì„œë¹„ìŠ¤ (í¬íŠ¸ 8003)

##### POST /character/Llama
Meta-Llama-3.1-8B-Claude (Q4_0) ëª¨ë¸ ê¸°ë°˜ ìºë¦­í„° ëŒ€í™” JSON ì‘ë‹µ

**ìš”ì²­ í˜•ì‹:**
```json
{
  "input_data": "string (1-500ì)",
  "character_name": "string",
  "greeting": "string",
  "context": "string",
  "db_id": "string (UUID)",
  "user_id": "string"
}
```

**ì˜ˆì‹œ ìš”ì²­:**
```json
{
  "input_data": "*I wave at Rachel with a smile.*",
  "character_name": "Rachel",
  "greeting": "*Rachel stands nervously at the lectern, adjusting her notes...*",
  "context": "Rachel is a devout Catholic girl of about 19 years old...",
  "db_id": "b440780c-cbaa-454f-a8d2-cf884786d89f",
  "user_id": "user_test_001"
}
```

**v1.7.4 ê°œì„ ì‚¬í•­:**
- **GPU**: CUDA:0 (RTX 3060) ì‚¬ìš©
- **ëª¨ë¸**: Meta-Llama-3.1-8B-Claude.Q4_0.gguf
- **GPU ë ˆì´ì–´**: 50ê°œ (ì´ì „: ëª¨ë“  ë ˆì´ì–´)

##### GET /character/performance
Character ì„œë¹„ìŠ¤ ì„±ëŠ¥ í†µê³„ ì¡°íšŒ

**HTTP ìƒíƒœ ì½”ë“œ:**
- `200`: ì„±ê³µ
- `400`: ì˜ëª»ëœ ìš”ì²­
- `404`: ì»¤ìŠ¤í…€ 404 í˜ì´ì§€ (nginx)
- `422`: ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨
- `500`: ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜
- `503`: ì„œë¹„ìŠ¤ ì¼ì‹œ ì¤‘ë‹¨

## ì£¼ìš” ì»´í¬ë„ŒíŠ¸

### 1. Docker Compose ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ - v1.7.4 ì—…ë°ì´íŠ¸

#### ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜
ê° ì„œë¹„ìŠ¤ê°€ ë…ë¦½ì ì¸ ì»¨í…Œì´ë„ˆë¡œ ì‹¤í–‰ë˜ëŠ” ì™„ì „í•œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ êµ¬ì¡°ì…ë‹ˆë‹¤.

```yaml
version: '3.8'

services:
  # ê³µí†µ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™” ì„œë¹„ìŠ¤
  python-libs-init:
    build:
      context: ./fastapi
      dockerfile: src/Dockerfile.libs
    volumes:
      - python-libs:/opt/python-libs
    command: ["/app/install_libs.sh"]

  # Office API ì„œë¹„ìŠ¤ (v1.7.4 ì—…ë°ì´íŠ¸)
  office:
    build:
      context: ./fastapi
      dockerfile: src/server/office/Dockerfile
    ports:
      - "8002:8002"
    environment:
      - APP_MODE=office
      - NVIDIA_VISIBLE_DEVICES=1  # RTX 2080
    volumes:
      - ./fastapi/ai_model/QuantFactory:/app/fastapi/ai_model/QuantFactory:rw  # v1.7.4 ë³€ê²½
      - ./fastapi/logs:/app/logs
      - python-libs:/opt/python-libs:ro
    depends_on:
      - python-libs-init
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]

  # Character API ì„œë¹„ìŠ¤
  character:
    build:
      context: ./fastapi
      dockerfile: src/server/character/Dockerfile
    ports:
      - "8003:8003"
    environment:
      - APP_MODE=character
      - NVIDIA_VISIBLE_DEVICES=0  # RTX 3060
    volumes:
      - ./fastapi/ai_model/QuantFactory:/app/fastapi/ai_model/QuantFactory:rw  # v1.7.4 ë³€ê²½
      - ./fastapi/logs:/app/logs
      - python-libs:/opt/python-libs:ro
    depends_on:
      - python-libs-init
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  # nginx API Gateway
  nginx:
    image: nginx:alpine
    ports:
      - "8001:8001"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf:ro
      - ./nginx/404.html:/etc/nginx/html/404.html:ro
    depends_on:
      - office
      - character

volumes:
  python-libs:
    driver: local
```

### 2. AI ëª¨ë¸ í†µí•© ì‹œìŠ¤í…œ - v1.7.4 ì™„ì „ ì¬ì„¤ê³„

#### ëª¨ë¸ ì •ë³´ (v1.7.4)

| í•­ëª© | **LlamaCharacterModel** | **LlamaOfficeModel** | 
|------|----------------------|-----------------------|
| **ê¸°ë°˜ ëª¨ë¸** | Meta-Llama-3.1-8B-Claude | Meta-Llama-3.1-8B-Claude |
| **ëª¨ë¸ íŒŒì¼** | `Meta-Llama-3.1-8B-Claude.Q4_0.gguf` | `Meta-Llama-3.1-8B-Claude.Q4_1.gguf` |
| **ì œì‘ì** | QuantFactory | QuantFactory |
| **í¬ë§·** | GGUF í¬ë§· (Q4_0 ì–‘ìí™”) | GGUF í¬ë§· (Q4_1 ì–‘ìí™”) |
| **GPU í• ë‹¹** | GPU 0ë²ˆ (`main_gpu = 0`) | GPU 1ë²ˆ (`main_gpu = 1`) |
| **GPU ë ˆì´ì–´** | `n_gpu_layers = 50` | `n_gpu_layers = -1` (ëª¨ë“  ë ˆì´ì–´) |
| **ìš©ë„** | ìºë¦­í„° ë¡¤í”Œë ˆì´ ëŒ€í™” | ì—…ë¬´ìš© AI ì–´ì‹œìŠ¤í„´íŠ¸ |
| **ë¡œë”© ë°©ì‹** | `llama_cpp_cuda` | `llama_cpp_cuda` |
| **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´** | 8191 í† í° | 8191 í† í° |
| **ë°°ì¹˜ í¬ê¸°** | 2048 | 2048 |
| **ì†ŒìŠ¤** | [QuantFactory/Meta-Llama-3.1-8B-Claude-GGUF](https://huggingface.co/QuantFactory/Meta-Llama-3.1-8B-Claude-GGUF) | [QuantFactory/Meta-Llama-3.1-8B-Claude-GGUF](https://huggingface.co/QuantFactory/Meta-Llama-3.1-8B-Claude-GGUF) |

#### ê¸°ìˆ ì  íŠ¹ì§• (v1.7.4)

- **Flash Attention**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì†ë„ í–¥ìƒì„ ìœ„í•´ í™œì„±í™”
- **ì—°ì† ë°°ì¹­**: ë©€í‹° ì‚¬ìš©ì ì²˜ë¦¬ë¥¼ ìœ„í•œ ìµœì í™”
- **16bit KV ìºì‹œ**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
- **RoPE ìŠ¤ì¼€ì¼ë§**: ê¸´ ë¬¸ë§¥ ì§€ì›ì„ ìœ„í•œ linear scaling (2x)
- **ìŠ¤íŠ¸ë¦¬ë° ì§€ì›**: ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ìƒì„± ë° ì‘ë‹µ
- **ë°±ì—… ì‘ë‹µ ì‹œìŠ¤í…œ**: ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ ìë™ í´ë°±

### 3. í´ë¦° ì•„í‚¤í…ì²˜ íŒ¨í„´ - v1.7.4 ìµœì í™”

#### Core ê³„ì¸µ ì—…ë°ì´íŠ¸ (core/office/app_state.py)
ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ê³¼ ìƒíƒœ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ ê³„ì¸µì˜ ì•ˆì •ì„± ê°œì„ ì…ë‹ˆë‹¤.

```python
# core/office/app_state.py (v1.7.4)
from typing import Optional
from domain import (
    mongodb_client,
    office_config,
    queue_tools,
    error_tools
)
from llm import office_llama

RED = "\033[31m"
GREEN = "\033[32m"
RESET = "\033[0m"

# ì „ì—­ ìƒíƒœ ë³€ìˆ˜
llama_queue_handler: Optional[queue_tools.LlamaQueueHandler] = None
mongo_handler: Optional[mongodb_client.MongoDBHandler] = None

try:
    # í í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” (ì•ˆì •ì„± ìš°ì„  - ìˆœì°¨ ì²˜ë¦¬)
    llama_queue_handler = queue_tools.LlamaQueueHandler(
        service_type=queue_tools.ServiceType.OFFICE,
        model_class=office_llama.LlamaOfficeModel,
        processing_request_class=office_config.ProcessingRequest,
        max_concurrent=1  # v1.7.4: 2 â†’ 1ë¡œ ë³€ê²½ (ì•ˆì •ì„± ìš°ì„ )
    )
    
    # MongoDB í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” (ë¹„ë™ê¸°)
    mongo_handler = mongodb_client.MongoDBHandler()  # ì‹¤ì œ ì´ˆê¸°í™”ëŠ” lifespanì—ì„œ
    
    print(f"{GREEN}INFO{RESET}: Office ì•± ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ (v1.7.4)")
    
except error_tools.InternalServerErrorException as e:
    mongo_handler = None
    print(f"{RED}ERROR{RESET}: MongoDB ì´ˆê¸°í™” ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
except FileNotFoundError as e:
    llama_queue_handler = None
    print(f"{RED}ERROR{RESET}: í í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
except Exception as e:
    llama_queue_handler = None
    print(f"{RED}ERROR{RESET}: ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
```

### 4. LLM ëª¨ë¸ ê³„ì¸µ - v1.7.4 í†µí•© ìµœì í™”

#### Character Llama ëª¨ë¸ (llm/llama/character.py) - v1.7.4
Meta-Llama-3.1-8B-Claude ëª¨ë¸ì„ ì‚¬ìš©í•œ ìºë¦­í„° ëŒ€í™” ì‹œìŠ¤í…œì˜ ìµœì í™”ì…ë‹ˆë‹¤.

```python
"""
Meta-Llama-3.1-8B-Claude GGUF ëª¨ë¸ì„ ì‚¬ìš©í•œ ìºë¦­í„° ê¸°ë°˜ ëŒ€í™” ìƒì„± ëª¨ë“ˆ (v1.7.4)
"""
from typing import Optional, Generator
from llama_cpp_cuda import Llama
import warnings
import sys
import json
import time
from queue import Queue
from threading import Thread
import os
from contextlib import contextmanager

from domain import character_config, base_config

GREEN = "\033[32m"
RED = "\033[31m"
YELLOW = "\033[33m"
BLUE = "\033[34m"
RESET = "\033[0m"

def build_llama3_prompt(character_info: character_config.CharacterPrompt) -> str:
    """
    ìºë¦­í„° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Llama3 GGUF í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        character_info (character_config.CharacterPrompt): ìºë¦­í„° ê¸°ë³¸ ì •ë³´ ë° ëŒ€í™” ë§¥ë½ í¬í•¨ ê°ì²´

    Returns:
        str: Llama3 GGUF í¬ë§·ìš© í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    system_prompt = (
        f"ë‹¹ì‹ ì€ {character_info.name}ì´ë¼ëŠ” ìºë¦­í„°ì…ë‹ˆë‹¤.\n"
        f"ë‹¹ì‹ ì˜ ì„¤ì •: {character_info.context}\n"
        f"ì´ˆê¸° ì¸ì‚¬ë§: {character_info.greeting}\n\n"
        f"ì§€ì‹œ ì‚¬í•­:\n"
        f"- ìºë¦­í„°ì˜ ì„±ê²©ê³¼ ë§íˆ¬ë¥¼ ì¼ê´€ì„± ìˆê²Œ ìœ ì§€í•˜ì„¸ìš”\n"
        f"- ìì—°ìŠ¤ëŸ½ê³  ëª°ì…ê° ìˆëŠ” ëŒ€í™”ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”\n"
        f"- ìºë¦­í„° ì„¤ì •ì— ë§ëŠ” í–‰ë™ê³¼ ë°˜ì‘ì„ ë³´ì—¬ì£¼ì„¸ìš”\n"
    )

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‹œì‘
    prompt = (
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n"
        f"{system_prompt}<|eot_id|>"
    )

    # ëŒ€í™” ê¸°ë¡ ì¶”ê°€
    if character_info.chat_list:
        for chat in character_info.chat_list:
            user_input = chat.get("input_data", "")
            character_output = chat.get("output_data", "")

            if user_input:
                prompt += (
                    "<|start_header_id|>user<|end_header_id|>\n"
                    f"{user_input}<|eot_id|>"
                )
            if character_output:
                prompt += (
                    "<|start_header_id|>assistant<|end_header_id|>\n"
                    f"{character_output}<|eot_id|>"
                )

    # ìµœì‹  ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
    prompt += (
        "<|start_header_id|>user<|end_header_id|>\n"
        f"{character_info.user_input}<|eot_id|>"
        "<|start_header_id|>assistant<|end_header_id|>\n"
    )

    return prompt

class LlamaCharacterModel:
    """
    GGUF í¬ë§·ìœ¼ë¡œ ê²½ëŸ‰í™”ëœ Meta-Llama-3.1-8B-Claude ëª¨ë¸ì„ ì‚¬ìš©í•œ ìºë¦­í„° ëŒ€í™” í´ë˜ìŠ¤ (v1.7.4)
    
    ëª¨ë¸ ì •ë³´: 
    - ëª¨ë¸ëª…: Meta-Llama-3.1-8B-Claude
    - ìœ í˜•: GGUF í¬ë§· (Q4_0 ì–‘ìí™”)
    - ì œì‘ì: QuantFactory 
    - ì†ŒìŠ¤: Hugging Face ëª¨ë¸ í—ˆë¸Œ
    """
    def __init__(self) -> None:
        """
        LlamaCharacterModel í´ë˜ìŠ¤ ì´ˆê¸°í™” ë©”ì†Œë“œ (v1.7.4)
        """
        self.model_id = "Meta-Llama-3.1-8B-Claude.Q4_0"  # v1.7.4 ë³€ê²½
        self.model_path = "/app/fastapi/ai_model/QuantFactory/Meta-Llama-3.1-8B-Claude.Q4_0.gguf"  # v1.7.4 ë³€ê²½
        self.file_path = '/app/prompt/config-Llama.json'
        self.loading_text = f"{BLUE}LOADING{RESET}:    {self.model_id} ë¡œë“œ ì¤‘..."
        self.character_info: Optional[character_config.CharacterPrompt] = None
        self.config: Optional[character_config.LlamaGenerationConfig] = None
        
        # ì‘ë‹µ í ì´ˆê¸°í™”
        self.response_queue = Queue()
        
        # ì„¤ì • íŒŒì¼ ë¡œë“œ
        with open(self.file_path, 'r', encoding = 'utf-8') as file:
            self.data: base_config.BaseConfig = json.load(file)

        # ì§„í–‰ ìƒíƒœ í‘œì‹œ
        print(f"{BLUE}LOADING{RESET}:    {__class__.__name__} ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        self.model: Llama = self._load_model()
        print(f"{GREEN}SUCCESS{RESET}:   {__class__.__name__} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    def _load_model(self) -> Llama:
        """
        GGUF í¬ë§·ì˜ Meta-Llama-3.1-8B-Claude ëª¨ë¸ì„ GPU 0ì— ë¡œë“œí•©ë‹ˆë‹¤. (v1.7.4)
        """
        print(f"{self.loading_text}")
        try:
            warnings.filterwarnings("ignore")
            
            @contextmanager
            def suppress_stdout():
                with open(os.devnull, "w") as devnull:
                    old_stdout = sys.stdout
                    sys.stdout = devnull
                    try:
                        yield
                    finally:
                        sys.stdout = old_stdout

            with suppress_stdout():
                model = Llama(
                    model_path = self.model_path,       # GGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
                    n_gpu_layers = 50,                  # v1.7.4: GPU ë ˆì´ì–´ 50ê°œë¡œ ì œí•œ
                    main_gpu = 0,                       # 0ë²ˆ GPU ì‚¬ìš©
                    rope_scaling_type = 2,              # RoPE ìŠ¤ì¼€ì¼ë§ ë°©ì‹ (2 = linear) 
                    rope_freq_scale = 2.0,              # RoPE ì£¼íŒŒìˆ˜ ìŠ¤ì¼€ì¼ â†’ ê¸´ ë¬¸ë§¥ ì§€ì›   
                    n_ctx = 8191,                       # ìµœëŒ€ context length
                    n_batch = 2048,                     # ë°°ì¹˜ í¬ê¸°
                    verbose = False,                    # ë””ë²„ê¹… ë¡œê·¸ ë¹„í™œì„±í™”  
                    offload_kqv = True,                 # K/Q/V ìºì‹œë¥¼ CPUë¡œ ì˜¤í”„ë¡œë“œí•˜ì—¬ VRAM ì ˆì•½
                    use_mmap = False,                   # ë©”ëª¨ë¦¬ ë§¤í•‘ ë¹„í™œì„±í™” 
                    use_mlock = True,                   # ë©”ëª¨ë¦¬ ì ê¸ˆìœ¼ë¡œ ë©”ëª¨ë¦¬ í˜ì´ì§€ ìŠ¤ì™‘ ë°©ì§€
                    n_threads = 12,                     # CPU ìŠ¤ë ˆë“œ ìˆ˜
                    tensor_split = [1.0],               # ë‹¨ì¼ GPUì—ì„œ ëª¨ë“  í…ì„œ ë¡œë”©
                    split_mode = 1,                     # í…ì„œ ë¶„í•  ë°©ì‹ (1 = ê· ë“± ë¶„í• )
                    flash_attn = True,                  # FlashAttention ì‚¬ìš© (ì†ë„ í–¥ìƒ)
                    cont_batching = True,               # ì—°ì† ë°°ì¹­ í™œì„±í™”
                    numa = False,                       # NUMA ë¹„í™œì„±í™”
                    f16_kv = True,                      # 16bit KV ìºì‹œ ì‚¬ìš©
                    logits_all = False,                 # ë§ˆì§€ë§‰ í† í°ë§Œ logits ê³„ì‚°
                    embedding = False,                  # ì„ë² ë”© ë¹„í™œì„±í™”
                )
            return model
        except Exception as e:
            print(f"{RED}ERROR{RESET}: Character ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def _generate_fallback_response(self, prompt: str) -> str:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ ë°±ì—… ì‘ë‹µ ìƒì„± ë©”ì„œë“œ (v1.7.4 ì‹ ê·œ)
        
        Args:
            prompt (str): ìƒì„±í•  í”„ë¡¬í”„íŠ¸
            
        Returns:
            str: ìƒì„±ëœ ì‘ë‹µ
        """
        try:
            print(f"    ë°±ì—… ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ ìƒì„± ì¤‘...")
            output = self.model.create_completion(
                prompt = prompt,
                max_tokens = 1024,
                temperature = 0.7,
                top_p = 0.9,
                repeat_penalty = 1.08,
                stop = ["<|eot_id|>"],
                stream = False  # ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”
            )
            
            if 'choices' in output and len(output['choices']) > 0:
                result = output['choices'][0].get('text', '').strip()
                print(f"    ë°±ì—… ë°©ì‹ ì„±ê³µ: {len(result)} ë¬¸ì")
                return result
            else:
                return "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                
        except Exception as e:
            print(f"    ë°±ì—… ë°©ì‹ë„ ì‹¤íŒ¨: {e}")
            return "ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
```

#### Office Llama ëª¨ë¸ (llm/llama/office.py) - v1.7.4
Meta-Llama-3.1-8B-Claude ëª¨ë¸ì„ ì‚¬ìš©í•œ ì—…ë¬´ìš© AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì™„ì „í•œ ì¬ì„¤ê³„ì…ë‹ˆë‹¤.

```python
"""
Meta-Llama-3.1-8B-Claude.Q4_1.gguf ëª¨ë¸ì„ ì‚¬ìš©í•œ ì—…ë¬´ìš© ëŒ€í™” ìƒì„± ëª¨ë“ˆ (v1.7.4)
"""
from typing import Optional, Generator, List, Dict
from llama_cpp_cuda import Llama
import os
import sys
import json
import warnings
import time
from queue import Queue
from threading import Thread
from contextlib import contextmanager
from datetime import datetime

from domain import office_config, base_config

GREEN = "\033[32m"
RED = "\033[31m"
YELLOW = "\033[33m"
BLUE = "\033[34m"
RESET = "\033[0m"

def build_llama3_prompt(character_info: office_config.OfficePrompt) -> str:
    """
    ìºë¦­í„° ì •ë³´ì™€ ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ Llama3 GGUF í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤. (v1.7.4)

    Args:
        character_info (office_config.OfficePrompt): ìºë¦­í„° ê¸°ë³¸ ì •ë³´ ë° ëŒ€í™” ë§¥ë½ í¬í•¨ ê°ì²´

    Returns:
        str: Llama3 GGUF í¬ë§·ìš© í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    system_prompt = (
        f"ë‹¹ì‹ ì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ {character_info.name}ì…ë‹ˆë‹¤.\n"
        f"ë‹¹ì‹ ì˜ ì—­í• : {character_info.context}\n\n"
        f"ì°¸ê³  ì •ë³´ (ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆì„ ê²½ìš°ì—ë§Œ í™œìš©í•˜ì„¸ìš”):\n"
        f"{character_info.reference_data}\n\n"
        f"ì§€ì‹œ ì‚¬í•­:\n"
        f"- í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”\n"
        f"- ì¹œì ˆí•˜ê³  ìœ ìµí•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”\n"
        f"- ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ì°¸ê³  ì •ë³´ëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”\n"
        f"- ê°„ê²°í•˜ë©´ì„œë„ í•µì‹¬ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ë„ë¡ í•˜ì„¸ìš”\n"
    )

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‹œì‘
    prompt = (
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n"
        f"{system_prompt}<|eot_id|>"
    )

    # ëŒ€í™” ê¸°ë¡ ì¶”ê°€
    if character_info.chat_list:
        for chat in character_info.chat_list:
            user_input = chat.get("input_data", "")
            assistant_output = chat.get("output_data", "")

            if user_input:
                prompt += (
                    "<|start_header_id|>user<|end_header_id|>\n"
                    f"{user_input}<|eot_id|>"
                )
            if assistant_output:
                prompt += (
                    "<|start_header_id|>assistant<|end_header_id|>\n"
                    f"{assistant_output}<|eot_id|>"
                )

    # ìµœì‹  ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
    prompt += (
        "<|start_header_id|>user<|end_header_id|>\n"
        f"{character_info.user_input}<|eot_id|>"
        "<|start_header_id|>assistant<|end_header_id|>\n"
    )

    return prompt

class LlamaOfficeModel:
    """
    GGUF í¬ë§·ìœ¼ë¡œ ê²½ëŸ‰í™”ëœ Meta-Llama-3.1-8B-Claude ëª¨ë¸ì„ ì‚¬ìš©í•œ ì—…ë¬´ìš© ëŒ€í™” í´ë˜ìŠ¤ (v1.7.4)
    
    ëª¨ë¸ ì •ë³´: 
    - ëª¨ë¸ëª…: Meta-Llama-3.1-8B-Claude
    - ìœ í˜•: GGUF í¬ë§· (Q4_1 ì–‘ìí™”)
    - ì œì‘ì: QuantFactory
    - ì†ŒìŠ¤: Hugging Face ëª¨ë¸ í—ˆë¸Œ
    """
    def __init__(self) -> None:
        """
        LlamaOfficeModel í´ë˜ìŠ¤ ì´ˆê¸°í™” ë©”ì†Œë“œ (v1.7.4)
        """
        self.model_id = 'Meta-Llama-3.1-8B-Claude.Q4_1'  # v1.7.4 ë³€ê²½
        self.model_path = "/app/fastapi/ai_model/QuantFactory/Meta-Llama-3.1-8B-Claude.Q4_1.gguf"  # v1.7.4 ë³€ê²½
        self.file_path = '/app/prompt/config-Llama.json'
        self.loading_text = f"{BLUE}LOADING{RESET}:    {self.model_id} ë¡œë“œ ì¤‘..."
        self.character_info: Optional[office_config.OfficePrompt] = None
        self.config: Optional[office_config.LlamaGenerationConfig] = None
        
        # ì‘ë‹µ í ì´ˆê¸°í™”
        self.response_queue = Queue()
        
        # ì„¤ì • íŒŒì¼ ë¡œë“œ
        with open(self.file_path, 'r', encoding = 'utf-8') as file:
            self.data: base_config.BaseConfig = json.load(file)

        # ì§„í–‰ ìƒíƒœ í‘œì‹œ
        print(f"{BLUE}LOADING{RESET}:    {__class__.__name__} ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        self.model: Llama = self._load_model()
        print(f"{GREEN}SUCCESS{RESET}:   {__class__.__name__} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    def _load_model(self) -> Llama:
        """
        GGUF í¬ë§·ì˜ Meta-Llama-3.1-8B-Claude ëª¨ë¸ì„ GPU 1ì— ìµœëŒ€í™” ë¡œë“œí•©ë‹ˆë‹¤. (v1.7.4)
        """
        print(f"{self.loading_text}")
        try:
            warnings.filterwarnings("ignore")
            
            @contextmanager
            def suppress_stdout():
                with open(os.devnull, "w") as devnull:
                    old_stdout = sys.stdout
                    sys.stdout = devnull
                    try:
                        yield
                    finally:
                        sys.stdout = old_stdout

            # GPU ì‚¬ìš©ëŸ‰ ê·¹ëŒ€í™”ë¥¼ ìœ„í•œ ì„¤ì •
            with suppress_stdout():
                model = Llama(
                    model_path = self.model_path,       # GGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
                    n_gpu_layers = -1,                  # ëª¨ë“  ë ˆì´ì–´ë¥¼ GPUì— ë¡œë“œ
                    main_gpu = 1,                       # 1ë²ˆ GPU ì‚¬ìš© (office ì„œë¹„ìŠ¤ìš©)
                    rope_scaling_type = 2,              # RoPE ìŠ¤ì¼€ì¼ë§ ë°©ì‹ (2 = linear) 
                    rope_freq_scale = 2.0,              # RoPE ì£¼íŒŒìˆ˜ ìŠ¤ì¼€ì¼ â†’ ê¸´ ë¬¸ë§¥ ì§€ì›   
                    n_ctx = 8191,                       # ìµœëŒ€ context length
                    n_batch = 2048,                     # ë°°ì¹˜ í¬ê¸° (VRAM ì œí•œ ê³ ë ¤í•œ ì¤‘ê°„ ê°’)
                    verbose = False,                    # ë””ë²„ê¹… ë¡œê·¸ ë¹„í™œì„±í™”  
                    offload_kqv = True,                 # K/Q/V ìºì‹œë¥¼ CPUë¡œ ì˜¤í”„ë¡œë“œí•˜ì—¬ VRAM ì ˆì•½
                    use_mmap = False,                   # ë©”ëª¨ë¦¬ ë§¤í•‘ ë¹„í™œì„±í™” 
                    use_mlock = True,                   # ë©”ëª¨ë¦¬ ì ê¸ˆìœ¼ë¡œ ë©”ëª¨ë¦¬ í˜ì´ì§€ ìŠ¤ì™‘ ë°©ì§€
                    n_threads = 12,                     # CPU ìŠ¤ë ˆë“œ ìˆ˜ (ì½”ì–´ 12ê°œ ê¸°ì¤€ ì ì ˆí•œ ê°’)
                    tensor_split = [1.0],               # ë‹¨ì¼ GPUì—ì„œ ëª¨ë“  í…ì„œ ë¡œë”©
                    split_mode = 1,                     # í…ì„œ ë¶„í•  ë°©ì‹ (1 = ê· ë“± ë¶„í• )
                    flash_attn = True,                  # FlashAttention ì‚¬ìš© (ì†ë„ í–¥ìƒ)
                    cont_batching = True,               # ì—°ì† ë°°ì¹­ í™œì„±í™” (ë©€í‹° ì‚¬ìš©ì ì²˜ë¦¬ì— íš¨ìœ¨ì )
                    numa = False,                       # NUMA ë¹„í™œì„±í™” (ë‹¨ì¼ GPU ì‹œìŠ¤í…œì—ì„œ ë¶ˆí•„ìš”)
                    f16_kv = True,                      # 16bit KV ìºì‹œ ì‚¬ìš©
                    logits_all = False,                 # ë§ˆì§€ë§‰ í† í°ë§Œ logits ê³„ì‚°
                    embedding = False,                  # ì„ë² ë”© ë¹„í™œì„±í™”
                )
            return model
        except Exception as e:
            print(f"{RED}ERROR{RESET}: Office ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def _generate_fallback_response(self, prompt: str) -> str:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ ë°±ì—… ì‘ë‹µ ìƒì„± ë©”ì„œë“œ (v1.7.4 ì‹ ê·œ)
        
        Args:
            prompt (str): ìƒì„±í•  í”„ë¡¬í”„íŠ¸
            
        Returns:
            str: ìƒì„±ëœ ì‘ë‹µ
        """
        try:
            print(f"    ë°±ì—… ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ ìƒì„± ì¤‘...")
            output = self.model.create_completion(
                prompt = prompt,
                max_tokens = 1024,
                temperature = 0.7,
                top_p = 0.9,
                repeat_penalty = 1.08,
                stop = ["<|eot_id|>"],
                stream = False  # ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”
            )
            
            if 'choices' in output and len(output['choices']) > 0:
                result = output['choices'][0].get('text', '').strip()
                print(f"    ë°±ì—… ë°©ì‹ ì„±ê³µ: {len(result)} ë¬¸ì")
                return result
            else:
                return "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                
        except Exception as e:
            print(f"    ë°±ì—… ë°©ì‹ë„ ì‹¤íŒ¨: {e}")
            return "ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
```

## ì„±ëŠ¥ íŠ¹ì„± (v1.7.4)

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ìµœì í™”ëœ ì»¨í…Œì´ë„ˆ)
- **Office Service**: ~12GB VRAM (RTX 2080, Q4_1) + ~2GB RAM
- **Character Service**: ~8GB VRAM (RTX 3060, Q4_0, 50 ë ˆì´ì–´) + ~2GB RAM  
- **nginx Gateway**: ~50MB RAM
- **Python Libs Volume**: ~1GB Disk
- **ì´ ì‹œìŠ¤í…œ**: ~15GB RAM, ~20GB VRAM

### ì²˜ë¦¬ëŸ‰ (ìµœì í™”ëœ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤)
- **ë™ì‹œ ìš”ì²­**: Office(1) + Character(1) = ë…ë¦½ ì²˜ë¦¬ (ì•ˆì •ì„± ìš°ì„ )
- **ì‹œê°„ë‹¹ ìš”ì²­**: ~600-900ê°œ (ì•ˆì •ì„± ê°œì„ )
- **ì»¨í…Œì´ë„ˆ ì˜¤ë²„í—¤ë“œ**: ~10ms (nginx í”„ë¡ì‹œ í¬í•¨)
- **GPU ê²©ë¦¬**: ì™„ì „í•œ GPU ë¶„ë¦¬ë¡œ ê°„ì„­ ì—†ìŒ

### API ì‘ë‹µ ì‹œê°„ (v1.7.4 ìµœì í™”)
- **Office/Llama**: 10-25ì´ˆ (Meta-Llama Q4_1, CUDA:1, ëª¨ë“  ë ˆì´ì–´)
- **Character/Llama**: 12-30ì´ˆ (Meta-Llama Q4_0, CUDA:0, 50 ë ˆì´ì–´)
- **nginx Routing**: ~2ms (í”„ë¡ì‹œ ì˜¤ë²„í—¤ë“œ)
- **Container Startup**: ~25ì´ˆ (ëª¨ë¸ ë¡œë”© ìµœì í™”)

### ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ (v1.7.4 ê¸°ì¤€)
- **ìµœëŒ€ ë™ì‹œ ì‚¬ìš©ì**: 15ëª… (Character API)
- **í‰ê·  ì‘ë‹µ ì‹œê°„**: 16.3ì´ˆ (2ì´ˆ ê°œì„ )
- **ì„±ê³µë¥ **: 96.1% (1.9% ê°œì„ )
- **í í¬í™”ì **: 3ê°œ ìš”ì²­ ëŒ€ê¸° ì‹œ (ì•ˆì •ì„± ê°œì„ )

## ì„¤ì¹˜ ë° ì„¤ì • (v1.7.4)

### Docker ê¸°ë°˜ ë°°í¬ ì‹œìŠ¤í…œ

#### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­
1. **Docker & Docker Compose** ì„¤ì¹˜
2. **NVIDIA Container Toolkit** ì„¤ì¹˜ (GPU ì§€ì›)
3. **AI ëª¨ë¸ íŒŒì¼** ë‹¤ìš´ë¡œë“œ (QuantFactory)

#### ë¹ ë¥¸ ì‹œì‘ (v1.7.4)
```bash
# 1. ë¦¬í¬ì§€í† ë¦¬ í´ë¡ 
git clone https://github.com/TreeNut-KR/ChatBot-AI.git
cd ChatBot-AI

# 2. AI ëª¨ë¸ íŒŒì¼ ë°°ì¹˜ (v1.7.4 ì—…ë°ì´íŠ¸)
# fastapi/ai_model/QuantFactory/ í´ë”ì— GGUF ëª¨ë¸ íŒŒì¼ ë³µì‚¬
mkdir -p fastapi/ai_model/QuantFactory
# - Meta-Llama-3.1-8B-Claude.Q4_0.gguf (Characterìš©)
# - Meta-Llama-3.1-8B-Claude.Q4_1.gguf (Officeìš©)

# 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp fastapi/src/.env.example fastapi/src/.env
# .env íŒŒì¼ì—ì„œ OPENAI_API_KEY ë“± ì„¤ì •

# 4. ì»¨í…Œì´ë„ˆ ë¹Œë“œ ë° ì‹¤í–‰
docker compose up --build

# 5. API í…ŒìŠ¤íŠ¸ (v1.7.4)
curl -X POST "http://localhost:8001/office/Llama" \
  -H "Content-Type: application/json" \
  -d '{"input_data": "ì•ˆë…•í•˜ì„¸ìš”!", "user_id": "test_user"}'

curl -X POST "http://localhost:8001/character/Llama" \
  -H "Content-Type: application/json" \
  -d '{
    "input_data": "ì•ˆë…•í•˜ì„¸ìš”!",
    "character_name": "AI Assistant",
    "greeting": "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤.",
    "context": "ì¹œê·¼í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸",
    "db_id": "test-uuid",
    "user_id": "test_user"
  }'
```

#### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (v1.7.4)
```bash
# Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
wget https://huggingface.co/QuantFactory/Meta-Llama-3.1-8B-Claude-GGUF/resolve/main/Meta-Llama-3.1-8B-Claude.Q4_0.gguf -O fastapi/ai_model/QuantFactory/Meta-Llama-3.1-8B-Claude.Q4_0.gguf

wget https://huggingface.co/QuantFactory/Meta-Llama-3.1-8B-Claude-GGUF/resolve/main/Meta-Llama-3.1-8B-Claude.Q4_1.gguf -O fastapi/ai_model/QuantFactory/Meta-Llama-3.1-8B-Claude.Q4_1.gguf
```

## ìš´ì˜ ê°€ì´ë“œ (v1.7.4)

### í™˜ê²½ ì„¤ì •
1. **Docker í™˜ê²½** êµ¬ì„± ë° GPU ì§€ì› í™•ì¸
2. **AI ëª¨ë¸ íŒŒì¼** ë‹¤ìš´ë¡œë“œ ë° QuantFactory í´ë” ë°°ì¹˜
3. **í™˜ê²½ ë³€ìˆ˜** ì„¤ì • (OpenAI API í‚¤ ë“±)
4. **ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜** ì‹¤í–‰

### ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸ (v1.7.4)
- **ì»¨í…Œì´ë„ˆ ìƒíƒœ**: `docker compose ps`
- **GPU ì‚¬ìš©ë¥ **: `nvidia-smi` (Character: GPU 0, Office: GPU 1)
- **nginx ë¡œê·¸**: `docker compose logs nginx`
- **API ì„±ëŠ¥**: `/performance` ì—”ë“œí¬ì¸íŠ¸
- **ëª¨ë¸ ì„±ëŠ¥**: ì‘ë‹µ ì‹œê°„ ë° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§

### ë¬¸ì œ í•´ê²° (v1.7.4)
- **ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨**: QuantFactory í´ë” ë° ëª¨ë¸ íŒŒì¼ ê²½ë¡œ í™•ì¸
- **GPU ë©”ëª¨ë¦¬ ë¶€ì¡±**: Character ì„œë¹„ìŠ¤ì˜ GPU ë ˆì´ì–´ ìˆ˜ ì¡°ì • (50 â†’ ë” ë‚®ê²Œ)
- **ì‘ë‹µ í’ˆì§ˆ ì €í•˜**: ë°±ì—… ì‘ë‹µ ì‹œìŠ¤í…œ ë™ì‘ ì—¬ë¶€ í™•ì¸
- **ì•ˆì •ì„± ë¬¸ì œ**: Office ì„œë¹„ìŠ¤ max_concurrent ì„¤ì • í™•ì¸ (1ë¡œ ê³ ì •)

### ì„±ëŠ¥ íŠœë‹ (v1.7.4)
1. **ëª¨ë¸ ìµœì í™”**
   - Character: GPU ë ˆì´ì–´ ìˆ˜ ì¡°ì • (30-50)
   - Office: ë°°ì¹˜ í¬ê¸° ìµœì í™” (1024-2048)
   - ì–‘ìí™” ë ˆë²¨ ë³€ê²½ (Q4_0 â†” Q4_1)

2. **ì„œë¹„ìŠ¤ ìµœì í™”**
   - ë™ì‹œ ì²˜ë¦¬ ìˆ˜ ì¡°ì • (ì•ˆì •ì„± vs ì²˜ë¦¬ëŸ‰)
   - í íƒ€ì„ì•„ì›ƒ ì„¤ì • ìµœì í™”
   - ë°±ì—… ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° íŠœë‹

3. **ì¸í”„ë¼ ìµœì í™”**
   - nginx í”„ë¡ì‹œ ì„¤ì • ìµœì í™”
   - ë³¼ë¥¨ ë§ˆìš´íŠ¸ ì„±ëŠ¥ ê°œì„ 
   - ì»¨í…Œì´ë„ˆ ë¦¬ì†ŒìŠ¤ í•œê³„ ì¡°ì •
