# ChatBot AI - ë²„ì „ ëª…ì„¸ì„œ v1.7.x

## ê°œìš”
ì´ ë¬¸ì„œëŠ” ChatBot AI ì‹œìŠ¤í…œì˜ v1.7.x ê³„ì—´ ë²„ì „ì— ëŒ€í•œ ê³µì‹ ëª…ì„¸ì„œì…ë‹ˆë‹¤. v1.6.xì˜ ì»¨íŠ¸ë¡¤ëŸ¬ íŒ¨í„´ì—ì„œ **ì™„ì „í•œ ë„ì»¤ ê¸°ë°˜ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜**ë¡œ ëŒ€ì „í™˜ë˜ì—ˆìœ¼ë©°, **nginx API Gateway**, **í´ë¦° ì•„í‚¤í…ì²˜ íŒ¨í„´**, **ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìë™í™”**ë¥¼ ë„ì…í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ AI ì±—ë´‡ í”Œë«í¼ì…ë‹ˆë‹¤.

## ë²„ì „ ì •ë³´

| ë²„ì „ | ë¦´ë¦¬ì¦ˆ ë‚ ì§œ | ì»¤ë°‹ í•´ì‹œ | ìƒíƒœ |
|------|-------------|-----------|------|
| **v1.7.0** | 2025-05-30 | `a066bff70648487579f6c95219b42d23552e5501` | Stable |
| **v1.7.1** | 2025-05-30 | `a0e78809eef728324f9dde17ee7481052cd2fbae` | Stable |
| **v1.7.2** | 2025-06-10 | `6ed3a4af85834340523085fc4c4e1b1e125a5f4f` | Stable |
| **v1.7.3** | 2025-06-16 | `b96f909d752032d39510891a8c147ee340584f36` | Latest |

## v1.6.xì—ì„œ v1.7.xë¡œì˜ ì£¼ìš” ë³€ê²½ì‚¬í•­

### ì•„í‚¤í…ì²˜ ëŒ€í˜ì‹ 
- **ëª¨ë…¸ë¦¬ì‹ FastAPI** â†’ **ë…ë¦½ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤** (office/character ë¶„ë¦¬)
- **ë¡œì»¬ ì„œë²„** â†’ **Docker ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**
- **ë‹¨ì¼ ì§„ì…ì ** â†’ **nginx API Gateway** (í¬íŠ¸ 8001)
- **ì»¨íŠ¸ë¡¤ëŸ¬ íŒ¨í„´** â†’ **í´ë¦° ì•„í‚¤í…ì²˜** (DDD ê¸°ë°˜)

### ì¸í”„ë¼ ì™„ì „ ì „í™˜
- **Windows ë„¤ì´í‹°ë¸Œ** â†’ **Docker Compose** ê¸°ë°˜ ë°°í¬
- **ë¡œì»¬ ëª¨ë¸ ê´€ë¦¬** â†’ **ê³µìœ  ë³¼ë¥¨** ì‹œìŠ¤í…œ
- **ê°œë³„ ì„¤ì •** â†’ **í†µí•© ì„¤ì • ê´€ë¦¬**
- **ìˆ˜ë™ ë°°í¬** â†’ **ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**

### ê°œë°œ í™˜ê²½ í˜ì‹ 
- **ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìë™í™”** (Locust ê¸°ë°˜)
- **AI ëª¨ë¸ í†µí•©** (llama.cpp CUDA 12.1)
- **í´ë¦° ì½”ë“œ ì•„í‚¤í…ì²˜** (Domain-Core-API ë¶„ë¦¬)
- **ì»¤ìŠ¤í…€ 404 í˜ì´ì§€** ì§€ì›

### ì‹ ê·œ ê¸°ëŠ¥
- **nginx ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œ** (ì»¤ìŠ¤í…€ 404, ë¡œë“œ ë°¸ëŸ°ì‹±)
- **ê³µìœ  Python ë¼ì´ë¸ŒëŸ¬ë¦¬** ë³¼ë¥¨
- **ì»¨í…Œì´ë„ˆ ê°„ í†µì‹ ** ìµœì í™”
- **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§** ëŒ€ì‹œë³´ë“œ (visualization.html)

### ì œê±°ëœ ê¸°ëŠ¥
- âŒ **ëª¨ë“  utils í´ë”** êµ¬ì¡° (í´ë¦° ì•„í‚¤í…ì²˜ë¡œ ì¬í¸)
- âŒ **Windows ë°°ì¹˜ íŒŒì¼** (ë„ì»¤ í™˜ê²½ìœ¼ë¡œ ì „í™˜)
- âŒ **ë¡œì»¬ ì„œë²„ ì‹¤í–‰** (ì»¨í…Œì´ë„ˆ ì „ìš©)
- âŒ **í”„ë¡œí† íƒ€ì… ì½”ë“œ** ì •ë¦¬ (í”„ë¡œë•ì…˜ ìµœì í™”)

## ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **GPU**: NVIDIA RTX 3060 (CUDA:0) + RTX 2080 (CUDA:1)
- **VRAM**: ìµœì†Œ 20GB RAM (3060 12GB + 2080 8GB)
- **ì €ì¥ê³µê°„**: ìµœì†Œ 100GB ì—¬ìœ  ê³µê°„ (ë„ì»¤ ì´ë¯¸ì§€ í¬í•¨)
- **ë„¤íŠ¸ì›Œí¬**: ê³ ì† ì¸í„°ë„· (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ + API í˜¸ì¶œ)

### ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **ìš´ì˜ì²´ì œ**: Docker ì§€ì› OS (Linux/Windows/macOS)
- **Docker**: 20.10 ì´ìƒ
- **Docker Compose**: 2.0 ì´ìƒ
- **NVIDIA Container Toolkit**: CUDA 12.1 ì§€ì›
- **OpenAI API**: ìœ íš¨í•œ API í‚¤

## ì•„í‚¤í…ì²˜

### í”„ë¡œì íŠ¸ êµ¬ì¡°
```
ğŸ“¦ ChatBot AI v1.7.x
â”œâ”€â”€ ğŸ“ fastapi/                  # ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
â”‚   â”œâ”€â”€ ğŸ“ ai_model/             # AI ëª¨ë¸ íŒŒì¼ (ë³¼ë¥¨ ë§ˆìš´íŠ¸)
â”‚   â”‚   â”œâ”€â”€ MLP-KTLim/           # í•œêµ­ì–´ Bllossom ëª¨ë¸
â”‚   â”‚   â””â”€â”€ QuantFactory/        # DarkIdol ìºë¦­í„° ëª¨ë¸
â”‚   â”œâ”€â”€ ğŸ“ logs/                 # ë¡œê·¸ íŒŒì¼ (ê³µìœ  ë³¼ë¥¨)
â”‚   â”œâ”€â”€ ğŸ“ prompt/               # í”„ë¡¬í”„íŠ¸ ì„¤ì •
â”‚   â”‚   â”œâ”€â”€ config-Llama.json [MOVED]
â”‚   â”‚   â””â”€â”€ config-OpenAI.json [MOVED]
â”‚   â”œâ”€â”€ ğŸ“ src/                  # ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”‚   â”œâ”€â”€ ğŸ“ api/              # API ê³„ì¸µ [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ character/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ llm_controller.py [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ office/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ llm_controller.py [NEW]
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py [NEW]
â”‚   â”‚   â”œâ”€â”€ ğŸ“ core/             # í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ character/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ app_state.py [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ office/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ app_state.py [NEW]
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py [NEW]
â”‚   â”‚   â”œâ”€â”€ ğŸ“ domain/           # ë„ë©”ì¸ ëª¨ë¸ [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ character/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ config.py [NEW]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ schema.py [MOVED]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ office/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ config.py [NEW]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ schema.py [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ shared/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ base_config.py [NEW]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ error_tools.py [MOVED]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ mongodb_client.py [MOVED]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ queue_tools.py [NEW]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ search_adapter.py [MOVED]
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py [NEW]
â”‚   â”‚   â”œâ”€â”€ ğŸ“ llm/              # LLM ëª¨ë¸ ê³„ì¸µ [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ llama/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ character.py [MOVED]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ office.py [MOVED]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ openai/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ character.py [MOVED]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ office.py [MOVED]
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py [NEW]
â”‚   â”‚   â”œâ”€â”€ ğŸ“ server/           # ì„œë²„ ì§„ì…ì  [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ character/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Dockerfile [NEW]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ server.py [NEW]
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“ office/
â”‚   â”‚   â”‚       â”œâ”€â”€ Dockerfile [NEW]
â”‚   â”‚   â”‚       â””â”€â”€ server.py [NEW]
â”‚   â”‚   â”œâ”€â”€ ğŸ“ test/             # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ performance_results/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ character_2025-06-04_031424.csv [NEW]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ character_2025-06-04_180716.csv [NEW]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ visualization.html [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ test.bat [NEW]
â”‚   â”‚   â”‚   â”œâ”€â”€ test_character_load.py [NEW]
â”‚   â”‚   â”‚   â””â”€â”€ test_office_load.py [NEW]
â”‚   â”‚   â”œâ”€â”€ Dockerfile.base [NEW]
â”‚   â”‚   â”œâ”€â”€ Dockerfile.libs [NEW]
â”‚   â”‚   â””â”€â”€ install_libs.sh [NEW]
â”‚   â”œâ”€â”€ requirements.txt [UPDATED]
â”‚   â””â”€â”€ requirements_llama.txt [UPDATED]
â”œâ”€â”€ ğŸ“ nginx/                    # API Gateway [NEW]
â”‚   â”œâ”€â”€ nginx.conf [NEW]
â”‚   â””â”€â”€ 404.html [NEW]
â”œâ”€â”€ docker-compose.yml [NEW]
â”œâ”€â”€ rebuild.bat [NEW]
â””â”€â”€ README.md [UPDATED]
```

## API ëª…ì„¸

### nginx API Gateway êµ¬ì¡°

#### í†µí•© ì§„ì…ì  (í¬íŠ¸ 8001)
ëª¨ë“  API ìš”ì²­ì„ nginxê°€ ë°›ì•„ ì ì ˆí•œ ì„œë¹„ìŠ¤ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.

```nginx
server {
    listen 8001;
    server_name localhost;

    # Office API ë¼ìš°íŒ…
    location ^~ /office/ {
        proxy_pass http://office_backend/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # Character API ë¼ìš°íŒ…
    location ^~ /character/ {
        proxy_pass http://character_backend/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # ì»¤ìŠ¤í…€ 404 í˜ì´ì§€
    error_page 404 /404.html;
    location = /404.html {
        root /etc/nginx/html;
        internal;
    }

    # ê¸°ë³¸ ê²½ë¡œ ì²˜ë¦¬
    location / {
        return 404;
    }
}
```

#### Office API (`/office/`)
ì—…ë¬´ìš© AI ì„œë¹„ìŠ¤ (í¬íŠ¸ 8002)

##### POST /office/Llama
Llama Office ëª¨ë¸ ê¸°ë°˜ ê²€ìƒ‰ ì—°ë™ JSON ì‘ë‹µ

**ìš”ì²­ í˜•ì‹:**
```json
{
  "input_data": "string (1-500ì)",
  "google_access": "boolean (default: false)",
  "db_id": "string (UUID)",
  "user_id": "string"
}
```

**ì‘ë‹µ í˜•ì‹:**
- **Content-Type**: `application/json`
- **Response**: `"string"` (JSON ë¬¸ìì—´ ì§ì ‘ ë°˜í™˜)

##### GET /office/performance
Office ì„œë¹„ìŠ¤ ì„±ëŠ¥ í†µê³„ ì¡°íšŒ

**ì‘ë‹µ í˜•ì‹:**
```json
{
  "total_requests": 1250,
  "avg_response_time": 18.5,
  "success_rate": 98.4,
  "active_connections": 3
}
```

#### Character API (`/character/`)
ìºë¦­í„° ëŒ€í™” AI ì„œë¹„ìŠ¤ (í¬íŠ¸ 8003)

##### POST /character/Llama
Llama Character ëª¨ë¸ ê¸°ë°˜ ìºë¦­í„° ëŒ€í™” JSON ì‘ë‹µ

**ìš”ì²­ í˜•ì‹:**
```json
{
  "input_data": "string (1-500ì)",
  "character_name": "string",
  "greeting": "string",
  "context": "string",
  "db_id": "string (UUID)",
  "user_id": "string"
}
```

**ì˜ˆì‹œ ìš”ì²­:**
```json
{
  "input_data": "*I wave at Rachel with a smile.*",
  "character_name": "Rachel",
  "greeting": "*Rachel stands nervously at the lectern, adjusting her notes...*",
  "context": "Rachel is a devout Catholic girl of about 19 years old...",
  "db_id": "b440780c-cbaa-454f-a8d2-cf884786d89f",
  "user_id": "user_test_001"
}
```

##### GET /character/performance
Character ì„œë¹„ìŠ¤ ì„±ëŠ¥ í†µê³„ ì¡°íšŒ

**HTTP ìƒíƒœ ì½”ë“œ:**
- `200`: ì„±ê³µ
- `400`: ì˜ëª»ëœ ìš”ì²­
- `404`: ì»¤ìŠ¤í…€ 404 í˜ì´ì§€ (nginx)
- `422`: ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨
- `500`: ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜
- `503`: ì„œë¹„ìŠ¤ ì¼ì‹œ ì¤‘ë‹¨

## ì£¼ìš” ì»´í¬ë„ŒíŠ¸

### 1. Docker Compose ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ - v1.7.x ì‹ ê·œ

#### ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜
ê° ì„œë¹„ìŠ¤ê°€ ë…ë¦½ì ì¸ ì»¨í…Œì´ë„ˆë¡œ ì‹¤í–‰ë˜ëŠ” ì™„ì „í•œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ êµ¬ì¡°ì…ë‹ˆë‹¤.

```yaml
version: '3.8'

services:
  # ê³µí†µ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™” ì„œë¹„ìŠ¤
  python-libs-init:
    build:
      context: ./fastapi
      dockerfile: src/Dockerfile.libs
    volumes:
      - python-libs:/opt/python-libs
    command: ["/app/install_libs.sh"]

  # Office API ì„œë¹„ìŠ¤
  office:
    build:
      context: ./fastapi
      dockerfile: src/server/office/Dockerfile
    ports:
      - "8002:8002"
    environment:
      - APP_MODE=office
      - NVIDIA_VISIBLE_DEVICES=1  # RTX 2080
    volumes:
      - ./fastapi/ai_model:/app/fastapi/ai_model:ro
      - ./fastapi/logs:/app/logs
      - python-libs:/opt/python-libs:ro
    depends_on:
      - python-libs-init
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]

  # Character API ì„œë¹„ìŠ¤
  character:
    build:
      context: ./fastapi
      dockerfile: src/server/character/Dockerfile
    ports:
      - "8003:8003"
    environment:
      - APP_MODE=character
      - NVIDIA_VISIBLE_DEVICES=0  # RTX 3060
    volumes:
      - ./fastapi/ai_model:/app/fastapi/ai_model:ro
      - ./fastapi/logs:/app/logs
      - python-libs:/opt/python-libs:ro
    depends_on:
      - python-libs-init
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  # nginx API Gateway
  nginx:
    image: nginx:alpine
    ports:
      - "8001:8001"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf:ro
      - ./nginx/404.html:/etc/nginx/html/404.html:ro
    depends_on:
      - office
      - character

volumes:
  python-libs:
    driver: local
```

### 2. í´ë¦° ì•„í‚¤í…ì²˜ íŒ¨í„´ - v1.7.x ì™„ì „ ì¬ì„¤ê³„

#### API ê³„ì¸µ (api/)
ì™¸ë¶€ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” í”„ë ˆì  í…Œì´ì…˜ ê³„ì¸µì…ë‹ˆë‹¤.

```python
# api/character/llm_controller.py
from fastapi import Path, APIRouter, HTTPException, Request
from fastapi.responses import JSONResponse
from pydantic import ValidationError

import time
from core import character_app_state as AppState
from domain import (
    character_schema as ChatModel,
    error_tools as ChatError,
)
from llm import character_openai

# ì²˜ë¦¬ ì‹œê°„ ì„ê³„ê°’ ì„¤ì •
MAX_PROCESSING_TIME = 180  # 3ë¶„ (nginx íƒ€ì„ì•„ì›ƒë³´ë‹¤ ì¶©ë¶„íˆ ì§§ê²Œ)
RETRY_AFTER_MINUTES = 3    # 3ë¶„ í›„ ì¬ì‹œë„ ê¶Œì¥

character_router = APIRouter()

@character_router.post("/Llama", summary="Llama ëª¨ë¸ì´ ìºë¦­í„° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±")
async def character_llama(request: ChatModel.character_Request, req: Request):
    """
    DarkIdol-Llama-3.1-8B GGUF ëª¨ë¸ì„ ì‚¬ìš©í•œ ìºë¦­í„° ê¸°ë°˜ ëŒ€í™”
    
    Args:
        request: ìºë¦­í„° ì„¤ì •ê³¼ ì‚¬ìš©ì ì…ë ¥ì„ í¬í•¨í•œ ìš”ì²­
        req: FastAPI ìš”ì²­ ê°ì²´
        
    Returns:
        str: ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
        
    Raises:
        HTTPException: ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ
    """
    start_time = time.time()
    
    try:
        # í ìƒíƒœ í™•ì¸
        if not AppState.llama_queue_handler:
            raise ChatError.InternalServerErrorException(detail="Llama í í•¸ë“¤ëŸ¬ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        queue_size = AppState.llama_queue_handler.get_queue_size()
        estimated_time = calculate_estimated_time(queue_size)
        
        if estimated_time > MAX_PROCESSING_TIME:
            return JSONResponse(
                status_code=429,
                content={
                    "detail": f"í˜„ì¬ ìš”ì²­ì´ ë§ì•„ ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ì´ {estimated_time:.1f}ì´ˆì…ë‹ˆë‹¤.",
                    "retry_after": RETRY_AFTER_MINUTES * 60,
                    "queue_size": queue_size
                },
                headers={"Retry-After": str(RETRY_AFTER_MINUTES * 60)}
            )
        
        # ìš”ì²­ ì²˜ë¦¬
        processing_request = character_config.ProcessingRequest(
            user_input=request.input_data,
            character_name=request.character_name,
            greeting=request.greeting,
            context=request.context,
            db_id=request.db_id,
            user_id=request.user_id
        )
        
        # ë¹„ë™ê¸° ì²˜ë¦¬ ìš”ì²­
        result = await AppState.llama_queue_handler.process_request(
            request=processing_request,
            service_type=queue_tools.ServiceType.CHARACTER
        )
        
        processing_time = time.time() - start_time
        print(f"{GREEN}INFO{RESET}: Character ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ)")
        
        return result
        
    except Exception as e:
        processing_time = time.time() - start_time
        print(f"{RED}ERROR{RESET}: Character ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨ (ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ): {str(e)}")
        raise ChatError.InternalServerErrorException(detail="ìºë¦­í„° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@character_router.get("/performance", summary="ì„±ëŠ¥ í†µê³„ ì¡°íšŒ")
async def get_performance():
    """
    Character ì„œë¹„ìŠ¤ì˜ ì„±ëŠ¥ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if AppState.llama_queue_handler:
        return AppState.llama_queue_handler.get_performance_stats()
    else:
        return {"status": "unavailable", "message": "ì„±ëŠ¥ í†µê³„ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
```

#### Core ê³„ì¸µ (core/)
ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ê³¼ ìƒíƒœ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ ê³„ì¸µì…ë‹ˆë‹¤.

```python
# core/character/app_state.py
from typing import Optional
from domain import (
    mongodb_client,
    character_config,
    queue_tools,
    error_tools
)
from llm import character_llama

RED = "\033[31m"
GREEN = "\033[32m"
RESET = "\033[0m"

# ì „ì—­ ìƒíƒœ ë³€ìˆ˜
llama_queue_handler: Optional[queue_tools.LlamaQueueHandler] = None
mongo_handler: Optional[mongodb_client.MongoDBHandler] = None

try:
    # í í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” (ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ)
    llama_queue_handler = queue_tools.LlamaQueueHandler(
        max_concurrent=1,  # ë™ì‹œ ì²˜ë¦¬ ì œí•œ
        timeout=180,       # 3ë¶„ íƒ€ì„ì•„ì›ƒ
        service_type=queue_tools.ServiceType.CHARACTER
    )
    
    # MongoDB í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” (ë¹„ë™ê¸°)
    mongo_handler = mongodb_client.MongoDBHandler()  # ì‹¤ì œ ì´ˆê¸°í™”ëŠ” lifespanì—ì„œ
    
    print(f"{GREEN}INFO{RESET}: Character ì•± ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ")
    
except error_tools.InternalServerErrorException as e:
    mongo_handler = None
    print(f"{RED}ERROR{RESET}: MongoDB ì´ˆê¸°í™” ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
except FileNotFoundError as e:
    llama_queue_handler = None
    print(f"{RED}ERROR{RESET}: í í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
except Exception as e:
    llama_queue_handler = None
    print(f"{RED}ERROR{RESET}: ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
```

#### Domain ê³„ì¸µ (domain/)
ë„ë©”ì¸ ëª¨ë¸ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ì„ ì •ì˜í•˜ëŠ” ê³„ì¸µì…ë‹ˆë‹¤.

```python
# domain/shared/queue_tools.py
"""
Llama ëª¨ë¸ì˜ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ í†µí•© í í•¸ë“¤ëŸ¬
"""
import asyncio
import time
import uuid
from typing import Dict, Any, Optional, Type
from enum import Enum

from .error_tools import ValueErrorException, InternalServerErrorException

class ServiceType(Enum):
    CHARACTER = "character"
    OFFICE = "office"

class LlamaQueueHandler:
    """
    Llama ëª¨ë¸ ìš”ì²­ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” í í•¸ë“¤ëŸ¬
    
    Features:
    - ìš”ì²­ í ê´€ë¦¬
    - ë™ì‹œ ì²˜ë¦¬ ì œí•œ
    - íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
    - ì„±ëŠ¥ í†µê³„
    """
    
    def __init__(self, max_concurrent: int = 1, timeout: int = 180, service_type: ServiceType = ServiceType.CHARACTER):
        self.max_concurrent = max_concurrent
        self.timeout = timeout
        self.service_type = service_type
        self.request_queue = asyncio.Queue()
        self.active_requests = 0
        self.total_requests = 0
        self.total_processing_time = 0.0
        self.successful_requests = 0
        self.failed_requests = 0
        self.start_time = time.time()
        
    async def process_request(self, request: Any, service_type: ServiceType) -> str:
        """
        ìš”ì²­ì„ íì— ì¶”ê°€í•˜ê³  ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            request: ì²˜ë¦¬í•  ìš”ì²­ ê°ì²´
            service_type: ì„œë¹„ìŠ¤ íƒ€ì…
            
        Returns:
            str: ì²˜ë¦¬ ê²°ê³¼
            
        Raises:
            TimeoutError: ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼
            InternalServerErrorException: ì²˜ë¦¬ ì‹¤íŒ¨
        """
        request_id = str(uuid.uuid4())
        start_time = time.time()
        
        try:
            self.total_requests += 1
            self.active_requests += 1
            
            # ì‹¤ì œ ëª¨ë¸ ì²˜ë¦¬ (ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ)
            if service_type == ServiceType.CHARACTER:
                result = await self._process_character_request(request)
            elif service_type == ServiceType.OFFICE:
                result = await self._process_office_request(request)
            else:
                raise ValueErrorException(detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„œë¹„ìŠ¤ íƒ€ì…ì…ë‹ˆë‹¤.")
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self.total_processing_time += processing_time
            self.successful_requests += 1
            
            return result
            
        except Exception as e:
            self.failed_requests += 1
            raise InternalServerErrorException(detail=f"ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        finally:
            self.active_requests -= 1
    
    def get_queue_size(self) -> int:
        """í˜„ì¬ íì— ëŒ€ê¸° ì¤‘ì¸ ìš”ì²­ ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.request_queue.qsize()
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        uptime = time.time() - self.start_time
        avg_response_time = (
            self.total_processing_time / self.successful_requests 
            if self.successful_requests > 0 else 0
        )
        success_rate = (
            (self.successful_requests / self.total_requests * 100) 
            if self.total_requests > 0 else 0
        )
        
        return {
            "service_type": self.service_type.value,
            "total_requests": self.total_requests,
            "successful_requests": self.successful_requests,
            "failed_requests": self.failed_requests,
            "active_requests": self.active_requests,
            "queue_size": self.get_queue_size(),
            "avg_response_time": round(avg_response_time, 2),
            "success_rate": round(success_rate, 2),
            "uptime_seconds": round(uptime, 2)
        }
    
    async def _process_character_request(self, request) -> str:
        """ìºë¦­í„° ìš”ì²­ ì²˜ë¦¬"""
        # ì‹¤ì œ Llama ëª¨ë¸ í˜¸ì¶œ ë¡œì§
        return f"Character response for: {request.user_input}"
    
    async def _process_office_request(self, request) -> str:
        """ì˜¤í”¼ìŠ¤ ìš”ì²­ ì²˜ë¦¬"""
        # ì‹¤ì œ Llama ëª¨ë¸ í˜¸ì¶œ ë¡œì§
        return f"Office response for: {request.user_input}"
```

### 3. LLM ëª¨ë¸ ê³„ì¸µ - v1.7.x ìµœì í™”

#### Character Llama ëª¨ë¸ (llm/llama/character.py)
DarkIdol-Llama-3.1-8B ëª¨ë¸ì„ ì‚¬ìš©í•œ ìºë¦­í„° ëŒ€í™” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

```python
'''
DarkIdol-Llama-3.1-8B GGUF ëª¨ë¸ì„ ì‚¬ìš©í•œ ìºë¦­í„° ê¸°ë°˜ ëŒ€í™” ìƒì„± ëª¨ë“ˆ
'''
from typing import Optional, Generator
from llama_cpp_cuda import Llama, LogitsProcessor
import warnings
import sys
import json
import time
from queue import Queue
from threading import Thread
import os

from domain import character_config, base_config

GREEN = "\033[32m"
RED = "\033[31m"
YELLOW = "\033[33m"
BLUE = "\033[34m"
RESET = "\033[0m"

def build_llama3_prompt(character_info: character_config.CharacterPrompt) -> str:
    """
    ìºë¦­í„° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Llama3 í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        character_info: ìºë¦­í„° ì„¤ì • ì •ë³´
        
    Returns:
        str: Llama3 í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸
    """
    system_prompt = (
        f"Character Name: {character_info.name}\n"
        f"Character Context: {character_info.context}\n"
        f"Initial Greeting: {character_info.greeting}\n"
        "You must respond as this character would, maintaining their personality and speaking style."
    )
    
    prompt = (
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n"
        f"{system_prompt}<|eot_id|>"
        "<|start_header_id|>user<|end_header_id|>\n"
        f"{character_info.user_input}<|eot_id|>"
        "<|start_header_id|>assistant<|end_header_id|>\n"
    )
    
    return prompt

class LlamaCharacterModel:
    """
    DarkIdol-Llama-3.1-8B GGUF ëª¨ë¸ì„ ì‚¬ìš©í•œ ìºë¦­í„° ëŒ€í™” í´ë˜ìŠ¤
    
    Features:
    - CUDA 0 (RTX 3060) ì „ìš© ì‚¬ìš©
    - Q8_0 ì–‘ìí™” (ê³ í’ˆì§ˆ)
    - ìºë¦­í„° í˜ë¥´ì†Œë‚˜ ìœ ì§€
    - ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì›
    """
    
    def __init__(self) -> None:
        """
        DarkIdol ëª¨ë¸ ì´ˆê¸°í™” - RTX 3060 ìµœì í™”
        """
        # ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
        warnings.filterwarnings("ignore", category=UserWarning)
        
        # ëª¨ë¸ ì„¤ì •
        self.model_path = "fastapi/ai_model/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q8_0.gguf"
        self.verbose = False
        self.gpu_layers = 35  # RTX 3060ì— ìµœì í™”
        
        # GGUF ëª¨ë¸ ë¡œë“œ
        self.model = self._load_model()
        self.response_queue = Queue()
        
        print(f"{GREEN}INFO{RESET}: DarkIdol Character ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _load_model(self) -> Llama:
        """
        DarkIdol GGUF ëª¨ë¸ì„ CUDA:0 (RTX 3060)ì— ë¡œë“œ
        
        Returns:
            Llama: ë¡œë“œëœ GGUF ëª¨ë¸ ê°ì²´
            
        Raises:
            Exception: ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ
        """
        print(f"{BLUE}INFO{RESET}: DarkIdol Character ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        try:
            model = Llama(
                model_path=self.model_path,
                n_gpu_layers=self.gpu_layers,
                main_gpu=0,                # RTX 3060 ì‚¬ìš©
                n_ctx=4096,                # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
                n_batch=512,               # ë°°ì¹˜ í¬ê¸°
                verbose=self.verbose,
                offload_kqv=True,          # KQV ìºì‹œ GPU ì˜¤í”„ë¡œë“œ
                use_mmap=False,            # ë©”ëª¨ë¦¬ ë§¤í•‘ ë¹„í™œì„±í™”
                use_mlock=True,            # ë©”ëª¨ë¦¬ ì ê¸ˆ í™œì„±í™”
                n_threads=6,               # ìŠ¤ë ˆë“œ ìˆ˜ (RTX 3060 ìµœì í™”)
                f16_kv=True,               # FP16 KV ìºì‹œ
                logits_all=False,          # ë©”ëª¨ë¦¬ ì ˆì•½
                vocab_only=False,
                use_mlock=True
            )
            
            print(f"{GREEN}SUCCESS{RESET}: DarkIdol Character ëª¨ë¸ì´ CUDA:0 (RTX 3060)ì— ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"{BLUE}CONFIG{RESET}: GPU ë ˆì´ì–´: {self.gpu_layers}, ì»¨í…ìŠ¤íŠ¸: 4096, ë°°ì¹˜: 512")
            
            return model
            
        except Exception as e:
            print(f"{RED}ERROR{RESET}: DarkIdol Character ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def generate_response(self, input_text: str, character_settings: dict = None, chat_list: list = None) -> str:
        """
        ìºë¦­í„° ì„¤ì •ì„ ë°˜ì˜í•œ ì‘ë‹µ ìƒì„±
        
        Args:
            input_text: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            character_settings: ìºë¦­í„° ì„¤ì • ë”•ì…”ë„ˆë¦¬
            chat_list: ì´ì „ ëŒ€í™” ê¸°ë¡ (ì„ íƒì )
            
        Returns:
            str: ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
            
        Raises:
            Exception: ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ ì‹œ
        """
        try:
            # ìºë¦­í„° ì •ë³´ ì„¤ì •
            if character_settings:
                character_info = character_config.CharacterPrompt(
                    name=character_settings.get("character_name", "Assistant"),
                    greeting=character_settings.get("greeting", ""),
                    context=character_settings.get("context", ""),
                    user_input=input_text,
                    chat_list=chat_list or []
                )
                
                prompt = build_llama3_prompt(character_info)
            else:
                prompt = input_text
            
            # ìƒì„± ì„¤ì •
            generation_config = character_config.LlamaGenerationConfig(
                prompt=prompt,
                max_tokens=2048,
                temperature=0.7,
                top_p=0.95,
                top_k=40,
                repeat_penalty=1.1
            )
            
            # ì‘ë‹µ ìƒì„±
            start_time = time.time()
            
            response = self.model(
                prompt=generation_config.prompt,
                max_tokens=generation_config.max_tokens,
                temperature=generation_config.temperature,
                top_p=generation_config.top_p,
                top_k=generation_config.top_k,
                repeat_penalty=generation_config.repeat_penalty,
                stop=["<|eot_id|>", "<|end_of_text|>"],
                echo=False
            )
            
            processing_time = time.time() - start_time
            generated_text = response['choices'][0]['text'].strip()
            
            print(f"{GREEN}INFO{RESET}: Character ì‘ë‹µ ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ, ê¸¸ì´: {len(generated_text)})")
            
            return generated_text
            
        except Exception as e:
            print(f"{RED}ERROR{RESET}: Character ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise Exception(f"ìºë¦­í„° ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
```

### 4. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ - v1.7.x ì‹ ê·œ

#### Locust ê¸°ë°˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ìë™í™”ëœ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

```python
# test/test_character_load.py
import time
import uuid
import json
import csv
from datetime import datetime
from locust import HttpUser, task, between

class CharacterLoadTest(HttpUser):
    """
    Character API ë¶€í•˜ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤
    
    ì‹¤ì œ ìºë¦­í„° ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
    """
    
    wait_time = between(1, 3)  # ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (1-3ì´ˆ)
    
    def on_start(self):
        """í…ŒìŠ¤íŠ¸ ì‹œì‘ ì‹œ ì‹¤í–‰"""
        self.user_id = f"char_user_{uuid.randint(1000, 9999)}"
        self.character_scenarios = [
            {
                "character_name": "ë ˆì´ë‚˜",
                "greeting": "*ë ˆì´ë‚˜ê°€ ì¡°ìš©íˆ ë¯¸ì†Œë¥¼ ì§€ìœ¼ë©° ë‹¤ê°€ì˜¨ë‹¤.*",
                "context": "ë ˆì´ë‚˜ëŠ” 20ëŒ€ ì´ˆë°˜ì˜ ìƒëƒ¥í•˜ê³  ì¹œê·¼í•œ ì„±ê²©ì„ ê°€ì§„ ì—¬ì„±ì´ë‹¤. í•­ìƒ ê¸ì •ì ì´ê³  ë°ì€ ì—ë„ˆì§€ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ë‹¤ë¥¸ ì‚¬ëŒë“¤ì„ ë°°ë ¤í•˜ëŠ” ë§ˆìŒì´ ê¹Šë‹¤.",
                "input_options": [
                    "ì•ˆë…•í•˜ì„¸ìš”, ë ˆì´ë‚˜ë‹˜! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì„¸ìš”?",
                    "*ë ˆì´ë‚˜ì—ê²Œ ì†ì„ í”ë“¤ë©° ì¸ì‚¬í•œë‹¤.*",
                    "ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”. í•¨ê»˜ ì‚°ì±…í• ê¹Œìš”?",
                    "ì˜¤ëŠ˜ ë­”ê°€ íŠ¹ë³„í•œ ì¼ì´ ìˆë‚˜ìš”?",
                    "*ë ˆì´ë‚˜ì˜ ë¯¸ì†Œë¥¼ ë³´ë©° ë”°ëœ»í•œ ê¸°ë¶„ì´ ë“ ë‹¤.*"
                ]
            }
        ]
        
        # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
        self.results = []
    
    @task(1)
    def test_character_llama(self):
        """ìºë¦­í„° Llama ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        scenario = self.character_scenarios[0]  # ë ˆì´ë‚˜ ì‹œë‚˜ë¦¬ì˜¤
        input_text = self.client.random.choice(scenario["input_options"])
        
        payload = {
            "input_data": input_text,
            "character_name": scenario["character_name"],
            "greeting": scenario["greeting"],
            "context": scenario["context"],
            "db_id": str(uuid.uuid4()),
            "user_id": self.user_id
        }
        
        start_time = time.time()
        
        try:
            with self.client.post(
                "/character/Llama",
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=300,  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                catch_response=True
            ) as response:
                
                response_time = time.time() - start_time
                
                # ê²°ê³¼ ê¸°ë¡
                result = {
                    "user_id": self.user_id,
                    "test_type": "Character-Llama",
                    "endpoint": "/character/Llama",
                    "character_name": scenario["character_name"],
                    "status_code": response.status_code,
                    "response_time": response_time,
                    "success": response.status_code == 200,
                    "failure_reason": "",
                    "retry_count": 0,
                    "retry_after_seconds": 0,
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "response_size": len(response.content) if response.content else 0,
                    "is_retry_record": False,
                    "is_final_failure": False,
                    "is_final_success": response.status_code == 200,
                    "error": "",
                    "process_time": 0.0
                }
                
                if response.status_code == 200:
                    try:
                        response_text = response.text
                        result["process_time"] = response_time
                        response.success()
                        print(f"âœ… Character ì‘ë‹µ ì„±ê³µ (ì†Œìš”ì‹œê°„: {response_time:.3f}ì´ˆ)")
                    except json.JSONDecodeError:
                        result["failure_reason"] = "invalid_json_response"
                        result["success"] = False
                        response.failure("ì‘ë‹µì´ ìœ íš¨í•œ JSONì´ ì•„ë‹™ë‹ˆë‹¤.")
                        
                elif response.status_code == 429:
                    # ì²˜ë¦¬ ëŠ¥ë ¥ ì´ˆê³¼ (í í¬í™”)
                    try:
                        error_data = response.json()
                        retry_after = error_data.get("retry_after", 300)
                        queue_size = error_data.get("queue_size", 0)
                        
                        result["failure_reason"] = "429_retry_attempt_1"
                        result["retry_count"] = 1
                        result["retry_after_seconds"] = retry_after
                        result["is_retry_record"] = True
                        
                        print(f"âš ï¸ í í¬í™”ë¡œ ì¸í•œ ì§€ì—° (ëŒ€ê¸°ì—´: {queue_size}, ì¬ì‹œë„: {retry_after}ì´ˆ í›„)")
                        response.failure(f"í í¬í™” - ì¬ì‹œë„ í•„ìš”: {retry_after}ì´ˆ")
                        
                    except:
                        result["failure_reason"] = "429_unknown_format"
                        response.failure("429 ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜")
                        
                elif response.status_code == 504:
                    # íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜
                    result["failure_reason"] = "http_error_504"
                    result["is_final_failure"] = True
                    response.failure("ì„œë²„ íƒ€ì„ì•„ì›ƒ (504)")
                    
                else:
                    result["failure_reason"] = f"http_error_{response.status_code}"
                    response.failure(f"HTTP ì˜¤ë¥˜: {response.status_code}")
                
                # ê²°ê³¼ ì €ì¥
                self.save_result(result)
                
        except Exception as e:
            # ì—°ê²° ì˜¤ë¥˜ ë“± ì˜ˆì™¸ ì²˜ë¦¬
            response_time = time.time() - start_time
            result = {
                "user_id": self.user_id,
                "test_type": "Character-Llama",
                "endpoint": "/character/Llama",
                "character_name": scenario["character_name"],
                "status_code": 0,
                "response_time": response_time,
                "success": False,
                "failure_reason": f"connection_error",
                "error": str(e),
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "is_final_failure": True
            }
            self.save_result(result)
            print(f"âŒ ì—°ê²° ì˜¤ë¥˜: {str(e)}")
    
    def save_result(self, result):
        """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        filename = f"performance_results/character_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}.csv"
        
        # í—¤ë” ì •ì˜
        fieldnames = [
            'user_id', 'test_type', 'endpoint', 'character_name', 'status_code', 
            'response_time', 'success', 'failure_reason', 'retry_count', 
            'retry_after_seconds', 'timestamp', 'response_size', 'is_retry_record',
            'is_final_failure', 'is_final_success', 'error', 'process_time'
        ]
        
        # íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë”ì™€ í•¨ê»˜ ìƒì„±
        try:
            with open(filename, 'a', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                
                # íŒŒì¼ì´ ë¹„ì–´ìˆìœ¼ë©´ í—¤ë” ì‘ì„±
                if csvfile.tell() == 0:
                    writer.writeheader()
                
                writer.writerow(result)
                
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
```

#### ì„±ëŠ¥ ê²°ê³¼ ì‹œê°í™” (visualization.html)
ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ëŠ” ëŒ€ì‹œë³´ë“œì…ë‹ˆë‹¤.

```html
<!DOCTYPE html>
<html>
<head>
    <title>ChatBot AI ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼</title>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .container { max-width: 1200px; margin: 0 auto; }
        .chart { margin: 30px 0; padding: 20px; border: 1px solid #ddd; border-radius: 8px; }
        .stats { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 20px 0; }
        .stat-card { padding: 20px; background: #f5f5f5; border-radius: 8px; text-align: center; }
        .stat-value { font-size: 2em; font-weight: bold; color: #2c3e50; }
        .stat-label { color: #7f8c8d; margin-top: 5px; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸš€ ChatBot AI ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼</h1>
        
        <div class="stats">
            <div class="stat-card">
                <div class="stat-value" id="total-requests">-</div>
                <div class="stat-label">ì´ ìš”ì²­ ìˆ˜</div>
            </div>
            <div class="stat-card">
                <div class="stat-value" id="success-rate">-</div>
                <div class="stat-label">ì„±ê³µë¥  (%)</div>
            </div>
            <div class="stat-card">
                <div class="stat-value" id="avg-response-time">-</div>
                <div class="stat-label">í‰ê·  ì‘ë‹µì‹œê°„ (ì´ˆ)</div>
            </div>
            <div class="stat-card">
                <div class="stat-value" id="max-response-time">-</div>
                <div class="stat-label">ìµœëŒ€ ì‘ë‹µì‹œê°„ (ì´ˆ)</div>
            </div>
        </div>
        
        <div class="chart">
            <h2>ğŸ“Š ì‘ë‹µ ì‹œê°„ ë¶„í¬</h2>
            <div id="response-time-chart"></div>
        </div>
        
        <div class="chart">
            <h2>ğŸ“ˆ ì‹œê°„ë³„ ì‘ë‹µ ì‹œê°„ ì¶”ì´</h2>
            <div id="timeline-chart"></div>
        </div>
        
        <div class="chart">
            <h2>âœ… ì„±ê³µ/ì‹¤íŒ¨ ë¹„ìœ¨</h2>
            <div id="success-pie-chart"></div>
        </div>
        
        <div class="chart">
            <h2>ğŸ”¥ ë™ì‹œ ì‚¬ìš©ì ì²˜ë¦¬ ì„±ëŠ¥</h2>
            <div id="concurrent-users-chart"></div>
        </div>
    </div>

    <script>
        // CSV ë°ì´í„° ë¡œë“œ ë° ì‹œê°í™” ë¡œì§
        // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” CSV íŒŒì¼ì„ ì½ì–´ì„œ ì°¨íŠ¸ë¥¼ ìƒì„±
        
        // ì˜ˆì‹œ ë°ì´í„°
        const sampleData = {
            totalRequests: 1250,
            successRate: 94.2,
            avgResponseTime: 18.7,
            maxResponseTime: 45.3
        };
        
        // í†µê³„ ì—…ë°ì´íŠ¸
        document.getElementById('total-requests').textContent = sampleData.totalRequests;
        document.getElementById('success-rate').textContent = sampleData.successRate + '%';
        document.getElementById('avg-response-time').textContent = sampleData.avgResponseTime + 's';
        document.getElementById('max-response-time').textContent = sampleData.maxResponseTime + 's';
        
        // ì‘ë‹µ ì‹œê°„ íˆìŠ¤í† ê·¸ë¨
        const responseTimeData = [{
            x: [5, 10, 15, 20, 25, 30, 35, 40, 45],
            type: 'histogram',
            marker: { color: '#3498db' },
            name: 'ì‘ë‹µ ì‹œê°„ ë¶„í¬'
        }];
        
        Plotly.newPlot('response-time-chart', responseTimeData, {
            title: 'ì‘ë‹µ ì‹œê°„ ë¶„í¬ (ì´ˆ)',
            xaxis: { title: 'ì‘ë‹µ ì‹œê°„ (ì´ˆ)' },
            yaxis: { title: 'ìš”ì²­ ìˆ˜' }
        });
        
        // ì„±ê³µ/ì‹¤íŒ¨ íŒŒì´ ì°¨íŠ¸
        const successData = [{
            values: [94.2, 5.8],
            labels: ['ì„±ê³µ', 'ì‹¤íŒ¨'],
            type: 'pie',
            marker: {
                colors: ['#2ecc71', '#e74c3c']
            }
        }];
        
        Plotly.newPlot('success-pie-chart', successData, {
            title: 'ìš”ì²­ ì„±ê³µ/ì‹¤íŒ¨ ë¹„ìœ¨'
        });
    </script>
</body>
</html>
```

### 5. nginx API Gateway - v1.7.x ì‹ ê·œ

#### ì»¤ìŠ¤í…€ 404 í˜ì´ì§€ (nginx/404.html)
ë¸Œëœë”©ëœ 404 ì˜¤ë¥˜ í˜ì´ì§€ì…ë‹ˆë‹¤.

```html
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>404 - ChatBot AI</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            color: white;
        }
        
        .container {
            text-align: center;
            max-width: 600px;
            padding: 2rem;
        }
        
        .error-code {
            font-size: 8rem;
            font-weight: bold;
            margin-bottom: 1rem;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }
        
        .error-message {
            font-size: 1.5rem;
            margin-bottom: 1rem;
            opacity: 0.9;
        }
        
        .error-description {
            font-size: 1.1rem;
            margin-bottom: 2rem;
            opacity: 0.8;
            line-height: 1.6;
        }
        
        .api-info {
            background: rgba(255, 255, 255, 0.1);
            padding: 1.5rem;
            border-radius: 10px;
            margin: 2rem 0;
            backdrop-filter: blur(10px);
        }
        
        .api-endpoints {
            text-align: left;
            margin-top: 1rem;
        }
        
        .endpoint {
            margin: 0.5rem 0;
            font-family: 'Courier New', monospace;
            background: rgba(0, 0, 0, 0.2);
            padding: 0.5rem;
            border-radius: 5px;
        }
        
        .btn {
            display: inline-block;
            padding: 12px 24px;
            background: rgba(255, 255, 255, 0.2);
            color: white;
            text-decoration: none;
            border-radius: 25px;
            transition: all 0.3s ease;
            border: 2px solid rgba(255, 255, 255, 0.3);
        }
        
        .btn:hover {
            background: rgba(255, 255, 255, 0.3);
            transform: translateY(-2px);
        }
        
        .robot-icon {
            font-size: 4rem;
            margin-bottom: 1rem;
            animation: float 3s ease-in-out infinite;
        }
        
        @keyframes float {
            0%, 100% { transform: translateY(0px); }
            50% { transform: translateY(-10px); }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="robot-icon">ğŸ¤–</div>
        <div class="error-code">404</div>
        <div class="error-message">ìš”ì²­í•˜ì‹  API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤</div>
        <div class="error-description">
            ChatBot AI API Gatewayì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤.<br>
            ì˜¬ë°”ë¥¸ API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
        </div>
        
        <div class="api-info">
            <h3>ğŸš€ ì‚¬ìš© ê°€ëŠ¥í•œ API ì—”ë“œí¬ì¸íŠ¸</h3>
            <div class="api-endpoints">
                <div class="endpoint">POST /office/Llama - ì—…ë¬´ìš© AI ì‘ë‹µ</div>
                <div class="endpoint">POST /character/Llama - ìºë¦­í„° ëŒ€í™”</div>
                <div class="endpoint">GET /office/performance - Office ì„±ëŠ¥ í†µê³„</div>
                <div class="endpoint">GET /character/performance - Character ì„±ëŠ¥ í†µê³„</div>
            </div>
        </div>
        
        <div>
            <a href="/office/docs" class="btn">ğŸ“š Office API ë¬¸ì„œ</a>
            <a href="/character/docs" class="btn">ğŸ’¬ Character API ë¬¸ì„œ</a>
        </div>
        
        <div style="margin-top: 2rem; opacity: 0.7; font-size: 0.9rem;">
            ChatBot AI v1.7.x | Powered by nginx + Docker
        </div>
    </div>
</body>
</html>
```

## ì„¤ì¹˜ ë° ì„¤ì •

### Docker ê¸°ë°˜ ë°°í¬ ì‹œìŠ¤í…œ - v1.7.x

#### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­
1. **Docker & Docker Compose** ì„¤ì¹˜
2. **NVIDIA Container Toolkit** ì„¤ì¹˜ (GPU ì§€ì›)
3. **AI ëª¨ë¸ íŒŒì¼** ë‹¤ìš´ë¡œë“œ

#### ë¹ ë¥¸ ì‹œì‘
```bash
# 1. ë¦¬í¬ì§€í† ë¦¬ í´ë¡ 
git clone https://github.com/your-repo/chatbot-ai.git
cd chatbot-ai

# 2. AI ëª¨ë¸ íŒŒì¼ ë°°ì¹˜
# fastapi/ai_model/ í´ë”ì— GGUF ëª¨ë¸ íŒŒì¼ ë³µì‚¬
# - MLP-KTLim/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf
# - QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q8_0.gguf

# 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp fastapi/src/.env.example fastapi/src/.env
# .env íŒŒì¼ì—ì„œ OPENAI_API_KEY ë“± ì„¤ì •

# 4. ì»¨í…Œì´ë„ˆ ë¹Œë“œ ë° ì‹¤í–‰
docker compose up --build

# 5. API í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8001/office/Llama" \
  -H "Content-Type: application/json" \
  -d '{"input_data": "ì•ˆë…•í•˜ì„¸ìš”!", "user_id": "test_user"}'
```

#### ê°œë³„ ì„œë¹„ìŠ¤ ì‹¤í–‰
```bash
# Office ì„œë¹„ìŠ¤ë§Œ ì‹¤í–‰
docker compose up office

# Character ì„œë¹„ìŠ¤ë§Œ ì‹¤í–‰
docker compose up character

# nginx Gatewayë§Œ ì‹¤í–‰
docker compose up nginx

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™”ë§Œ ì‹¤í–‰
docker compose up python-libs-init
```

#### ê°œë°œ ëª¨ë“œ ì‹¤í–‰
```bash
# ê°œë°œìš© ë³¼ë¥¨ ë§ˆìš´íŠ¸ì™€ í•¨ê»˜ ì‹¤í–‰
docker compose -f docker-compose.yml -f docker-compose.dev.yml up --build
```

### ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
# Character API ë¶€í•˜ í…ŒìŠ¤íŠ¸
cd fastapi/src/test
python test_character_load.py

# Office API ë¶€í•˜ í…ŒìŠ¤íŠ¸  
python test_office_load.py

# Locust ì›¹ UIë¡œ í…ŒìŠ¤íŠ¸ (í¬íŠ¸ 8089)
locust -f test_character_load.py --host=http://localhost:8001
```

## ì„±ëŠ¥ íŠ¹ì„±

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì»¨í…Œì´ë„ˆ ê¸°ë°˜)
- **Office Service**: ~10GB VRAM (RTX 2080) + ~2GB RAM
- **Character Service**: ~8GB VRAM (RTX 3060) + ~2GB RAM  
- **nginx Gateway**: ~50MB RAM
- **Python Libs Volume**: ~1GB Disk
- **ì´ ì‹œìŠ¤í…œ**: ~15GB RAM, ~20GB VRAM

### ì²˜ë¦¬ëŸ‰ (ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤)
- **ë™ì‹œ ìš”ì²­**: Office(1) + Character(1) = 2ê°œ ë…ë¦½ ì²˜ë¦¬
- **ì‹œê°„ë‹¹ ìš”ì²­**: ~800-1200ê°œ (nginx ë¡œë“œë°¸ëŸ°ì‹±)
- **ì»¨í…Œì´ë„ˆ ì˜¤ë²„í—¤ë“œ**: ~10ms (nginx í”„ë¡ì‹œ í¬í•¨)
- **GPU ê²©ë¦¬**: ì™„ì „í•œ GPU ë¶„ë¦¬ë¡œ ê°„ì„­ ì—†ìŒ

### API ì‘ë‹µ ì‹œê°„ (ì»¨í…Œì´ë„ˆ í™˜ê²½)
- **Office/Llama**: 12-28ì´ˆ (Bllossom Q4_K_M, CUDA:1)
- **Character/Llama**: 15-35ì´ˆ (DarkIdol Q8_0, CUDA:0)
- **nginx Routing**: ~2ms (í”„ë¡ì‹œ ì˜¤ë²„í—¤ë“œ)
- **Container Startup**: ~30ì´ˆ (ì´ˆê¸° ëª¨ë¸ ë¡œë”©)

### ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ (Locust ê¸°ì¤€)
- **ìµœëŒ€ ë™ì‹œ ì‚¬ìš©ì**: 20ëª… (Character API)
- **í‰ê·  ì‘ë‹µ ì‹œê°„**: 18.7ì´ˆ
- **ì„±ê³µë¥ **: 94.2%
- **í í¬í™”ì **: 5ê°œ ìš”ì²­ ëŒ€ê¸° ì‹œ

## ìš´ì˜ ê°€ì´ë“œ

### í™˜ê²½ ì„¤ì •
1. **Docker í™˜ê²½** êµ¬ì„± ë° GPU ì§€ì› í™•ì¸
2. **AI ëª¨ë¸ íŒŒì¼** ë‹¤ìš´ë¡œë“œ ë° ë°°ì¹˜
3. **í™˜ê²½ ë³€ìˆ˜** ì„¤ì • (OpenAI API í‚¤ ë“±)
4. **ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜** ì‹¤í–‰

### ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸
- **ì»¨í…Œì´ë„ˆ ìƒíƒœ**: `docker compose ps`
- **GPU ì‚¬ìš©ë¥ **: `nvidia-smi` (ê° ì»¨í…Œì´ë„ˆë³„)
- **nginx ë¡œê·¸**: `docker compose logs nginx`
- **API ì„±ëŠ¥**: `/performance` ì—”ë“œí¬ì¸íŠ¸
- **ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰**: ë³¼ë¥¨ ë° ì´ë¯¸ì§€ í¬ê¸°

### ë¬¸ì œ í•´ê²°
- **ì»¨í…Œì´ë„ˆ ì‹œì‘ ì‹¤íŒ¨**: `docker compose logs <service>` í™•ì¸
- **GPU ì¸ì‹ ì‹¤íŒ¨**: NVIDIA Container Toolkit ì„¤ì¹˜ í™•ì¸
- **ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨**: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë° ê¶Œí•œ í™•ì¸
- **nginx ë¼ìš°íŒ… ì‹¤íŒ¨**: ì„œë¹„ìŠ¤ ê°„ ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸

### ì„±ëŠ¥ íŠœë‹
1. **ì»¨í…Œì´ë„ˆ ìµœì í™”**
   - CPU/Memory limits ì¡°ì •
   - GPU ì¥ì¹˜ í• ë‹¹ ìµœì í™”
   - ë³¼ë¥¨ ë§ˆìš´íŠ¸ ì„±ëŠ¥ ê°œì„ 

2. **nginx ìµœì í™”**
   - worker_processes ì„¤ì •
   - proxy_buffer ì¡°ì •
   - keepalive ì—°ê²° ì„¤ì •

3. **ëª¨ë¸ ìµœì í™”**
   - GGUF íŒŒë¼ë¯¸í„° íŠœë‹
   - í ì‹œìŠ¤í…œ ìµœì í™”
   - ë©”ëª¨ë¦¬ í’€ ê´€ë¦¬